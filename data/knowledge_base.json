{
  "pdf_memories": [
    {
      "name": "Recipes-Filipino.pdf",
      "chunks": [
        "Tasty and Healthy —\nHeart Healthy Filipino Recipes\nFish Cardillo\nThis is a delicious, low-cost recipe with low-sodium ingredients.\nKeep it low-fat by not adding meat fat (lard) or other fat.\nIngredients Directions\n1. Thoroughly clean fish. Remove scale\n• 1 pound (½ kg) red snapper\nand gills, and wash thoroughly. Drain and\n• 4 teaspoons corn oil for sauté\nset aside.\n• ¼ cup flou\n2. Slice the raw fish into six pieces\n• 1 large onion, sliced\n3. Heat corn oil in frying pan.\n• 3 or 4 medium-sized tomatoes,\n4. Place the flour into a bowl or plastic bag\nchopped\nPlace the raw fish in the flour and cov\n• ½ cup egg whites, beaten the outside of each fish with flo .\n• ½ cup water 5. Sauté fish until golden brown. Set asid\n• A dash ground pepper on top of a paper towel.\n• 15 stalks green onions, chopped 6. Sauté onion and tomatoes. Add ½ cup of\nwater.\n7. Add the beaten egg whites and fish\nCover and let it simmer for 5–10 minutes.\nNutrition Information\n8. Season with ground pepper.\nMakes 6 servings",
        " 9. Sprinkle with chopped green onions.\nEach serving provides:\nCalories: 170\nTotal Fat: 4 g\nSaturated Fat: 1 g\nCholesterol: 45 mg\nQuick Tip\nSodium: 115 mg\nTotal Fiber: 3 g\nProtein: 20 g\nThis recipe is lower in salt and sodium than other\nCarbohydrates: 13 g\nCardillo recipes because it uses:\nPotassium: 600 mg\n• Fresh, not canned, tomatoes\n• Ground pepper and corn oil with no salt added\n• Fresh onion and green onions\n• Fresh, not smoked or canned, fis\nSource: Philippine Heart Center’s Healthy Heart Cookbook.\n1\nAdobong Manok (Marinated Chicken)\nThis low-cost, low-sodium recipe has great flavor that you and your\nfamily will love! It uses chicken breast which is lower in fat than\nother parts of the chicken, like the thigh and leg.\nIngredients Directions\n• 1 teaspoon olive oil 1. Combine olive oil, garlic, and onion in a frying\npan. Add chicken and sauté together until\n• 2 cloves fresh crushed garlic\nchicken has browned.\n• 2 medium chopped onions\n2. Add light soy sauce, vinegar, paprika,\n• 1 ",
        "pound (½ kg) chicken breasts, no skin\nblack pepper, and bay leaf. Stir.\n• 2 tablespoons light soy sauce\n3. Bring to a boil. Simmer for 45–60 minutes or\n• ¼ cup vinegar until chicken is done.\n• 1 teaspoon paprika 4. Remove the chicken and save the liquid in the\npot. Arrange the chicken on a broiler pan. Broil\n• 2 tablespoons ground black pepper\nuntil the chicken has nicely browned. Remove\n• 1 bay leaf, broken in half\nfrom the broiler and place it in a serving bowl.\n• 1 medium red tomato (optional)\n5. Continue to boil the sauce in the uncovered\npan until volume is reduced to about half and\nthe sauce is thick.\nNutrition Information\n6. Pour the thickened sauce over broiled\nadobong (chicken) and garnish with red\nMakes 4 servings\ntomatoes, if desired.\nServing size: ½ cup\nEach serving provides:\nCalories: 190\nTotal Fat: 5 g\nQuick Tip\nSaturated Fat: 1 g\nCholesterol: 70 mg\nSodium: 330 mg This recipe is lower in saturated fat and cholesterol because:\nTotal Fiber: 2 g\n• It is made using chicken wi",
        "thout the skin and any extra fat\nProtein: 26 g\nis removed.\nCarbohydrates: 10 g\nPotassium: 370 mg • Only 1 teaspoon of unsaturated fat (olive oil) is added.\n• It is flavored with vegetables and herbs and is boiled an\nbroiled slowly in moist heat instead of fat.\nSource: Filipino-American Nutrition and Fitness Teacher’s\nGuide, Kalusugan Community Services, San Diego, CA.\n2\nLumpiang Sariwa (Fresh Lumpia)\nYou and your family will love this tasty recipe. The ingredients—ground chicken\nor pork, olive oil, peanuts, and fresh herbs and spices—add flavor. Also, the\nlumpiang sariwa is served fresh so it has fewer calories than fried lumpiang.\nIngredients Directions\n• ½ cup ground chicken or lean pork 1. Heat oil, and sauté ground meat with the\nshrimp and garlic.\n• ½ cup shrimp, cleaned and deveined\n2. Add vegetables until slightly crisp. Pour in\n• 1 tablespoon olive oil\nthe chicken broth until cooked.\n• 2 cloves chopped garlic\n3. Season with salt and pepper.\n• ½ cup cabbage, julienned\n4. Set asid",
        "e and drain in a colander.\n• ½ cup green beans, julienned\n5. Save the broth for the lumpia sauce.\n• ½ cup carrots, julienned\n6. Soak the Vietnamese spring roll wrappers\n• ¼ cup celery, julienned\none at a time in water until soft and\n• ¼ cup jicama, julienned (may substitute transparent. Dry immediately with a paper\nchestnuts) towel.\n• ½ cup chicken broth 7. Lay the lettuce on the wrapper.\n• ¼ teaspoon salt 8. Place 2 tablespoons of the vegetable\n• ¼ teaspoon pepper mixture on the wrapper.\n• 8 Vietnamese spring-roll wrappers or 9. Fold in one side of the wrapper and roll\nlumpia wrappers tightly.\n• 8 pieces red leaf lettuce 10. Serve with lumpia sauce on top.\nSprinkle with chopped peanuts.\n• ⅓ cup peanuts, dry, roasted, and chopped\nNutrition Information\nLumpia Sauce\nMakes 8 servings\n• 1 cup broth from the sautéed vegetables Serving size: 1 lumpia\n• 1 tablespoon light soy sauce\nEach serving provides:\n• 1 tablespoon brown sugar\nCalories: 160\n• 3 cloves garlic, minced\nTotal Fat: 4 g\n• 1 tea",
        "spoon cornstarch Saturated Fat: 0.5 g\n• 2 tablespoons cold water for mixing cornstarch Cholesterol: 55 mg\nSodium: 150 mg\nDirections Total Fiber: 2 g\n1. Mix together vegetable broth, soy sauce, brown Protein: 10 g\nsugar, and garlic, and bring to a boil. Carbohydrates: 21 g\nPotassium: 170 mg\n2. Mix the cornstarch in 2 tablespoons of cold water.\n3. Slowly add the cornstarch mixture to the broth.\nStir until sauce thickens.\nSource: Mula sa Puso, Heart Healthy Traditional\nFilipino Recipes, American Heart Association, 1999.\n3\nPesang Isda (Fish Simmered With\nGinger and Tomatoes)\nThis main dish is heart healthy because the fish is simmered in water,\nnot fried, and no fat is added. Flavoring comes from the herbs and spices\ninstead of sauces that are high in sodium.\nIngredients Directions\n1. In a 4-quart saucepan,\n• ¼ cup fresh ginger, thinly\nsimmer sliced ginger,\nsliced (about 2 inches long)\ntomatoes, and onions in\n• 1 cup ripe tomatoes, chopped\n4 cups of water over medium\n• 1 cup white or yello",
        "w onions, heat until onions are tender\nthinly sliced (about 7 to 8 minutes).\n• 4 cups water 2. Reduce heat to low, add fish\n• 2 pounds fleshy fish (c and poach gently until almost\nfillet, halibut steak, or trout done (about 3 to 4 minutes).\n• 2 cups pechay (bok choy) 3. Add pechay stems, salt, and\nstems and leaves, cut up ground pepper. Cook for\nseparately 1 minute; then add pechay\nleaves and green onions.\n• ½ teaspoon salt\nCook another 30 seconds.\n• ½ teaspoon ground pepper\n4. Serve immediately.\n• 1 cup green onions, sliced\nNutrition Information\nMakes 6 servings\nServing size: 3 ounces lean\nfish and ½ cup vegetables\nEach serving provides:\nCalories: 160\nTotal Fat: 2 g\nSaturated Fat: 0.5 g\nCholesterol: 80 mg\nSodium: 340 mg\nTotal Fiber: 2 g\nProtein: 30 g\nCarbohydrates: 6 g\nPotassium: 630 mg\nSource: Filipino American Food Practices, Customs, and Holidays, American Dietetic Association, 1994.\n4\nMunggo Guisado (Sautéed Mung Beans)\nYour family will love this heart healthy side dish. It is mad",
        "e with vegetables,\nseafood, lean meat, and a small amount of corn oil. The pork is slowly\nsimmered in moist heat with vegetables and mung beans, creating flavors\nthat will make your taste buds jump for joy!\nIngredients Directions\n• 1 tablespoon corn oil 1. In a skillet, heat oil, and sauté\ncrushed garlic until lightly brown.\n• 2 cloves fresh garlic, crushed (or\n1 tablespoon, minced) 2. Add onion and tomatoes. Sauté\nuntil skin begins to curl.\n• 1 cup white onions, chopped\n3. Add pork, and sauté until lightly\n• 1 cup ripe tomatoes, chopped\nbrown.\n• 1 cup (4 ounces) lean pork, thinly\n4. Add water, and simmer pork for\nsliced\nabout 15 minutes.\n• 4 cups water\n5. Add the sautéed mix to mung\n• 3½ cups precooked mung beans\nbeans, and continue to simmer\n(from 1¾ cups dry beans)*\n15 minutes.\n• 1 teaspoon salt\n6. Season with salt and ground\n• 1 teaspoon ground pepper\npepper.\n• 1 cup (4 ounces) shrimp, peeled\n7. Add peeled shrimp.\nand deveined\n8. Add frozen leaf spinach, and\n• 1 cup (about ⅔ of a 1",
        "0-ounce\ncook 4 minutes until done.\npackage) leaf spinach, frozen\n* To cook dry, uncooked mung beans:\nWash and boil the uncooked mung\nbeans in a large saucepan, using 6 Nutrition Information\ncups of water. Cook until tender, about\n1½ to 2 hours. Drain. Makes 8 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 160\nTotal Fat: 3.5 g\nSaturated Fat: 1 g\nCholesterol: 35 mg\nSodium: 350 mg\nSource: Filipino American Food Practices, Customs,\nand Holidays, American Dietetic Association, 1994. Total Fiber: 8 g\nProtein: 13 g\nCarbohydrates: 19 g\nPotassium: 370 mg\n5\nAmpalaya (Bitter Melon) With Pork\nThis recipe is lower in fat and sodium than a typical ampalaya dish\nbecause it uses lean meat that is sautéed and simmered instead of fried.\nIngredients Directions\n• 1 cup onion, chopped 1. Using a large skillet, lightly sauté\nonions and garlic in hot olive oil.\n• 6 cloves garlic, crushed\n2. Add the ground pork and cook until\n• 1 tablespoon olive oil\nalmost done.\n• ½ pound (0.2 kg) lean ground ",
        "pork\n3. Add the sliced bitter melon.\n• 2 cups Ampalaya*, sliced\n4. Cover and simmer until bitter melon\n• 2 teaspoons light soy sauce\nturns green. Do not overcook.\n• ½ teaspoon black pepper\n5. Season with light soy sauce and\nblack pepper.\n* Ampalaya (bitter melon) is a fruit that is\noblong, cylindrical, pointed at both ends,\nribbed, and wrinkled.\nNutrition Information\nMakes 4 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 150\nTotal Fat: 6 g\nSaturated Fat: 1.5 g\nCholesterol: 45 mg\nSodium: 200 mg\nTotal Fiber: 1 g\nProtein: 17 g\nCarbohydrates: 7 g\nPotassium: 600 mg\nSource: Adapted from Mula Sa Puso, Heart Healthy Traditional Filipino Recipes, American Heart Association, 1999.\n6\nCantaloupe Crush\nTry this refreshing, heart healthy drink that uses fresh fruit, fat-free milk, and low\namounts of sweetener. Children and adults alike will love it!\nIngredients Directions\n• ½ cantaloupe 1. Cut cantaloupe into small\ncubes or thin strips.\n• 1 cup fat-free milk\n2. Mix cantaloupe, milk, a",
        "nd\n• 1½ cups ice\nice in a blender until smooth.\n• Sweetener, as needed\n3. Sweeten to taste.\n(about 1 to 2 teaspoons\nsugar or equivalent of\nanother sweetener)\nNutrition Information\nMakes 4 servings\nServing size: ½ cup\nEach serving provides:\nCalories: 50\nTotal Fat: 0 g\nSaturated Fat: 0 g\nCholesterol: 0 mg\nSodium: 40 mg\nTotal Fiber: 0 g\nProtein: 3 g\nCarbohydrates: 10 g\nPotassium: 280 mg\nSource: Adapted from the National Cancer Institute and InteliHealth (intelihealth.com), 2013.\n7\nVegetable Kare-Kare (Peanut Stew)\nThis version of vegetable kare-kare is healthier than the traditional Filipino dish\nbecause it has no cholesterol. It uses gluten instead of oxtail or other meat. It is also\npacked with vegetables and made complete with a nutty, low-sodium sauce.\nIngredients\n• 1 ¼ cup gluten or seitan,* cubes\n• 2 tablespoons corn oil\n• 2 cloves garlic, crushed\nNutrition Information\n• 1 onion, medium, sliced\n• ½ cup ground peanuts Makes 6 servings\n• ¼ cup ground toasted rice**\nEach serving provid",
        "es:\n• ¼ teaspoon salt\nCalories: 300\n• 1 cup eggplant, sliced\nTotal Fat: 12 g\n• ½ cup string beans, sliced Saturated Fat: 1.5 g\n• ⅔ cup banana heart or bud Cholesterol: 0 mg\nSodium: 125 mg\n• ½ cup bok choy (pechay), sliced\nTotal Fiber: 4 g\n* Gluten is made from protein that is in a variety of grains, such Protein: 36 g\nas wheat and rye, and is mixed and kneaded with water. Seitan\nCarbohydrates: 20 g\nis a form of wheat gluten. It is sold as strips or in cans at health\nPotassium: 320 mg\nfood stores and Asian supermarkets.\n** To make ground, toasted rice: Place rice, ½ cup at a time,\nin a frying pan or wok and heat over moderate heat, stirring\nfrequently to keep it from burning and to allow it to develop a\nuniform, deep golden color—2 to 3 minutes. Then remove it\nfrom heat and cool to room temperature. Grind the toasted rice\ncoarsely—not finely grounded—in a blende, or spice or coffee\ngrinder.\nDirections\n1. Sauté gluten cubes in corn oil. Add garlic and onions.\n2. Pour enough water to cove",
        "r gluten, and add ground\npeanuts and ground rice to thicken.\n3. Season with salt.\n4. Add the eggplant, then string beans, then banana,\nthen bok choy (pechay) on top of the cooked gluten.\nSource: PHC Alive Diet, Division of Nutrition and Dietetics, Philippine Heart Center, East Avenue, Quezon City, Philippines, page 91.\n8\nNational Heart, Lung, and Blood Institute\nAn Initiative of the to help reduce health disparities in cardiovascular disease\nand asthma in underserved and minority communities. For more information, visit www.nhlbi.nih.gov/health/healthdisp."
      ],
      "uploaded_at": "2025-07-25T19:50:43.933380",
      "source": "pdf_upload"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "chunks": [
        "Tasty and Healthy —\nHeart Healthy Filipino Recipes\nFish Cardillo\nThis is a delicious, low-cost recipe with low-sodium ingredients.\nKeep it low-fat by not adding meat fat (lard) or other fat.\nIngredients Directions\n1. Thoroughly clean fish. Remove scale\n• 1 pound (½ kg) red snapper\nand gills, and wash thoroughly. Drain and\n• 4 teaspoons corn oil for sauté\nset aside.\n• ¼ cup flou\n2. Slice the raw fish into six pieces\n• 1 large onion, sliced\n3. Heat corn oil in frying pan.\n• 3 or 4 medium-sized tomatoes,\n4. Place the flour into a bowl or plastic bag\nchopped\nPlace the raw fish in the flour and cov\n• ½ cup egg whites, beaten the outside of each fish with flo .\n• ½ cup water 5. Sauté fish until golden brown. Set asid\n• A dash ground pepper on top of a paper towel.\n• 15 stalks green onions, chopped 6. Sauté onion and tomatoes. Add ½ cup of\nwater.\n7. Add the beaten egg whites and fish\nCover and let it simmer for 5–10 minutes.\nNutrition Information\n8. Season with ground pepper.\nMakes 6 servings",
        " 9. Sprinkle with chopped green onions.\nEach serving provides:\nCalories: 170\nTotal Fat: 4 g\nSaturated Fat: 1 g\nCholesterol: 45 mg\nQuick Tip\nSodium: 115 mg\nTotal Fiber: 3 g\nProtein: 20 g\nThis recipe is lower in salt and sodium than other\nCarbohydrates: 13 g\nCardillo recipes because it uses:\nPotassium: 600 mg\n• Fresh, not canned, tomatoes\n• Ground pepper and corn oil with no salt added\n• Fresh onion and green onions\n• Fresh, not smoked or canned, fis\nSource: Philippine Heart Center’s Healthy Heart Cookbook.\n1\nAdobong Manok (Marinated Chicken)\nThis low-cost, low-sodium recipe has great flavor that you and your\nfamily will love! It uses chicken breast which is lower in fat than\nother parts of the chicken, like the thigh and leg.\nIngredients Directions\n• 1 teaspoon olive oil 1. Combine olive oil, garlic, and onion in a frying\npan. Add chicken and sauté together until\n• 2 cloves fresh crushed garlic\nchicken has browned.\n• 2 medium chopped onions\n2. Add light soy sauce, vinegar, paprika,\n• 1 ",
        "pound (½ kg) chicken breasts, no skin\nblack pepper, and bay leaf. Stir.\n• 2 tablespoons light soy sauce\n3. Bring to a boil. Simmer for 45–60 minutes or\n• ¼ cup vinegar until chicken is done.\n• 1 teaspoon paprika 4. Remove the chicken and save the liquid in the\npot. Arrange the chicken on a broiler pan. Broil\n• 2 tablespoons ground black pepper\nuntil the chicken has nicely browned. Remove\n• 1 bay leaf, broken in half\nfrom the broiler and place it in a serving bowl.\n• 1 medium red tomato (optional)\n5. Continue to boil the sauce in the uncovered\npan until volume is reduced to about half and\nthe sauce is thick.\nNutrition Information\n6. Pour the thickened sauce over broiled\nadobong (chicken) and garnish with red\nMakes 4 servings\ntomatoes, if desired.\nServing size: ½ cup\nEach serving provides:\nCalories: 190\nTotal Fat: 5 g\nQuick Tip\nSaturated Fat: 1 g\nCholesterol: 70 mg\nSodium: 330 mg This recipe is lower in saturated fat and cholesterol because:\nTotal Fiber: 2 g\n• It is made using chicken wi",
        "thout the skin and any extra fat\nProtein: 26 g\nis removed.\nCarbohydrates: 10 g\nPotassium: 370 mg • Only 1 teaspoon of unsaturated fat (olive oil) is added.\n• It is flavored with vegetables and herbs and is boiled an\nbroiled slowly in moist heat instead of fat.\nSource: Filipino-American Nutrition and Fitness Teacher’s\nGuide, Kalusugan Community Services, San Diego, CA.\n2\nLumpiang Sariwa (Fresh Lumpia)\nYou and your family will love this tasty recipe. The ingredients—ground chicken\nor pork, olive oil, peanuts, and fresh herbs and spices—add flavor. Also, the\nlumpiang sariwa is served fresh so it has fewer calories than fried lumpiang.\nIngredients Directions\n• ½ cup ground chicken or lean pork 1. Heat oil, and sauté ground meat with the\nshrimp and garlic.\n• ½ cup shrimp, cleaned and deveined\n2. Add vegetables until slightly crisp. Pour in\n• 1 tablespoon olive oil\nthe chicken broth until cooked.\n• 2 cloves chopped garlic\n3. Season with salt and pepper.\n• ½ cup cabbage, julienned\n4. Set asid",
        "e and drain in a colander.\n• ½ cup green beans, julienned\n5. Save the broth for the lumpia sauce.\n• ½ cup carrots, julienned\n6. Soak the Vietnamese spring roll wrappers\n• ¼ cup celery, julienned\none at a time in water until soft and\n• ¼ cup jicama, julienned (may substitute transparent. Dry immediately with a paper\nchestnuts) towel.\n• ½ cup chicken broth 7. Lay the lettuce on the wrapper.\n• ¼ teaspoon salt 8. Place 2 tablespoons of the vegetable\n• ¼ teaspoon pepper mixture on the wrapper.\n• 8 Vietnamese spring-roll wrappers or 9. Fold in one side of the wrapper and roll\nlumpia wrappers tightly.\n• 8 pieces red leaf lettuce 10. Serve with lumpia sauce on top.\nSprinkle with chopped peanuts.\n• ⅓ cup peanuts, dry, roasted, and chopped\nNutrition Information\nLumpia Sauce\nMakes 8 servings\n• 1 cup broth from the sautéed vegetables Serving size: 1 lumpia\n• 1 tablespoon light soy sauce\nEach serving provides:\n• 1 tablespoon brown sugar\nCalories: 160\n• 3 cloves garlic, minced\nTotal Fat: 4 g\n• 1 tea",
        "spoon cornstarch Saturated Fat: 0.5 g\n• 2 tablespoons cold water for mixing cornstarch Cholesterol: 55 mg\nSodium: 150 mg\nDirections Total Fiber: 2 g\n1. Mix together vegetable broth, soy sauce, brown Protein: 10 g\nsugar, and garlic, and bring to a boil. Carbohydrates: 21 g\nPotassium: 170 mg\n2. Mix the cornstarch in 2 tablespoons of cold water.\n3. Slowly add the cornstarch mixture to the broth.\nStir until sauce thickens.\nSource: Mula sa Puso, Heart Healthy Traditional\nFilipino Recipes, American Heart Association, 1999.\n3\nPesang Isda (Fish Simmered With\nGinger and Tomatoes)\nThis main dish is heart healthy because the fish is simmered in water,\nnot fried, and no fat is added. Flavoring comes from the herbs and spices\ninstead of sauces that are high in sodium.\nIngredients Directions\n1. In a 4-quart saucepan,\n• ¼ cup fresh ginger, thinly\nsimmer sliced ginger,\nsliced (about 2 inches long)\ntomatoes, and onions in\n• 1 cup ripe tomatoes, chopped\n4 cups of water over medium\n• 1 cup white or yello",
        "w onions, heat until onions are tender\nthinly sliced (about 7 to 8 minutes).\n• 4 cups water 2. Reduce heat to low, add fish\n• 2 pounds fleshy fish (c and poach gently until almost\nfillet, halibut steak, or trout done (about 3 to 4 minutes).\n• 2 cups pechay (bok choy) 3. Add pechay stems, salt, and\nstems and leaves, cut up ground pepper. Cook for\nseparately 1 minute; then add pechay\nleaves and green onions.\n• ½ teaspoon salt\nCook another 30 seconds.\n• ½ teaspoon ground pepper\n4. Serve immediately.\n• 1 cup green onions, sliced\nNutrition Information\nMakes 6 servings\nServing size: 3 ounces lean\nfish and ½ cup vegetables\nEach serving provides:\nCalories: 160\nTotal Fat: 2 g\nSaturated Fat: 0.5 g\nCholesterol: 80 mg\nSodium: 340 mg\nTotal Fiber: 2 g\nProtein: 30 g\nCarbohydrates: 6 g\nPotassium: 630 mg\nSource: Filipino American Food Practices, Customs, and Holidays, American Dietetic Association, 1994.\n4\nMunggo Guisado (Sautéed Mung Beans)\nYour family will love this heart healthy side dish. It is mad",
        "e with vegetables,\nseafood, lean meat, and a small amount of corn oil. The pork is slowly\nsimmered in moist heat with vegetables and mung beans, creating flavors\nthat will make your taste buds jump for joy!\nIngredients Directions\n• 1 tablespoon corn oil 1. In a skillet, heat oil, and sauté\ncrushed garlic until lightly brown.\n• 2 cloves fresh garlic, crushed (or\n1 tablespoon, minced) 2. Add onion and tomatoes. Sauté\nuntil skin begins to curl.\n• 1 cup white onions, chopped\n3. Add pork, and sauté until lightly\n• 1 cup ripe tomatoes, chopped\nbrown.\n• 1 cup (4 ounces) lean pork, thinly\n4. Add water, and simmer pork for\nsliced\nabout 15 minutes.\n• 4 cups water\n5. Add the sautéed mix to mung\n• 3½ cups precooked mung beans\nbeans, and continue to simmer\n(from 1¾ cups dry beans)*\n15 minutes.\n• 1 teaspoon salt\n6. Season with salt and ground\n• 1 teaspoon ground pepper\npepper.\n• 1 cup (4 ounces) shrimp, peeled\n7. Add peeled shrimp.\nand deveined\n8. Add frozen leaf spinach, and\n• 1 cup (about ⅔ of a 1",
        "0-ounce\ncook 4 minutes until done.\npackage) leaf spinach, frozen\n* To cook dry, uncooked mung beans:\nWash and boil the uncooked mung\nbeans in a large saucepan, using 6 Nutrition Information\ncups of water. Cook until tender, about\n1½ to 2 hours. Drain. Makes 8 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 160\nTotal Fat: 3.5 g\nSaturated Fat: 1 g\nCholesterol: 35 mg\nSodium: 350 mg\nSource: Filipino American Food Practices, Customs,\nand Holidays, American Dietetic Association, 1994. Total Fiber: 8 g\nProtein: 13 g\nCarbohydrates: 19 g\nPotassium: 370 mg\n5\nAmpalaya (Bitter Melon) With Pork\nThis recipe is lower in fat and sodium than a typical ampalaya dish\nbecause it uses lean meat that is sautéed and simmered instead of fried.\nIngredients Directions\n• 1 cup onion, chopped 1. Using a large skillet, lightly sauté\nonions and garlic in hot olive oil.\n• 6 cloves garlic, crushed\n2. Add the ground pork and cook until\n• 1 tablespoon olive oil\nalmost done.\n• ½ pound (0.2 kg) lean ground ",
        "pork\n3. Add the sliced bitter melon.\n• 2 cups Ampalaya*, sliced\n4. Cover and simmer until bitter melon\n• 2 teaspoons light soy sauce\nturns green. Do not overcook.\n• ½ teaspoon black pepper\n5. Season with light soy sauce and\nblack pepper.\n* Ampalaya (bitter melon) is a fruit that is\noblong, cylindrical, pointed at both ends,\nribbed, and wrinkled.\nNutrition Information\nMakes 4 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 150\nTotal Fat: 6 g\nSaturated Fat: 1.5 g\nCholesterol: 45 mg\nSodium: 200 mg\nTotal Fiber: 1 g\nProtein: 17 g\nCarbohydrates: 7 g\nPotassium: 600 mg\nSource: Adapted from Mula Sa Puso, Heart Healthy Traditional Filipino Recipes, American Heart Association, 1999.\n6\nCantaloupe Crush\nTry this refreshing, heart healthy drink that uses fresh fruit, fat-free milk, and low\namounts of sweetener. Children and adults alike will love it!\nIngredients Directions\n• ½ cantaloupe 1. Cut cantaloupe into small\ncubes or thin strips.\n• 1 cup fat-free milk\n2. Mix cantaloupe, milk, a",
        "nd\n• 1½ cups ice\nice in a blender until smooth.\n• Sweetener, as needed\n3. Sweeten to taste.\n(about 1 to 2 teaspoons\nsugar or equivalent of\nanother sweetener)\nNutrition Information\nMakes 4 servings\nServing size: ½ cup\nEach serving provides:\nCalories: 50\nTotal Fat: 0 g\nSaturated Fat: 0 g\nCholesterol: 0 mg\nSodium: 40 mg\nTotal Fiber: 0 g\nProtein: 3 g\nCarbohydrates: 10 g\nPotassium: 280 mg\nSource: Adapted from the National Cancer Institute and InteliHealth (intelihealth.com), 2013.\n7\nVegetable Kare-Kare (Peanut Stew)\nThis version of vegetable kare-kare is healthier than the traditional Filipino dish\nbecause it has no cholesterol. It uses gluten instead of oxtail or other meat. It is also\npacked with vegetables and made complete with a nutty, low-sodium sauce.\nIngredients\n• 1 ¼ cup gluten or seitan,* cubes\n• 2 tablespoons corn oil\n• 2 cloves garlic, crushed\nNutrition Information\n• 1 onion, medium, sliced\n• ½ cup ground peanuts Makes 6 servings\n• ¼ cup ground toasted rice**\nEach serving provid",
        "es:\n• ¼ teaspoon salt\nCalories: 300\n• 1 cup eggplant, sliced\nTotal Fat: 12 g\n• ½ cup string beans, sliced Saturated Fat: 1.5 g\n• ⅔ cup banana heart or bud Cholesterol: 0 mg\nSodium: 125 mg\n• ½ cup bok choy (pechay), sliced\nTotal Fiber: 4 g\n* Gluten is made from protein that is in a variety of grains, such Protein: 36 g\nas wheat and rye, and is mixed and kneaded with water. Seitan\nCarbohydrates: 20 g\nis a form of wheat gluten. It is sold as strips or in cans at health\nPotassium: 320 mg\nfood stores and Asian supermarkets.\n** To make ground, toasted rice: Place rice, ½ cup at a time,\nin a frying pan or wok and heat over moderate heat, stirring\nfrequently to keep it from burning and to allow it to develop a\nuniform, deep golden color—2 to 3 minutes. Then remove it\nfrom heat and cool to room temperature. Grind the toasted rice\ncoarsely—not finely grounded—in a blende, or spice or coffee\ngrinder.\nDirections\n1. Sauté gluten cubes in corn oil. Add garlic and onions.\n2. Pour enough water to cove",
        "r gluten, and add ground\npeanuts and ground rice to thicken.\n3. Season with salt.\n4. Add the eggplant, then string beans, then banana,\nthen bok choy (pechay) on top of the cooked gluten.\nSource: PHC Alive Diet, Division of Nutrition and Dietetics, Philippine Heart Center, East Avenue, Quezon City, Philippines, page 91.\n8\nNational Heart, Lung, and Blood Institute\nAn Initiative of the to help reduce health disparities in cardiovascular disease\nand asthma in underserved and minority communities. For more information, visit www.nhlbi.nih.gov/health/healthdisp."
      ],
      "uploaded_at": "2025-07-25T19:50:48.684958",
      "source": "pdf_upload"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "chunks": [
        "Tasty and Healthy —\nHeart Healthy Filipino Recipes\nFish Cardillo\nThis is a delicious, low-cost recipe with low-sodium ingredients.\nKeep it low-fat by not adding meat fat (lard) or other fat.\nIngredients Directions\n1. Thoroughly clean fish. Remove scale\n• 1 pound (½ kg) red snapper\nand gills, and wash thoroughly. Drain and\n• 4 teaspoons corn oil for sauté\nset aside.\n• ¼ cup flou\n2. Slice the raw fish into six pieces\n• 1 large onion, sliced\n3. Heat corn oil in frying pan.\n• 3 or 4 medium-sized tomatoes,\n4. Place the flour into a bowl or plastic bag\nchopped\nPlace the raw fish in the flour and cov\n• ½ cup egg whites, beaten the outside of each fish with flo .\n• ½ cup water 5. Sauté fish until golden brown. Set asid\n• A dash ground pepper on top of a paper towel.\n• 15 stalks green onions, chopped 6. Sauté onion and tomatoes. Add ½ cup of\nwater.\n7. Add the beaten egg whites and fish\nCover and let it simmer for 5–10 minutes.\nNutrition Information\n8. Season with ground pepper.\nMakes 6 servings",
        " 9. Sprinkle with chopped green onions.\nEach serving provides:\nCalories: 170\nTotal Fat: 4 g\nSaturated Fat: 1 g\nCholesterol: 45 mg\nQuick Tip\nSodium: 115 mg\nTotal Fiber: 3 g\nProtein: 20 g\nThis recipe is lower in salt and sodium than other\nCarbohydrates: 13 g\nCardillo recipes because it uses:\nPotassium: 600 mg\n• Fresh, not canned, tomatoes\n• Ground pepper and corn oil with no salt added\n• Fresh onion and green onions\n• Fresh, not smoked or canned, fis\nSource: Philippine Heart Center’s Healthy Heart Cookbook.\n1\nAdobong Manok (Marinated Chicken)\nThis low-cost, low-sodium recipe has great flavor that you and your\nfamily will love! It uses chicken breast which is lower in fat than\nother parts of the chicken, like the thigh and leg.\nIngredients Directions\n• 1 teaspoon olive oil 1. Combine olive oil, garlic, and onion in a frying\npan. Add chicken and sauté together until\n• 2 cloves fresh crushed garlic\nchicken has browned.\n• 2 medium chopped onions\n2. Add light soy sauce, vinegar, paprika,\n• 1 ",
        "pound (½ kg) chicken breasts, no skin\nblack pepper, and bay leaf. Stir.\n• 2 tablespoons light soy sauce\n3. Bring to a boil. Simmer for 45–60 minutes or\n• ¼ cup vinegar until chicken is done.\n• 1 teaspoon paprika 4. Remove the chicken and save the liquid in the\npot. Arrange the chicken on a broiler pan. Broil\n• 2 tablespoons ground black pepper\nuntil the chicken has nicely browned. Remove\n• 1 bay leaf, broken in half\nfrom the broiler and place it in a serving bowl.\n• 1 medium red tomato (optional)\n5. Continue to boil the sauce in the uncovered\npan until volume is reduced to about half and\nthe sauce is thick.\nNutrition Information\n6. Pour the thickened sauce over broiled\nadobong (chicken) and garnish with red\nMakes 4 servings\ntomatoes, if desired.\nServing size: ½ cup\nEach serving provides:\nCalories: 190\nTotal Fat: 5 g\nQuick Tip\nSaturated Fat: 1 g\nCholesterol: 70 mg\nSodium: 330 mg This recipe is lower in saturated fat and cholesterol because:\nTotal Fiber: 2 g\n• It is made using chicken wi",
        "thout the skin and any extra fat\nProtein: 26 g\nis removed.\nCarbohydrates: 10 g\nPotassium: 370 mg • Only 1 teaspoon of unsaturated fat (olive oil) is added.\n• It is flavored with vegetables and herbs and is boiled an\nbroiled slowly in moist heat instead of fat.\nSource: Filipino-American Nutrition and Fitness Teacher’s\nGuide, Kalusugan Community Services, San Diego, CA.\n2\nLumpiang Sariwa (Fresh Lumpia)\nYou and your family will love this tasty recipe. The ingredients—ground chicken\nor pork, olive oil, peanuts, and fresh herbs and spices—add flavor. Also, the\nlumpiang sariwa is served fresh so it has fewer calories than fried lumpiang.\nIngredients Directions\n• ½ cup ground chicken or lean pork 1. Heat oil, and sauté ground meat with the\nshrimp and garlic.\n• ½ cup shrimp, cleaned and deveined\n2. Add vegetables until slightly crisp. Pour in\n• 1 tablespoon olive oil\nthe chicken broth until cooked.\n• 2 cloves chopped garlic\n3. Season with salt and pepper.\n• ½ cup cabbage, julienned\n4. Set asid",
        "e and drain in a colander.\n• ½ cup green beans, julienned\n5. Save the broth for the lumpia sauce.\n• ½ cup carrots, julienned\n6. Soak the Vietnamese spring roll wrappers\n• ¼ cup celery, julienned\none at a time in water until soft and\n• ¼ cup jicama, julienned (may substitute transparent. Dry immediately with a paper\nchestnuts) towel.\n• ½ cup chicken broth 7. Lay the lettuce on the wrapper.\n• ¼ teaspoon salt 8. Place 2 tablespoons of the vegetable\n• ¼ teaspoon pepper mixture on the wrapper.\n• 8 Vietnamese spring-roll wrappers or 9. Fold in one side of the wrapper and roll\nlumpia wrappers tightly.\n• 8 pieces red leaf lettuce 10. Serve with lumpia sauce on top.\nSprinkle with chopped peanuts.\n• ⅓ cup peanuts, dry, roasted, and chopped\nNutrition Information\nLumpia Sauce\nMakes 8 servings\n• 1 cup broth from the sautéed vegetables Serving size: 1 lumpia\n• 1 tablespoon light soy sauce\nEach serving provides:\n• 1 tablespoon brown sugar\nCalories: 160\n• 3 cloves garlic, minced\nTotal Fat: 4 g\n• 1 tea",
        "spoon cornstarch Saturated Fat: 0.5 g\n• 2 tablespoons cold water for mixing cornstarch Cholesterol: 55 mg\nSodium: 150 mg\nDirections Total Fiber: 2 g\n1. Mix together vegetable broth, soy sauce, brown Protein: 10 g\nsugar, and garlic, and bring to a boil. Carbohydrates: 21 g\nPotassium: 170 mg\n2. Mix the cornstarch in 2 tablespoons of cold water.\n3. Slowly add the cornstarch mixture to the broth.\nStir until sauce thickens.\nSource: Mula sa Puso, Heart Healthy Traditional\nFilipino Recipes, American Heart Association, 1999.\n3\nPesang Isda (Fish Simmered With\nGinger and Tomatoes)\nThis main dish is heart healthy because the fish is simmered in water,\nnot fried, and no fat is added. Flavoring comes from the herbs and spices\ninstead of sauces that are high in sodium.\nIngredients Directions\n1. In a 4-quart saucepan,\n• ¼ cup fresh ginger, thinly\nsimmer sliced ginger,\nsliced (about 2 inches long)\ntomatoes, and onions in\n• 1 cup ripe tomatoes, chopped\n4 cups of water over medium\n• 1 cup white or yello",
        "w onions, heat until onions are tender\nthinly sliced (about 7 to 8 minutes).\n• 4 cups water 2. Reduce heat to low, add fish\n• 2 pounds fleshy fish (c and poach gently until almost\nfillet, halibut steak, or trout done (about 3 to 4 minutes).\n• 2 cups pechay (bok choy) 3. Add pechay stems, salt, and\nstems and leaves, cut up ground pepper. Cook for\nseparately 1 minute; then add pechay\nleaves and green onions.\n• ½ teaspoon salt\nCook another 30 seconds.\n• ½ teaspoon ground pepper\n4. Serve immediately.\n• 1 cup green onions, sliced\nNutrition Information\nMakes 6 servings\nServing size: 3 ounces lean\nfish and ½ cup vegetables\nEach serving provides:\nCalories: 160\nTotal Fat: 2 g\nSaturated Fat: 0.5 g\nCholesterol: 80 mg\nSodium: 340 mg\nTotal Fiber: 2 g\nProtein: 30 g\nCarbohydrates: 6 g\nPotassium: 630 mg\nSource: Filipino American Food Practices, Customs, and Holidays, American Dietetic Association, 1994.\n4\nMunggo Guisado (Sautéed Mung Beans)\nYour family will love this heart healthy side dish. It is mad",
        "e with vegetables,\nseafood, lean meat, and a small amount of corn oil. The pork is slowly\nsimmered in moist heat with vegetables and mung beans, creating flavors\nthat will make your taste buds jump for joy!\nIngredients Directions\n• 1 tablespoon corn oil 1. In a skillet, heat oil, and sauté\ncrushed garlic until lightly brown.\n• 2 cloves fresh garlic, crushed (or\n1 tablespoon, minced) 2. Add onion and tomatoes. Sauté\nuntil skin begins to curl.\n• 1 cup white onions, chopped\n3. Add pork, and sauté until lightly\n• 1 cup ripe tomatoes, chopped\nbrown.\n• 1 cup (4 ounces) lean pork, thinly\n4. Add water, and simmer pork for\nsliced\nabout 15 minutes.\n• 4 cups water\n5. Add the sautéed mix to mung\n• 3½ cups precooked mung beans\nbeans, and continue to simmer\n(from 1¾ cups dry beans)*\n15 minutes.\n• 1 teaspoon salt\n6. Season with salt and ground\n• 1 teaspoon ground pepper\npepper.\n• 1 cup (4 ounces) shrimp, peeled\n7. Add peeled shrimp.\nand deveined\n8. Add frozen leaf spinach, and\n• 1 cup (about ⅔ of a 1",
        "0-ounce\ncook 4 minutes until done.\npackage) leaf spinach, frozen\n* To cook dry, uncooked mung beans:\nWash and boil the uncooked mung\nbeans in a large saucepan, using 6 Nutrition Information\ncups of water. Cook until tender, about\n1½ to 2 hours. Drain. Makes 8 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 160\nTotal Fat: 3.5 g\nSaturated Fat: 1 g\nCholesterol: 35 mg\nSodium: 350 mg\nSource: Filipino American Food Practices, Customs,\nand Holidays, American Dietetic Association, 1994. Total Fiber: 8 g\nProtein: 13 g\nCarbohydrates: 19 g\nPotassium: 370 mg\n5\nAmpalaya (Bitter Melon) With Pork\nThis recipe is lower in fat and sodium than a typical ampalaya dish\nbecause it uses lean meat that is sautéed and simmered instead of fried.\nIngredients Directions\n• 1 cup onion, chopped 1. Using a large skillet, lightly sauté\nonions and garlic in hot olive oil.\n• 6 cloves garlic, crushed\n2. Add the ground pork and cook until\n• 1 tablespoon olive oil\nalmost done.\n• ½ pound (0.2 kg) lean ground ",
        "pork\n3. Add the sliced bitter melon.\n• 2 cups Ampalaya*, sliced\n4. Cover and simmer until bitter melon\n• 2 teaspoons light soy sauce\nturns green. Do not overcook.\n• ½ teaspoon black pepper\n5. Season with light soy sauce and\nblack pepper.\n* Ampalaya (bitter melon) is a fruit that is\noblong, cylindrical, pointed at both ends,\nribbed, and wrinkled.\nNutrition Information\nMakes 4 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 150\nTotal Fat: 6 g\nSaturated Fat: 1.5 g\nCholesterol: 45 mg\nSodium: 200 mg\nTotal Fiber: 1 g\nProtein: 17 g\nCarbohydrates: 7 g\nPotassium: 600 mg\nSource: Adapted from Mula Sa Puso, Heart Healthy Traditional Filipino Recipes, American Heart Association, 1999.\n6\nCantaloupe Crush\nTry this refreshing, heart healthy drink that uses fresh fruit, fat-free milk, and low\namounts of sweetener. Children and adults alike will love it!\nIngredients Directions\n• ½ cantaloupe 1. Cut cantaloupe into small\ncubes or thin strips.\n• 1 cup fat-free milk\n2. Mix cantaloupe, milk, a",
        "nd\n• 1½ cups ice\nice in a blender until smooth.\n• Sweetener, as needed\n3. Sweeten to taste.\n(about 1 to 2 teaspoons\nsugar or equivalent of\nanother sweetener)\nNutrition Information\nMakes 4 servings\nServing size: ½ cup\nEach serving provides:\nCalories: 50\nTotal Fat: 0 g\nSaturated Fat: 0 g\nCholesterol: 0 mg\nSodium: 40 mg\nTotal Fiber: 0 g\nProtein: 3 g\nCarbohydrates: 10 g\nPotassium: 280 mg\nSource: Adapted from the National Cancer Institute and InteliHealth (intelihealth.com), 2013.\n7\nVegetable Kare-Kare (Peanut Stew)\nThis version of vegetable kare-kare is healthier than the traditional Filipino dish\nbecause it has no cholesterol. It uses gluten instead of oxtail or other meat. It is also\npacked with vegetables and made complete with a nutty, low-sodium sauce.\nIngredients\n• 1 ¼ cup gluten or seitan,* cubes\n• 2 tablespoons corn oil\n• 2 cloves garlic, crushed\nNutrition Information\n• 1 onion, medium, sliced\n• ½ cup ground peanuts Makes 6 servings\n• ¼ cup ground toasted rice**\nEach serving provid",
        "es:\n• ¼ teaspoon salt\nCalories: 300\n• 1 cup eggplant, sliced\nTotal Fat: 12 g\n• ½ cup string beans, sliced Saturated Fat: 1.5 g\n• ⅔ cup banana heart or bud Cholesterol: 0 mg\nSodium: 125 mg\n• ½ cup bok choy (pechay), sliced\nTotal Fiber: 4 g\n* Gluten is made from protein that is in a variety of grains, such Protein: 36 g\nas wheat and rye, and is mixed and kneaded with water. Seitan\nCarbohydrates: 20 g\nis a form of wheat gluten. It is sold as strips or in cans at health\nPotassium: 320 mg\nfood stores and Asian supermarkets.\n** To make ground, toasted rice: Place rice, ½ cup at a time,\nin a frying pan or wok and heat over moderate heat, stirring\nfrequently to keep it from burning and to allow it to develop a\nuniform, deep golden color—2 to 3 minutes. Then remove it\nfrom heat and cool to room temperature. Grind the toasted rice\ncoarsely—not finely grounded—in a blende, or spice or coffee\ngrinder.\nDirections\n1. Sauté gluten cubes in corn oil. Add garlic and onions.\n2. Pour enough water to cove",
        "r gluten, and add ground\npeanuts and ground rice to thicken.\n3. Season with salt.\n4. Add the eggplant, then string beans, then banana,\nthen bok choy (pechay) on top of the cooked gluten.\nSource: PHC Alive Diet, Division of Nutrition and Dietetics, Philippine Heart Center, East Avenue, Quezon City, Philippines, page 91.\n8\nNational Heart, Lung, and Blood Institute\nAn Initiative of the to help reduce health disparities in cardiovascular disease\nand asthma in underserved and minority communities. For more information, visit www.nhlbi.nih.gov/health/healthdisp."
      ],
      "uploaded_at": "2025-07-25T19:50:53.332412",
      "source": "pdf_upload"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "chunks": [
        "Tasty and Healthy —\nHeart Healthy Filipino Recipes\nFish Cardillo\nThis is a delicious, low-cost recipe with low-sodium ingredients.\nKeep it low-fat by not adding meat fat (lard) or other fat.\nIngredients Directions\n1. Thoroughly clean fish. Remove scale\n• 1 pound (½ kg) red snapper\nand gills, and wash thoroughly. Drain and\n• 4 teaspoons corn oil for sauté\nset aside.\n• ¼ cup flou\n2. Slice the raw fish into six pieces\n• 1 large onion, sliced\n3. Heat corn oil in frying pan.\n• 3 or 4 medium-sized tomatoes,\n4. Place the flour into a bowl or plastic bag\nchopped\nPlace the raw fish in the flour and cov\n• ½ cup egg whites, beaten the outside of each fish with flo .\n• ½ cup water 5. Sauté fish until golden brown. Set asid\n• A dash ground pepper on top of a paper towel.\n• 15 stalks green onions, chopped 6. Sauté onion and tomatoes. Add ½ cup of\nwater.\n7. Add the beaten egg whites and fish\nCover and let it simmer for 5–10 minutes.\nNutrition Information\n8. Season with ground pepper.\nMakes 6 servings",
        " 9. Sprinkle with chopped green onions.\nEach serving provides:\nCalories: 170\nTotal Fat: 4 g\nSaturated Fat: 1 g\nCholesterol: 45 mg\nQuick Tip\nSodium: 115 mg\nTotal Fiber: 3 g\nProtein: 20 g\nThis recipe is lower in salt and sodium than other\nCarbohydrates: 13 g\nCardillo recipes because it uses:\nPotassium: 600 mg\n• Fresh, not canned, tomatoes\n• Ground pepper and corn oil with no salt added\n• Fresh onion and green onions\n• Fresh, not smoked or canned, fis\nSource: Philippine Heart Center’s Healthy Heart Cookbook.\n1\nAdobong Manok (Marinated Chicken)\nThis low-cost, low-sodium recipe has great flavor that you and your\nfamily will love! It uses chicken breast which is lower in fat than\nother parts of the chicken, like the thigh and leg.\nIngredients Directions\n• 1 teaspoon olive oil 1. Combine olive oil, garlic, and onion in a frying\npan. Add chicken and sauté together until\n• 2 cloves fresh crushed garlic\nchicken has browned.\n• 2 medium chopped onions\n2. Add light soy sauce, vinegar, paprika,\n• 1 ",
        "pound (½ kg) chicken breasts, no skin\nblack pepper, and bay leaf. Stir.\n• 2 tablespoons light soy sauce\n3. Bring to a boil. Simmer for 45–60 minutes or\n• ¼ cup vinegar until chicken is done.\n• 1 teaspoon paprika 4. Remove the chicken and save the liquid in the\npot. Arrange the chicken on a broiler pan. Broil\n• 2 tablespoons ground black pepper\nuntil the chicken has nicely browned. Remove\n• 1 bay leaf, broken in half\nfrom the broiler and place it in a serving bowl.\n• 1 medium red tomato (optional)\n5. Continue to boil the sauce in the uncovered\npan until volume is reduced to about half and\nthe sauce is thick.\nNutrition Information\n6. Pour the thickened sauce over broiled\nadobong (chicken) and garnish with red\nMakes 4 servings\ntomatoes, if desired.\nServing size: ½ cup\nEach serving provides:\nCalories: 190\nTotal Fat: 5 g\nQuick Tip\nSaturated Fat: 1 g\nCholesterol: 70 mg\nSodium: 330 mg This recipe is lower in saturated fat and cholesterol because:\nTotal Fiber: 2 g\n• It is made using chicken wi",
        "thout the skin and any extra fat\nProtein: 26 g\nis removed.\nCarbohydrates: 10 g\nPotassium: 370 mg • Only 1 teaspoon of unsaturated fat (olive oil) is added.\n• It is flavored with vegetables and herbs and is boiled an\nbroiled slowly in moist heat instead of fat.\nSource: Filipino-American Nutrition and Fitness Teacher’s\nGuide, Kalusugan Community Services, San Diego, CA.\n2\nLumpiang Sariwa (Fresh Lumpia)\nYou and your family will love this tasty recipe. The ingredients—ground chicken\nor pork, olive oil, peanuts, and fresh herbs and spices—add flavor. Also, the\nlumpiang sariwa is served fresh so it has fewer calories than fried lumpiang.\nIngredients Directions\n• ½ cup ground chicken or lean pork 1. Heat oil, and sauté ground meat with the\nshrimp and garlic.\n• ½ cup shrimp, cleaned and deveined\n2. Add vegetables until slightly crisp. Pour in\n• 1 tablespoon olive oil\nthe chicken broth until cooked.\n• 2 cloves chopped garlic\n3. Season with salt and pepper.\n• ½ cup cabbage, julienned\n4. Set asid",
        "e and drain in a colander.\n• ½ cup green beans, julienned\n5. Save the broth for the lumpia sauce.\n• ½ cup carrots, julienned\n6. Soak the Vietnamese spring roll wrappers\n• ¼ cup celery, julienned\none at a time in water until soft and\n• ¼ cup jicama, julienned (may substitute transparent. Dry immediately with a paper\nchestnuts) towel.\n• ½ cup chicken broth 7. Lay the lettuce on the wrapper.\n• ¼ teaspoon salt 8. Place 2 tablespoons of the vegetable\n• ¼ teaspoon pepper mixture on the wrapper.\n• 8 Vietnamese spring-roll wrappers or 9. Fold in one side of the wrapper and roll\nlumpia wrappers tightly.\n• 8 pieces red leaf lettuce 10. Serve with lumpia sauce on top.\nSprinkle with chopped peanuts.\n• ⅓ cup peanuts, dry, roasted, and chopped\nNutrition Information\nLumpia Sauce\nMakes 8 servings\n• 1 cup broth from the sautéed vegetables Serving size: 1 lumpia\n• 1 tablespoon light soy sauce\nEach serving provides:\n• 1 tablespoon brown sugar\nCalories: 160\n• 3 cloves garlic, minced\nTotal Fat: 4 g\n• 1 tea",
        "spoon cornstarch Saturated Fat: 0.5 g\n• 2 tablespoons cold water for mixing cornstarch Cholesterol: 55 mg\nSodium: 150 mg\nDirections Total Fiber: 2 g\n1. Mix together vegetable broth, soy sauce, brown Protein: 10 g\nsugar, and garlic, and bring to a boil. Carbohydrates: 21 g\nPotassium: 170 mg\n2. Mix the cornstarch in 2 tablespoons of cold water.\n3. Slowly add the cornstarch mixture to the broth.\nStir until sauce thickens.\nSource: Mula sa Puso, Heart Healthy Traditional\nFilipino Recipes, American Heart Association, 1999.\n3\nPesang Isda (Fish Simmered With\nGinger and Tomatoes)\nThis main dish is heart healthy because the fish is simmered in water,\nnot fried, and no fat is added. Flavoring comes from the herbs and spices\ninstead of sauces that are high in sodium.\nIngredients Directions\n1. In a 4-quart saucepan,\n• ¼ cup fresh ginger, thinly\nsimmer sliced ginger,\nsliced (about 2 inches long)\ntomatoes, and onions in\n• 1 cup ripe tomatoes, chopped\n4 cups of water over medium\n• 1 cup white or yello",
        "w onions, heat until onions are tender\nthinly sliced (about 7 to 8 minutes).\n• 4 cups water 2. Reduce heat to low, add fish\n• 2 pounds fleshy fish (c and poach gently until almost\nfillet, halibut steak, or trout done (about 3 to 4 minutes).\n• 2 cups pechay (bok choy) 3. Add pechay stems, salt, and\nstems and leaves, cut up ground pepper. Cook for\nseparately 1 minute; then add pechay\nleaves and green onions.\n• ½ teaspoon salt\nCook another 30 seconds.\n• ½ teaspoon ground pepper\n4. Serve immediately.\n• 1 cup green onions, sliced\nNutrition Information\nMakes 6 servings\nServing size: 3 ounces lean\nfish and ½ cup vegetables\nEach serving provides:\nCalories: 160\nTotal Fat: 2 g\nSaturated Fat: 0.5 g\nCholesterol: 80 mg\nSodium: 340 mg\nTotal Fiber: 2 g\nProtein: 30 g\nCarbohydrates: 6 g\nPotassium: 630 mg\nSource: Filipino American Food Practices, Customs, and Holidays, American Dietetic Association, 1994.\n4\nMunggo Guisado (Sautéed Mung Beans)\nYour family will love this heart healthy side dish. It is mad",
        "e with vegetables,\nseafood, lean meat, and a small amount of corn oil. The pork is slowly\nsimmered in moist heat with vegetables and mung beans, creating flavors\nthat will make your taste buds jump for joy!\nIngredients Directions\n• 1 tablespoon corn oil 1. In a skillet, heat oil, and sauté\ncrushed garlic until lightly brown.\n• 2 cloves fresh garlic, crushed (or\n1 tablespoon, minced) 2. Add onion and tomatoes. Sauté\nuntil skin begins to curl.\n• 1 cup white onions, chopped\n3. Add pork, and sauté until lightly\n• 1 cup ripe tomatoes, chopped\nbrown.\n• 1 cup (4 ounces) lean pork, thinly\n4. Add water, and simmer pork for\nsliced\nabout 15 minutes.\n• 4 cups water\n5. Add the sautéed mix to mung\n• 3½ cups precooked mung beans\nbeans, and continue to simmer\n(from 1¾ cups dry beans)*\n15 minutes.\n• 1 teaspoon salt\n6. Season with salt and ground\n• 1 teaspoon ground pepper\npepper.\n• 1 cup (4 ounces) shrimp, peeled\n7. Add peeled shrimp.\nand deveined\n8. Add frozen leaf spinach, and\n• 1 cup (about ⅔ of a 1",
        "0-ounce\ncook 4 minutes until done.\npackage) leaf spinach, frozen\n* To cook dry, uncooked mung beans:\nWash and boil the uncooked mung\nbeans in a large saucepan, using 6 Nutrition Information\ncups of water. Cook until tender, about\n1½ to 2 hours. Drain. Makes 8 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 160\nTotal Fat: 3.5 g\nSaturated Fat: 1 g\nCholesterol: 35 mg\nSodium: 350 mg\nSource: Filipino American Food Practices, Customs,\nand Holidays, American Dietetic Association, 1994. Total Fiber: 8 g\nProtein: 13 g\nCarbohydrates: 19 g\nPotassium: 370 mg\n5\nAmpalaya (Bitter Melon) With Pork\nThis recipe is lower in fat and sodium than a typical ampalaya dish\nbecause it uses lean meat that is sautéed and simmered instead of fried.\nIngredients Directions\n• 1 cup onion, chopped 1. Using a large skillet, lightly sauté\nonions and garlic in hot olive oil.\n• 6 cloves garlic, crushed\n2. Add the ground pork and cook until\n• 1 tablespoon olive oil\nalmost done.\n• ½ pound (0.2 kg) lean ground ",
        "pork\n3. Add the sliced bitter melon.\n• 2 cups Ampalaya*, sliced\n4. Cover and simmer until bitter melon\n• 2 teaspoons light soy sauce\nturns green. Do not overcook.\n• ½ teaspoon black pepper\n5. Season with light soy sauce and\nblack pepper.\n* Ampalaya (bitter melon) is a fruit that is\noblong, cylindrical, pointed at both ends,\nribbed, and wrinkled.\nNutrition Information\nMakes 4 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 150\nTotal Fat: 6 g\nSaturated Fat: 1.5 g\nCholesterol: 45 mg\nSodium: 200 mg\nTotal Fiber: 1 g\nProtein: 17 g\nCarbohydrates: 7 g\nPotassium: 600 mg\nSource: Adapted from Mula Sa Puso, Heart Healthy Traditional Filipino Recipes, American Heart Association, 1999.\n6\nCantaloupe Crush\nTry this refreshing, heart healthy drink that uses fresh fruit, fat-free milk, and low\namounts of sweetener. Children and adults alike will love it!\nIngredients Directions\n• ½ cantaloupe 1. Cut cantaloupe into small\ncubes or thin strips.\n• 1 cup fat-free milk\n2. Mix cantaloupe, milk, a",
        "nd\n• 1½ cups ice\nice in a blender until smooth.\n• Sweetener, as needed\n3. Sweeten to taste.\n(about 1 to 2 teaspoons\nsugar or equivalent of\nanother sweetener)\nNutrition Information\nMakes 4 servings\nServing size: ½ cup\nEach serving provides:\nCalories: 50\nTotal Fat: 0 g\nSaturated Fat: 0 g\nCholesterol: 0 mg\nSodium: 40 mg\nTotal Fiber: 0 g\nProtein: 3 g\nCarbohydrates: 10 g\nPotassium: 280 mg\nSource: Adapted from the National Cancer Institute and InteliHealth (intelihealth.com), 2013.\n7\nVegetable Kare-Kare (Peanut Stew)\nThis version of vegetable kare-kare is healthier than the traditional Filipino dish\nbecause it has no cholesterol. It uses gluten instead of oxtail or other meat. It is also\npacked with vegetables and made complete with a nutty, low-sodium sauce.\nIngredients\n• 1 ¼ cup gluten or seitan,* cubes\n• 2 tablespoons corn oil\n• 2 cloves garlic, crushed\nNutrition Information\n• 1 onion, medium, sliced\n• ½ cup ground peanuts Makes 6 servings\n• ¼ cup ground toasted rice**\nEach serving provid",
        "es:\n• ¼ teaspoon salt\nCalories: 300\n• 1 cup eggplant, sliced\nTotal Fat: 12 g\n• ½ cup string beans, sliced Saturated Fat: 1.5 g\n• ⅔ cup banana heart or bud Cholesterol: 0 mg\nSodium: 125 mg\n• ½ cup bok choy (pechay), sliced\nTotal Fiber: 4 g\n* Gluten is made from protein that is in a variety of grains, such Protein: 36 g\nas wheat and rye, and is mixed and kneaded with water. Seitan\nCarbohydrates: 20 g\nis a form of wheat gluten. It is sold as strips or in cans at health\nPotassium: 320 mg\nfood stores and Asian supermarkets.\n** To make ground, toasted rice: Place rice, ½ cup at a time,\nin a frying pan or wok and heat over moderate heat, stirring\nfrequently to keep it from burning and to allow it to develop a\nuniform, deep golden color—2 to 3 minutes. Then remove it\nfrom heat and cool to room temperature. Grind the toasted rice\ncoarsely—not finely grounded—in a blende, or spice or coffee\ngrinder.\nDirections\n1. Sauté gluten cubes in corn oil. Add garlic and onions.\n2. Pour enough water to cove",
        "r gluten, and add ground\npeanuts and ground rice to thicken.\n3. Season with salt.\n4. Add the eggplant, then string beans, then banana,\nthen bok choy (pechay) on top of the cooked gluten.\nSource: PHC Alive Diet, Division of Nutrition and Dietetics, Philippine Heart Center, East Avenue, Quezon City, Philippines, page 91.\n8\nNational Heart, Lung, and Blood Institute\nAn Initiative of the to help reduce health disparities in cardiovascular disease\nand asthma in underserved and minority communities. For more information, visit www.nhlbi.nih.gov/health/healthdisp."
      ],
      "uploaded_at": "2025-07-25T19:50:58.195599",
      "source": "pdf_upload"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "chunks": [
        "Tasty and Healthy —\nHeart Healthy Filipino Recipes\nFish Cardillo\nThis is a delicious, low-cost recipe with low-sodium ingredients.\nKeep it low-fat by not adding meat fat (lard) or other fat.\nIngredients Directions\n1. Thoroughly clean fish. Remove scale\n• 1 pound (½ kg) red snapper\nand gills, and wash thoroughly. Drain and\n• 4 teaspoons corn oil for sauté\nset aside.\n• ¼ cup flou\n2. Slice the raw fish into six pieces\n• 1 large onion, sliced\n3. Heat corn oil in frying pan.\n• 3 or 4 medium-sized tomatoes,\n4. Place the flour into a bowl or plastic bag\nchopped\nPlace the raw fish in the flour and cov\n• ½ cup egg whites, beaten the outside of each fish with flo .\n• ½ cup water 5. Sauté fish until golden brown. Set asid\n• A dash ground pepper on top of a paper towel.\n• 15 stalks green onions, chopped 6. Sauté onion and tomatoes. Add ½ cup of\nwater.\n7. Add the beaten egg whites and fish\nCover and let it simmer for 5–10 minutes.\nNutrition Information\n8. Season with ground pepper.\nMakes 6 servings",
        " 9. Sprinkle with chopped green onions.\nEach serving provides:\nCalories: 170\nTotal Fat: 4 g\nSaturated Fat: 1 g\nCholesterol: 45 mg\nQuick Tip\nSodium: 115 mg\nTotal Fiber: 3 g\nProtein: 20 g\nThis recipe is lower in salt and sodium than other\nCarbohydrates: 13 g\nCardillo recipes because it uses:\nPotassium: 600 mg\n• Fresh, not canned, tomatoes\n• Ground pepper and corn oil with no salt added\n• Fresh onion and green onions\n• Fresh, not smoked or canned, fis\nSource: Philippine Heart Center’s Healthy Heart Cookbook.\n1\nAdobong Manok (Marinated Chicken)\nThis low-cost, low-sodium recipe has great flavor that you and your\nfamily will love! It uses chicken breast which is lower in fat than\nother parts of the chicken, like the thigh and leg.\nIngredients Directions\n• 1 teaspoon olive oil 1. Combine olive oil, garlic, and onion in a frying\npan. Add chicken and sauté together until\n• 2 cloves fresh crushed garlic\nchicken has browned.\n• 2 medium chopped onions\n2. Add light soy sauce, vinegar, paprika,\n• 1 ",
        "pound (½ kg) chicken breasts, no skin\nblack pepper, and bay leaf. Stir.\n• 2 tablespoons light soy sauce\n3. Bring to a boil. Simmer for 45–60 minutes or\n• ¼ cup vinegar until chicken is done.\n• 1 teaspoon paprika 4. Remove the chicken and save the liquid in the\npot. Arrange the chicken on a broiler pan. Broil\n• 2 tablespoons ground black pepper\nuntil the chicken has nicely browned. Remove\n• 1 bay leaf, broken in half\nfrom the broiler and place it in a serving bowl.\n• 1 medium red tomato (optional)\n5. Continue to boil the sauce in the uncovered\npan until volume is reduced to about half and\nthe sauce is thick.\nNutrition Information\n6. Pour the thickened sauce over broiled\nadobong (chicken) and garnish with red\nMakes 4 servings\ntomatoes, if desired.\nServing size: ½ cup\nEach serving provides:\nCalories: 190\nTotal Fat: 5 g\nQuick Tip\nSaturated Fat: 1 g\nCholesterol: 70 mg\nSodium: 330 mg This recipe is lower in saturated fat and cholesterol because:\nTotal Fiber: 2 g\n• It is made using chicken wi",
        "thout the skin and any extra fat\nProtein: 26 g\nis removed.\nCarbohydrates: 10 g\nPotassium: 370 mg • Only 1 teaspoon of unsaturated fat (olive oil) is added.\n• It is flavored with vegetables and herbs and is boiled an\nbroiled slowly in moist heat instead of fat.\nSource: Filipino-American Nutrition and Fitness Teacher’s\nGuide, Kalusugan Community Services, San Diego, CA.\n2\nLumpiang Sariwa (Fresh Lumpia)\nYou and your family will love this tasty recipe. The ingredients—ground chicken\nor pork, olive oil, peanuts, and fresh herbs and spices—add flavor. Also, the\nlumpiang sariwa is served fresh so it has fewer calories than fried lumpiang.\nIngredients Directions\n• ½ cup ground chicken or lean pork 1. Heat oil, and sauté ground meat with the\nshrimp and garlic.\n• ½ cup shrimp, cleaned and deveined\n2. Add vegetables until slightly crisp. Pour in\n• 1 tablespoon olive oil\nthe chicken broth until cooked.\n• 2 cloves chopped garlic\n3. Season with salt and pepper.\n• ½ cup cabbage, julienned\n4. Set asid",
        "e and drain in a colander.\n• ½ cup green beans, julienned\n5. Save the broth for the lumpia sauce.\n• ½ cup carrots, julienned\n6. Soak the Vietnamese spring roll wrappers\n• ¼ cup celery, julienned\none at a time in water until soft and\n• ¼ cup jicama, julienned (may substitute transparent. Dry immediately with a paper\nchestnuts) towel.\n• ½ cup chicken broth 7. Lay the lettuce on the wrapper.\n• ¼ teaspoon salt 8. Place 2 tablespoons of the vegetable\n• ¼ teaspoon pepper mixture on the wrapper.\n• 8 Vietnamese spring-roll wrappers or 9. Fold in one side of the wrapper and roll\nlumpia wrappers tightly.\n• 8 pieces red leaf lettuce 10. Serve with lumpia sauce on top.\nSprinkle with chopped peanuts.\n• ⅓ cup peanuts, dry, roasted, and chopped\nNutrition Information\nLumpia Sauce\nMakes 8 servings\n• 1 cup broth from the sautéed vegetables Serving size: 1 lumpia\n• 1 tablespoon light soy sauce\nEach serving provides:\n• 1 tablespoon brown sugar\nCalories: 160\n• 3 cloves garlic, minced\nTotal Fat: 4 g\n• 1 tea",
        "spoon cornstarch Saturated Fat: 0.5 g\n• 2 tablespoons cold water for mixing cornstarch Cholesterol: 55 mg\nSodium: 150 mg\nDirections Total Fiber: 2 g\n1. Mix together vegetable broth, soy sauce, brown Protein: 10 g\nsugar, and garlic, and bring to a boil. Carbohydrates: 21 g\nPotassium: 170 mg\n2. Mix the cornstarch in 2 tablespoons of cold water.\n3. Slowly add the cornstarch mixture to the broth.\nStir until sauce thickens.\nSource: Mula sa Puso, Heart Healthy Traditional\nFilipino Recipes, American Heart Association, 1999.\n3\nPesang Isda (Fish Simmered With\nGinger and Tomatoes)\nThis main dish is heart healthy because the fish is simmered in water,\nnot fried, and no fat is added. Flavoring comes from the herbs and spices\ninstead of sauces that are high in sodium.\nIngredients Directions\n1. In a 4-quart saucepan,\n• ¼ cup fresh ginger, thinly\nsimmer sliced ginger,\nsliced (about 2 inches long)\ntomatoes, and onions in\n• 1 cup ripe tomatoes, chopped\n4 cups of water over medium\n• 1 cup white or yello",
        "w onions, heat until onions are tender\nthinly sliced (about 7 to 8 minutes).\n• 4 cups water 2. Reduce heat to low, add fish\n• 2 pounds fleshy fish (c and poach gently until almost\nfillet, halibut steak, or trout done (about 3 to 4 minutes).\n• 2 cups pechay (bok choy) 3. Add pechay stems, salt, and\nstems and leaves, cut up ground pepper. Cook for\nseparately 1 minute; then add pechay\nleaves and green onions.\n• ½ teaspoon salt\nCook another 30 seconds.\n• ½ teaspoon ground pepper\n4. Serve immediately.\n• 1 cup green onions, sliced\nNutrition Information\nMakes 6 servings\nServing size: 3 ounces lean\nfish and ½ cup vegetables\nEach serving provides:\nCalories: 160\nTotal Fat: 2 g\nSaturated Fat: 0.5 g\nCholesterol: 80 mg\nSodium: 340 mg\nTotal Fiber: 2 g\nProtein: 30 g\nCarbohydrates: 6 g\nPotassium: 630 mg\nSource: Filipino American Food Practices, Customs, and Holidays, American Dietetic Association, 1994.\n4\nMunggo Guisado (Sautéed Mung Beans)\nYour family will love this heart healthy side dish. It is mad",
        "e with vegetables,\nseafood, lean meat, and a small amount of corn oil. The pork is slowly\nsimmered in moist heat with vegetables and mung beans, creating flavors\nthat will make your taste buds jump for joy!\nIngredients Directions\n• 1 tablespoon corn oil 1. In a skillet, heat oil, and sauté\ncrushed garlic until lightly brown.\n• 2 cloves fresh garlic, crushed (or\n1 tablespoon, minced) 2. Add onion and tomatoes. Sauté\nuntil skin begins to curl.\n• 1 cup white onions, chopped\n3. Add pork, and sauté until lightly\n• 1 cup ripe tomatoes, chopped\nbrown.\n• 1 cup (4 ounces) lean pork, thinly\n4. Add water, and simmer pork for\nsliced\nabout 15 minutes.\n• 4 cups water\n5. Add the sautéed mix to mung\n• 3½ cups precooked mung beans\nbeans, and continue to simmer\n(from 1¾ cups dry beans)*\n15 minutes.\n• 1 teaspoon salt\n6. Season with salt and ground\n• 1 teaspoon ground pepper\npepper.\n• 1 cup (4 ounces) shrimp, peeled\n7. Add peeled shrimp.\nand deveined\n8. Add frozen leaf spinach, and\n• 1 cup (about ⅔ of a 1",
        "0-ounce\ncook 4 minutes until done.\npackage) leaf spinach, frozen\n* To cook dry, uncooked mung beans:\nWash and boil the uncooked mung\nbeans in a large saucepan, using 6 Nutrition Information\ncups of water. Cook until tender, about\n1½ to 2 hours. Drain. Makes 8 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 160\nTotal Fat: 3.5 g\nSaturated Fat: 1 g\nCholesterol: 35 mg\nSodium: 350 mg\nSource: Filipino American Food Practices, Customs,\nand Holidays, American Dietetic Association, 1994. Total Fiber: 8 g\nProtein: 13 g\nCarbohydrates: 19 g\nPotassium: 370 mg\n5\nAmpalaya (Bitter Melon) With Pork\nThis recipe is lower in fat and sodium than a typical ampalaya dish\nbecause it uses lean meat that is sautéed and simmered instead of fried.\nIngredients Directions\n• 1 cup onion, chopped 1. Using a large skillet, lightly sauté\nonions and garlic in hot olive oil.\n• 6 cloves garlic, crushed\n2. Add the ground pork and cook until\n• 1 tablespoon olive oil\nalmost done.\n• ½ pound (0.2 kg) lean ground ",
        "pork\n3. Add the sliced bitter melon.\n• 2 cups Ampalaya*, sliced\n4. Cover and simmer until bitter melon\n• 2 teaspoons light soy sauce\nturns green. Do not overcook.\n• ½ teaspoon black pepper\n5. Season with light soy sauce and\nblack pepper.\n* Ampalaya (bitter melon) is a fruit that is\noblong, cylindrical, pointed at both ends,\nribbed, and wrinkled.\nNutrition Information\nMakes 4 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 150\nTotal Fat: 6 g\nSaturated Fat: 1.5 g\nCholesterol: 45 mg\nSodium: 200 mg\nTotal Fiber: 1 g\nProtein: 17 g\nCarbohydrates: 7 g\nPotassium: 600 mg\nSource: Adapted from Mula Sa Puso, Heart Healthy Traditional Filipino Recipes, American Heart Association, 1999.\n6\nCantaloupe Crush\nTry this refreshing, heart healthy drink that uses fresh fruit, fat-free milk, and low\namounts of sweetener. Children and adults alike will love it!\nIngredients Directions\n• ½ cantaloupe 1. Cut cantaloupe into small\ncubes or thin strips.\n• 1 cup fat-free milk\n2. Mix cantaloupe, milk, a",
        "nd\n• 1½ cups ice\nice in a blender until smooth.\n• Sweetener, as needed\n3. Sweeten to taste.\n(about 1 to 2 teaspoons\nsugar or equivalent of\nanother sweetener)\nNutrition Information\nMakes 4 servings\nServing size: ½ cup\nEach serving provides:\nCalories: 50\nTotal Fat: 0 g\nSaturated Fat: 0 g\nCholesterol: 0 mg\nSodium: 40 mg\nTotal Fiber: 0 g\nProtein: 3 g\nCarbohydrates: 10 g\nPotassium: 280 mg\nSource: Adapted from the National Cancer Institute and InteliHealth (intelihealth.com), 2013.\n7\nVegetable Kare-Kare (Peanut Stew)\nThis version of vegetable kare-kare is healthier than the traditional Filipino dish\nbecause it has no cholesterol. It uses gluten instead of oxtail or other meat. It is also\npacked with vegetables and made complete with a nutty, low-sodium sauce.\nIngredients\n• 1 ¼ cup gluten or seitan,* cubes\n• 2 tablespoons corn oil\n• 2 cloves garlic, crushed\nNutrition Information\n• 1 onion, medium, sliced\n• ½ cup ground peanuts Makes 6 servings\n• ¼ cup ground toasted rice**\nEach serving provid",
        "es:\n• ¼ teaspoon salt\nCalories: 300\n• 1 cup eggplant, sliced\nTotal Fat: 12 g\n• ½ cup string beans, sliced Saturated Fat: 1.5 g\n• ⅔ cup banana heart or bud Cholesterol: 0 mg\nSodium: 125 mg\n• ½ cup bok choy (pechay), sliced\nTotal Fiber: 4 g\n* Gluten is made from protein that is in a variety of grains, such Protein: 36 g\nas wheat and rye, and is mixed and kneaded with water. Seitan\nCarbohydrates: 20 g\nis a form of wheat gluten. It is sold as strips or in cans at health\nPotassium: 320 mg\nfood stores and Asian supermarkets.\n** To make ground, toasted rice: Place rice, ½ cup at a time,\nin a frying pan or wok and heat over moderate heat, stirring\nfrequently to keep it from burning and to allow it to develop a\nuniform, deep golden color—2 to 3 minutes. Then remove it\nfrom heat and cool to room temperature. Grind the toasted rice\ncoarsely—not finely grounded—in a blende, or spice or coffee\ngrinder.\nDirections\n1. Sauté gluten cubes in corn oil. Add garlic and onions.\n2. Pour enough water to cove",
        "r gluten, and add ground\npeanuts and ground rice to thicken.\n3. Season with salt.\n4. Add the eggplant, then string beans, then banana,\nthen bok choy (pechay) on top of the cooked gluten.\nSource: PHC Alive Diet, Division of Nutrition and Dietetics, Philippine Heart Center, East Avenue, Quezon City, Philippines, page 91.\n8\nNational Heart, Lung, and Blood Institute\nAn Initiative of the to help reduce health disparities in cardiovascular disease\nand asthma in underserved and minority communities. For more information, visit www.nhlbi.nih.gov/health/healthdisp."
      ],
      "uploaded_at": "2025-07-25T19:51:03.032347",
      "source": "pdf_upload"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "chunks": [
        "Tasty and Healthy —\nHeart Healthy Filipino Recipes\nFish Cardillo\nThis is a delicious, low-cost recipe with low-sodium ingredients.\nKeep it low-fat by not adding meat fat (lard) or other fat.\nIngredients Directions\n1. Thoroughly clean fish. Remove scale\n• 1 pound (½ kg) red snapper\nand gills, and wash thoroughly. Drain and\n• 4 teaspoons corn oil for sauté\nset aside.\n• ¼ cup flou\n2. Slice the raw fish into six pieces\n• 1 large onion, sliced\n3. Heat corn oil in frying pan.\n• 3 or 4 medium-sized tomatoes,\n4. Place the flour into a bowl or plastic bag\nchopped\nPlace the raw fish in the flour and cov\n• ½ cup egg whites, beaten the outside of each fish with flo .\n• ½ cup water 5. Sauté fish until golden brown. Set asid\n• A dash ground pepper on top of a paper towel.\n• 15 stalks green onions, chopped 6. Sauté onion and tomatoes. Add ½ cup of\nwater.\n7. Add the beaten egg whites and fish\nCover and let it simmer for 5–10 minutes.\nNutrition Information\n8. Season with ground pepper.\nMakes 6 servings",
        " 9. Sprinkle with chopped green onions.\nEach serving provides:\nCalories: 170\nTotal Fat: 4 g\nSaturated Fat: 1 g\nCholesterol: 45 mg\nQuick Tip\nSodium: 115 mg\nTotal Fiber: 3 g\nProtein: 20 g\nThis recipe is lower in salt and sodium than other\nCarbohydrates: 13 g\nCardillo recipes because it uses:\nPotassium: 600 mg\n• Fresh, not canned, tomatoes\n• Ground pepper and corn oil with no salt added\n• Fresh onion and green onions\n• Fresh, not smoked or canned, fis\nSource: Philippine Heart Center’s Healthy Heart Cookbook.\n1\nAdobong Manok (Marinated Chicken)\nThis low-cost, low-sodium recipe has great flavor that you and your\nfamily will love! It uses chicken breast which is lower in fat than\nother parts of the chicken, like the thigh and leg.\nIngredients Directions\n• 1 teaspoon olive oil 1. Combine olive oil, garlic, and onion in a frying\npan. Add chicken and sauté together until\n• 2 cloves fresh crushed garlic\nchicken has browned.\n• 2 medium chopped onions\n2. Add light soy sauce, vinegar, paprika,\n• 1 ",
        "pound (½ kg) chicken breasts, no skin\nblack pepper, and bay leaf. Stir.\n• 2 tablespoons light soy sauce\n3. Bring to a boil. Simmer for 45–60 minutes or\n• ¼ cup vinegar until chicken is done.\n• 1 teaspoon paprika 4. Remove the chicken and save the liquid in the\npot. Arrange the chicken on a broiler pan. Broil\n• 2 tablespoons ground black pepper\nuntil the chicken has nicely browned. Remove\n• 1 bay leaf, broken in half\nfrom the broiler and place it in a serving bowl.\n• 1 medium red tomato (optional)\n5. Continue to boil the sauce in the uncovered\npan until volume is reduced to about half and\nthe sauce is thick.\nNutrition Information\n6. Pour the thickened sauce over broiled\nadobong (chicken) and garnish with red\nMakes 4 servings\ntomatoes, if desired.\nServing size: ½ cup\nEach serving provides:\nCalories: 190\nTotal Fat: 5 g\nQuick Tip\nSaturated Fat: 1 g\nCholesterol: 70 mg\nSodium: 330 mg This recipe is lower in saturated fat and cholesterol because:\nTotal Fiber: 2 g\n• It is made using chicken wi",
        "thout the skin and any extra fat\nProtein: 26 g\nis removed.\nCarbohydrates: 10 g\nPotassium: 370 mg • Only 1 teaspoon of unsaturated fat (olive oil) is added.\n• It is flavored with vegetables and herbs and is boiled an\nbroiled slowly in moist heat instead of fat.\nSource: Filipino-American Nutrition and Fitness Teacher’s\nGuide, Kalusugan Community Services, San Diego, CA.\n2\nLumpiang Sariwa (Fresh Lumpia)\nYou and your family will love this tasty recipe. The ingredients—ground chicken\nor pork, olive oil, peanuts, and fresh herbs and spices—add flavor. Also, the\nlumpiang sariwa is served fresh so it has fewer calories than fried lumpiang.\nIngredients Directions\n• ½ cup ground chicken or lean pork 1. Heat oil, and sauté ground meat with the\nshrimp and garlic.\n• ½ cup shrimp, cleaned and deveined\n2. Add vegetables until slightly crisp. Pour in\n• 1 tablespoon olive oil\nthe chicken broth until cooked.\n• 2 cloves chopped garlic\n3. Season with salt and pepper.\n• ½ cup cabbage, julienned\n4. Set asid",
        "e and drain in a colander.\n• ½ cup green beans, julienned\n5. Save the broth for the lumpia sauce.\n• ½ cup carrots, julienned\n6. Soak the Vietnamese spring roll wrappers\n• ¼ cup celery, julienned\none at a time in water until soft and\n• ¼ cup jicama, julienned (may substitute transparent. Dry immediately with a paper\nchestnuts) towel.\n• ½ cup chicken broth 7. Lay the lettuce on the wrapper.\n• ¼ teaspoon salt 8. Place 2 tablespoons of the vegetable\n• ¼ teaspoon pepper mixture on the wrapper.\n• 8 Vietnamese spring-roll wrappers or 9. Fold in one side of the wrapper and roll\nlumpia wrappers tightly.\n• 8 pieces red leaf lettuce 10. Serve with lumpia sauce on top.\nSprinkle with chopped peanuts.\n• ⅓ cup peanuts, dry, roasted, and chopped\nNutrition Information\nLumpia Sauce\nMakes 8 servings\n• 1 cup broth from the sautéed vegetables Serving size: 1 lumpia\n• 1 tablespoon light soy sauce\nEach serving provides:\n• 1 tablespoon brown sugar\nCalories: 160\n• 3 cloves garlic, minced\nTotal Fat: 4 g\n• 1 tea",
        "spoon cornstarch Saturated Fat: 0.5 g\n• 2 tablespoons cold water for mixing cornstarch Cholesterol: 55 mg\nSodium: 150 mg\nDirections Total Fiber: 2 g\n1. Mix together vegetable broth, soy sauce, brown Protein: 10 g\nsugar, and garlic, and bring to a boil. Carbohydrates: 21 g\nPotassium: 170 mg\n2. Mix the cornstarch in 2 tablespoons of cold water.\n3. Slowly add the cornstarch mixture to the broth.\nStir until sauce thickens.\nSource: Mula sa Puso, Heart Healthy Traditional\nFilipino Recipes, American Heart Association, 1999.\n3\nPesang Isda (Fish Simmered With\nGinger and Tomatoes)\nThis main dish is heart healthy because the fish is simmered in water,\nnot fried, and no fat is added. Flavoring comes from the herbs and spices\ninstead of sauces that are high in sodium.\nIngredients Directions\n1. In a 4-quart saucepan,\n• ¼ cup fresh ginger, thinly\nsimmer sliced ginger,\nsliced (about 2 inches long)\ntomatoes, and onions in\n• 1 cup ripe tomatoes, chopped\n4 cups of water over medium\n• 1 cup white or yello",
        "w onions, heat until onions are tender\nthinly sliced (about 7 to 8 minutes).\n• 4 cups water 2. Reduce heat to low, add fish\n• 2 pounds fleshy fish (c and poach gently until almost\nfillet, halibut steak, or trout done (about 3 to 4 minutes).\n• 2 cups pechay (bok choy) 3. Add pechay stems, salt, and\nstems and leaves, cut up ground pepper. Cook for\nseparately 1 minute; then add pechay\nleaves and green onions.\n• ½ teaspoon salt\nCook another 30 seconds.\n• ½ teaspoon ground pepper\n4. Serve immediately.\n• 1 cup green onions, sliced\nNutrition Information\nMakes 6 servings\nServing size: 3 ounces lean\nfish and ½ cup vegetables\nEach serving provides:\nCalories: 160\nTotal Fat: 2 g\nSaturated Fat: 0.5 g\nCholesterol: 80 mg\nSodium: 340 mg\nTotal Fiber: 2 g\nProtein: 30 g\nCarbohydrates: 6 g\nPotassium: 630 mg\nSource: Filipino American Food Practices, Customs, and Holidays, American Dietetic Association, 1994.\n4\nMunggo Guisado (Sautéed Mung Beans)\nYour family will love this heart healthy side dish. It is mad",
        "e with vegetables,\nseafood, lean meat, and a small amount of corn oil. The pork is slowly\nsimmered in moist heat with vegetables and mung beans, creating flavors\nthat will make your taste buds jump for joy!\nIngredients Directions\n• 1 tablespoon corn oil 1. In a skillet, heat oil, and sauté\ncrushed garlic until lightly brown.\n• 2 cloves fresh garlic, crushed (or\n1 tablespoon, minced) 2. Add onion and tomatoes. Sauté\nuntil skin begins to curl.\n• 1 cup white onions, chopped\n3. Add pork, and sauté until lightly\n• 1 cup ripe tomatoes, chopped\nbrown.\n• 1 cup (4 ounces) lean pork, thinly\n4. Add water, and simmer pork for\nsliced\nabout 15 minutes.\n• 4 cups water\n5. Add the sautéed mix to mung\n• 3½ cups precooked mung beans\nbeans, and continue to simmer\n(from 1¾ cups dry beans)*\n15 minutes.\n• 1 teaspoon salt\n6. Season with salt and ground\n• 1 teaspoon ground pepper\npepper.\n• 1 cup (4 ounces) shrimp, peeled\n7. Add peeled shrimp.\nand deveined\n8. Add frozen leaf spinach, and\n• 1 cup (about ⅔ of a 1",
        "0-ounce\ncook 4 minutes until done.\npackage) leaf spinach, frozen\n* To cook dry, uncooked mung beans:\nWash and boil the uncooked mung\nbeans in a large saucepan, using 6 Nutrition Information\ncups of water. Cook until tender, about\n1½ to 2 hours. Drain. Makes 8 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 160\nTotal Fat: 3.5 g\nSaturated Fat: 1 g\nCholesterol: 35 mg\nSodium: 350 mg\nSource: Filipino American Food Practices, Customs,\nand Holidays, American Dietetic Association, 1994. Total Fiber: 8 g\nProtein: 13 g\nCarbohydrates: 19 g\nPotassium: 370 mg\n5\nAmpalaya (Bitter Melon) With Pork\nThis recipe is lower in fat and sodium than a typical ampalaya dish\nbecause it uses lean meat that is sautéed and simmered instead of fried.\nIngredients Directions\n• 1 cup onion, chopped 1. Using a large skillet, lightly sauté\nonions and garlic in hot olive oil.\n• 6 cloves garlic, crushed\n2. Add the ground pork and cook until\n• 1 tablespoon olive oil\nalmost done.\n• ½ pound (0.2 kg) lean ground ",
        "pork\n3. Add the sliced bitter melon.\n• 2 cups Ampalaya*, sliced\n4. Cover and simmer until bitter melon\n• 2 teaspoons light soy sauce\nturns green. Do not overcook.\n• ½ teaspoon black pepper\n5. Season with light soy sauce and\nblack pepper.\n* Ampalaya (bitter melon) is a fruit that is\noblong, cylindrical, pointed at both ends,\nribbed, and wrinkled.\nNutrition Information\nMakes 4 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 150\nTotal Fat: 6 g\nSaturated Fat: 1.5 g\nCholesterol: 45 mg\nSodium: 200 mg\nTotal Fiber: 1 g\nProtein: 17 g\nCarbohydrates: 7 g\nPotassium: 600 mg\nSource: Adapted from Mula Sa Puso, Heart Healthy Traditional Filipino Recipes, American Heart Association, 1999.\n6\nCantaloupe Crush\nTry this refreshing, heart healthy drink that uses fresh fruit, fat-free milk, and low\namounts of sweetener. Children and adults alike will love it!\nIngredients Directions\n• ½ cantaloupe 1. Cut cantaloupe into small\ncubes or thin strips.\n• 1 cup fat-free milk\n2. Mix cantaloupe, milk, a",
        "nd\n• 1½ cups ice\nice in a blender until smooth.\n• Sweetener, as needed\n3. Sweeten to taste.\n(about 1 to 2 teaspoons\nsugar or equivalent of\nanother sweetener)\nNutrition Information\nMakes 4 servings\nServing size: ½ cup\nEach serving provides:\nCalories: 50\nTotal Fat: 0 g\nSaturated Fat: 0 g\nCholesterol: 0 mg\nSodium: 40 mg\nTotal Fiber: 0 g\nProtein: 3 g\nCarbohydrates: 10 g\nPotassium: 280 mg\nSource: Adapted from the National Cancer Institute and InteliHealth (intelihealth.com), 2013.\n7\nVegetable Kare-Kare (Peanut Stew)\nThis version of vegetable kare-kare is healthier than the traditional Filipino dish\nbecause it has no cholesterol. It uses gluten instead of oxtail or other meat. It is also\npacked with vegetables and made complete with a nutty, low-sodium sauce.\nIngredients\n• 1 ¼ cup gluten or seitan,* cubes\n• 2 tablespoons corn oil\n• 2 cloves garlic, crushed\nNutrition Information\n• 1 onion, medium, sliced\n• ½ cup ground peanuts Makes 6 servings\n• ¼ cup ground toasted rice**\nEach serving provid",
        "es:\n• ¼ teaspoon salt\nCalories: 300\n• 1 cup eggplant, sliced\nTotal Fat: 12 g\n• ½ cup string beans, sliced Saturated Fat: 1.5 g\n• ⅔ cup banana heart or bud Cholesterol: 0 mg\nSodium: 125 mg\n• ½ cup bok choy (pechay), sliced\nTotal Fiber: 4 g\n* Gluten is made from protein that is in a variety of grains, such Protein: 36 g\nas wheat and rye, and is mixed and kneaded with water. Seitan\nCarbohydrates: 20 g\nis a form of wheat gluten. It is sold as strips or in cans at health\nPotassium: 320 mg\nfood stores and Asian supermarkets.\n** To make ground, toasted rice: Place rice, ½ cup at a time,\nin a frying pan or wok and heat over moderate heat, stirring\nfrequently to keep it from burning and to allow it to develop a\nuniform, deep golden color—2 to 3 minutes. Then remove it\nfrom heat and cool to room temperature. Grind the toasted rice\ncoarsely—not finely grounded—in a blende, or spice or coffee\ngrinder.\nDirections\n1. Sauté gluten cubes in corn oil. Add garlic and onions.\n2. Pour enough water to cove",
        "r gluten, and add ground\npeanuts and ground rice to thicken.\n3. Season with salt.\n4. Add the eggplant, then string beans, then banana,\nthen bok choy (pechay) on top of the cooked gluten.\nSource: PHC Alive Diet, Division of Nutrition and Dietetics, Philippine Heart Center, East Avenue, Quezon City, Philippines, page 91.\n8\nNational Heart, Lung, and Blood Institute\nAn Initiative of the to help reduce health disparities in cardiovascular disease\nand asthma in underserved and minority communities. For more information, visit www.nhlbi.nih.gov/health/healthdisp."
      ],
      "uploaded_at": "2025-07-25T19:51:07.594307",
      "source": "pdf_upload"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "chunks": [
        "Tasty and Healthy —\nHeart Healthy Filipino Recipes\nFish Cardillo\nThis is a delicious, low-cost recipe with low-sodium ingredients.\nKeep it low-fat by not adding meat fat (lard) or other fat.\nIngredients Directions\n1. Thoroughly clean fish. Remove scale\n• 1 pound (½ kg) red snapper\nand gills, and wash thoroughly. Drain and\n• 4 teaspoons corn oil for sauté\nset aside.\n• ¼ cup flou\n2. Slice the raw fish into six pieces\n• 1 large onion, sliced\n3. Heat corn oil in frying pan.\n• 3 or 4 medium-sized tomatoes,\n4. Place the flour into a bowl or plastic bag\nchopped\nPlace the raw fish in the flour and cov\n• ½ cup egg whites, beaten the outside of each fish with flo .\n• ½ cup water 5. Sauté fish until golden brown. Set asid\n• A dash ground pepper on top of a paper towel.\n• 15 stalks green onions, chopped 6. Sauté onion and tomatoes. Add ½ cup of\nwater.\n7. Add the beaten egg whites and fish\nCover and let it simmer for 5–10 minutes.\nNutrition Information\n8. Season with ground pepper.\nMakes 6 servings",
        " 9. Sprinkle with chopped green onions.\nEach serving provides:\nCalories: 170\nTotal Fat: 4 g\nSaturated Fat: 1 g\nCholesterol: 45 mg\nQuick Tip\nSodium: 115 mg\nTotal Fiber: 3 g\nProtein: 20 g\nThis recipe is lower in salt and sodium than other\nCarbohydrates: 13 g\nCardillo recipes because it uses:\nPotassium: 600 mg\n• Fresh, not canned, tomatoes\n• Ground pepper and corn oil with no salt added\n• Fresh onion and green onions\n• Fresh, not smoked or canned, fis\nSource: Philippine Heart Center’s Healthy Heart Cookbook.\n1\nAdobong Manok (Marinated Chicken)\nThis low-cost, low-sodium recipe has great flavor that you and your\nfamily will love! It uses chicken breast which is lower in fat than\nother parts of the chicken, like the thigh and leg.\nIngredients Directions\n• 1 teaspoon olive oil 1. Combine olive oil, garlic, and onion in a frying\npan. Add chicken and sauté together until\n• 2 cloves fresh crushed garlic\nchicken has browned.\n• 2 medium chopped onions\n2. Add light soy sauce, vinegar, paprika,\n• 1 ",
        "pound (½ kg) chicken breasts, no skin\nblack pepper, and bay leaf. Stir.\n• 2 tablespoons light soy sauce\n3. Bring to a boil. Simmer for 45–60 minutes or\n• ¼ cup vinegar until chicken is done.\n• 1 teaspoon paprika 4. Remove the chicken and save the liquid in the\npot. Arrange the chicken on a broiler pan. Broil\n• 2 tablespoons ground black pepper\nuntil the chicken has nicely browned. Remove\n• 1 bay leaf, broken in half\nfrom the broiler and place it in a serving bowl.\n• 1 medium red tomato (optional)\n5. Continue to boil the sauce in the uncovered\npan until volume is reduced to about half and\nthe sauce is thick.\nNutrition Information\n6. Pour the thickened sauce over broiled\nadobong (chicken) and garnish with red\nMakes 4 servings\ntomatoes, if desired.\nServing size: ½ cup\nEach serving provides:\nCalories: 190\nTotal Fat: 5 g\nQuick Tip\nSaturated Fat: 1 g\nCholesterol: 70 mg\nSodium: 330 mg This recipe is lower in saturated fat and cholesterol because:\nTotal Fiber: 2 g\n• It is made using chicken wi",
        "thout the skin and any extra fat\nProtein: 26 g\nis removed.\nCarbohydrates: 10 g\nPotassium: 370 mg • Only 1 teaspoon of unsaturated fat (olive oil) is added.\n• It is flavored with vegetables and herbs and is boiled an\nbroiled slowly in moist heat instead of fat.\nSource: Filipino-American Nutrition and Fitness Teacher’s\nGuide, Kalusugan Community Services, San Diego, CA.\n2\nLumpiang Sariwa (Fresh Lumpia)\nYou and your family will love this tasty recipe. The ingredients—ground chicken\nor pork, olive oil, peanuts, and fresh herbs and spices—add flavor. Also, the\nlumpiang sariwa is served fresh so it has fewer calories than fried lumpiang.\nIngredients Directions\n• ½ cup ground chicken or lean pork 1. Heat oil, and sauté ground meat with the\nshrimp and garlic.\n• ½ cup shrimp, cleaned and deveined\n2. Add vegetables until slightly crisp. Pour in\n• 1 tablespoon olive oil\nthe chicken broth until cooked.\n• 2 cloves chopped garlic\n3. Season with salt and pepper.\n• ½ cup cabbage, julienned\n4. Set asid",
        "e and drain in a colander.\n• ½ cup green beans, julienned\n5. Save the broth for the lumpia sauce.\n• ½ cup carrots, julienned\n6. Soak the Vietnamese spring roll wrappers\n• ¼ cup celery, julienned\none at a time in water until soft and\n• ¼ cup jicama, julienned (may substitute transparent. Dry immediately with a paper\nchestnuts) towel.\n• ½ cup chicken broth 7. Lay the lettuce on the wrapper.\n• ¼ teaspoon salt 8. Place 2 tablespoons of the vegetable\n• ¼ teaspoon pepper mixture on the wrapper.\n• 8 Vietnamese spring-roll wrappers or 9. Fold in one side of the wrapper and roll\nlumpia wrappers tightly.\n• 8 pieces red leaf lettuce 10. Serve with lumpia sauce on top.\nSprinkle with chopped peanuts.\n• ⅓ cup peanuts, dry, roasted, and chopped\nNutrition Information\nLumpia Sauce\nMakes 8 servings\n• 1 cup broth from the sautéed vegetables Serving size: 1 lumpia\n• 1 tablespoon light soy sauce\nEach serving provides:\n• 1 tablespoon brown sugar\nCalories: 160\n• 3 cloves garlic, minced\nTotal Fat: 4 g\n• 1 tea",
        "spoon cornstarch Saturated Fat: 0.5 g\n• 2 tablespoons cold water for mixing cornstarch Cholesterol: 55 mg\nSodium: 150 mg\nDirections Total Fiber: 2 g\n1. Mix together vegetable broth, soy sauce, brown Protein: 10 g\nsugar, and garlic, and bring to a boil. Carbohydrates: 21 g\nPotassium: 170 mg\n2. Mix the cornstarch in 2 tablespoons of cold water.\n3. Slowly add the cornstarch mixture to the broth.\nStir until sauce thickens.\nSource: Mula sa Puso, Heart Healthy Traditional\nFilipino Recipes, American Heart Association, 1999.\n3\nPesang Isda (Fish Simmered With\nGinger and Tomatoes)\nThis main dish is heart healthy because the fish is simmered in water,\nnot fried, and no fat is added. Flavoring comes from the herbs and spices\ninstead of sauces that are high in sodium.\nIngredients Directions\n1. In a 4-quart saucepan,\n• ¼ cup fresh ginger, thinly\nsimmer sliced ginger,\nsliced (about 2 inches long)\ntomatoes, and onions in\n• 1 cup ripe tomatoes, chopped\n4 cups of water over medium\n• 1 cup white or yello",
        "w onions, heat until onions are tender\nthinly sliced (about 7 to 8 minutes).\n• 4 cups water 2. Reduce heat to low, add fish\n• 2 pounds fleshy fish (c and poach gently until almost\nfillet, halibut steak, or trout done (about 3 to 4 minutes).\n• 2 cups pechay (bok choy) 3. Add pechay stems, salt, and\nstems and leaves, cut up ground pepper. Cook for\nseparately 1 minute; then add pechay\nleaves and green onions.\n• ½ teaspoon salt\nCook another 30 seconds.\n• ½ teaspoon ground pepper\n4. Serve immediately.\n• 1 cup green onions, sliced\nNutrition Information\nMakes 6 servings\nServing size: 3 ounces lean\nfish and ½ cup vegetables\nEach serving provides:\nCalories: 160\nTotal Fat: 2 g\nSaturated Fat: 0.5 g\nCholesterol: 80 mg\nSodium: 340 mg\nTotal Fiber: 2 g\nProtein: 30 g\nCarbohydrates: 6 g\nPotassium: 630 mg\nSource: Filipino American Food Practices, Customs, and Holidays, American Dietetic Association, 1994.\n4\nMunggo Guisado (Sautéed Mung Beans)\nYour family will love this heart healthy side dish. It is mad",
        "e with vegetables,\nseafood, lean meat, and a small amount of corn oil. The pork is slowly\nsimmered in moist heat with vegetables and mung beans, creating flavors\nthat will make your taste buds jump for joy!\nIngredients Directions\n• 1 tablespoon corn oil 1. In a skillet, heat oil, and sauté\ncrushed garlic until lightly brown.\n• 2 cloves fresh garlic, crushed (or\n1 tablespoon, minced) 2. Add onion and tomatoes. Sauté\nuntil skin begins to curl.\n• 1 cup white onions, chopped\n3. Add pork, and sauté until lightly\n• 1 cup ripe tomatoes, chopped\nbrown.\n• 1 cup (4 ounces) lean pork, thinly\n4. Add water, and simmer pork for\nsliced\nabout 15 minutes.\n• 4 cups water\n5. Add the sautéed mix to mung\n• 3½ cups precooked mung beans\nbeans, and continue to simmer\n(from 1¾ cups dry beans)*\n15 minutes.\n• 1 teaspoon salt\n6. Season with salt and ground\n• 1 teaspoon ground pepper\npepper.\n• 1 cup (4 ounces) shrimp, peeled\n7. Add peeled shrimp.\nand deveined\n8. Add frozen leaf spinach, and\n• 1 cup (about ⅔ of a 1",
        "0-ounce\ncook 4 minutes until done.\npackage) leaf spinach, frozen\n* To cook dry, uncooked mung beans:\nWash and boil the uncooked mung\nbeans in a large saucepan, using 6 Nutrition Information\ncups of water. Cook until tender, about\n1½ to 2 hours. Drain. Makes 8 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 160\nTotal Fat: 3.5 g\nSaturated Fat: 1 g\nCholesterol: 35 mg\nSodium: 350 mg\nSource: Filipino American Food Practices, Customs,\nand Holidays, American Dietetic Association, 1994. Total Fiber: 8 g\nProtein: 13 g\nCarbohydrates: 19 g\nPotassium: 370 mg\n5\nAmpalaya (Bitter Melon) With Pork\nThis recipe is lower in fat and sodium than a typical ampalaya dish\nbecause it uses lean meat that is sautéed and simmered instead of fried.\nIngredients Directions\n• 1 cup onion, chopped 1. Using a large skillet, lightly sauté\nonions and garlic in hot olive oil.\n• 6 cloves garlic, crushed\n2. Add the ground pork and cook until\n• 1 tablespoon olive oil\nalmost done.\n• ½ pound (0.2 kg) lean ground ",
        "pork\n3. Add the sliced bitter melon.\n• 2 cups Ampalaya*, sliced\n4. Cover and simmer until bitter melon\n• 2 teaspoons light soy sauce\nturns green. Do not overcook.\n• ½ teaspoon black pepper\n5. Season with light soy sauce and\nblack pepper.\n* Ampalaya (bitter melon) is a fruit that is\noblong, cylindrical, pointed at both ends,\nribbed, and wrinkled.\nNutrition Information\nMakes 4 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 150\nTotal Fat: 6 g\nSaturated Fat: 1.5 g\nCholesterol: 45 mg\nSodium: 200 mg\nTotal Fiber: 1 g\nProtein: 17 g\nCarbohydrates: 7 g\nPotassium: 600 mg\nSource: Adapted from Mula Sa Puso, Heart Healthy Traditional Filipino Recipes, American Heart Association, 1999.\n6\nCantaloupe Crush\nTry this refreshing, heart healthy drink that uses fresh fruit, fat-free milk, and low\namounts of sweetener. Children and adults alike will love it!\nIngredients Directions\n• ½ cantaloupe 1. Cut cantaloupe into small\ncubes or thin strips.\n• 1 cup fat-free milk\n2. Mix cantaloupe, milk, a",
        "nd\n• 1½ cups ice\nice in a blender until smooth.\n• Sweetener, as needed\n3. Sweeten to taste.\n(about 1 to 2 teaspoons\nsugar or equivalent of\nanother sweetener)\nNutrition Information\nMakes 4 servings\nServing size: ½ cup\nEach serving provides:\nCalories: 50\nTotal Fat: 0 g\nSaturated Fat: 0 g\nCholesterol: 0 mg\nSodium: 40 mg\nTotal Fiber: 0 g\nProtein: 3 g\nCarbohydrates: 10 g\nPotassium: 280 mg\nSource: Adapted from the National Cancer Institute and InteliHealth (intelihealth.com), 2013.\n7\nVegetable Kare-Kare (Peanut Stew)\nThis version of vegetable kare-kare is healthier than the traditional Filipino dish\nbecause it has no cholesterol. It uses gluten instead of oxtail or other meat. It is also\npacked with vegetables and made complete with a nutty, low-sodium sauce.\nIngredients\n• 1 ¼ cup gluten or seitan,* cubes\n• 2 tablespoons corn oil\n• 2 cloves garlic, crushed\nNutrition Information\n• 1 onion, medium, sliced\n• ½ cup ground peanuts Makes 6 servings\n• ¼ cup ground toasted rice**\nEach serving provid",
        "es:\n• ¼ teaspoon salt\nCalories: 300\n• 1 cup eggplant, sliced\nTotal Fat: 12 g\n• ½ cup string beans, sliced Saturated Fat: 1.5 g\n• ⅔ cup banana heart or bud Cholesterol: 0 mg\nSodium: 125 mg\n• ½ cup bok choy (pechay), sliced\nTotal Fiber: 4 g\n* Gluten is made from protein that is in a variety of grains, such Protein: 36 g\nas wheat and rye, and is mixed and kneaded with water. Seitan\nCarbohydrates: 20 g\nis a form of wheat gluten. It is sold as strips or in cans at health\nPotassium: 320 mg\nfood stores and Asian supermarkets.\n** To make ground, toasted rice: Place rice, ½ cup at a time,\nin a frying pan or wok and heat over moderate heat, stirring\nfrequently to keep it from burning and to allow it to develop a\nuniform, deep golden color—2 to 3 minutes. Then remove it\nfrom heat and cool to room temperature. Grind the toasted rice\ncoarsely—not finely grounded—in a blende, or spice or coffee\ngrinder.\nDirections\n1. Sauté gluten cubes in corn oil. Add garlic and onions.\n2. Pour enough water to cove",
        "r gluten, and add ground\npeanuts and ground rice to thicken.\n3. Season with salt.\n4. Add the eggplant, then string beans, then banana,\nthen bok choy (pechay) on top of the cooked gluten.\nSource: PHC Alive Diet, Division of Nutrition and Dietetics, Philippine Heart Center, East Avenue, Quezon City, Philippines, page 91.\n8\nNational Heart, Lung, and Blood Institute\nAn Initiative of the to help reduce health disparities in cardiovascular disease\nand asthma in underserved and minority communities. For more information, visit www.nhlbi.nih.gov/health/healthdisp."
      ],
      "uploaded_at": "2025-07-25T19:51:12.119632",
      "source": "pdf_upload"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "chunks": [
        "Tasty and Healthy —\nHeart Healthy Filipino Recipes\nFish Cardillo\nThis is a delicious, low-cost recipe with low-sodium ingredients.\nKeep it low-fat by not adding meat fat (lard) or other fat.\nIngredients Directions\n1. Thoroughly clean fish. Remove scale\n• 1 pound (½ kg) red snapper\nand gills, and wash thoroughly. Drain and\n• 4 teaspoons corn oil for sauté\nset aside.\n• ¼ cup flou\n2. Slice the raw fish into six pieces\n• 1 large onion, sliced\n3. Heat corn oil in frying pan.\n• 3 or 4 medium-sized tomatoes,\n4. Place the flour into a bowl or plastic bag\nchopped\nPlace the raw fish in the flour and cov\n• ½ cup egg whites, beaten the outside of each fish with flo .\n• ½ cup water 5. Sauté fish until golden brown. Set asid\n• A dash ground pepper on top of a paper towel.\n• 15 stalks green onions, chopped 6. Sauté onion and tomatoes. Add ½ cup of\nwater.\n7. Add the beaten egg whites and fish\nCover and let it simmer for 5–10 minutes.\nNutrition Information\n8. Season with ground pepper.\nMakes 6 servings",
        " 9. Sprinkle with chopped green onions.\nEach serving provides:\nCalories: 170\nTotal Fat: 4 g\nSaturated Fat: 1 g\nCholesterol: 45 mg\nQuick Tip\nSodium: 115 mg\nTotal Fiber: 3 g\nProtein: 20 g\nThis recipe is lower in salt and sodium than other\nCarbohydrates: 13 g\nCardillo recipes because it uses:\nPotassium: 600 mg\n• Fresh, not canned, tomatoes\n• Ground pepper and corn oil with no salt added\n• Fresh onion and green onions\n• Fresh, not smoked or canned, fis\nSource: Philippine Heart Center’s Healthy Heart Cookbook.\n1\nAdobong Manok (Marinated Chicken)\nThis low-cost, low-sodium recipe has great flavor that you and your\nfamily will love! It uses chicken breast which is lower in fat than\nother parts of the chicken, like the thigh and leg.\nIngredients Directions\n• 1 teaspoon olive oil 1. Combine olive oil, garlic, and onion in a frying\npan. Add chicken and sauté together until\n• 2 cloves fresh crushed garlic\nchicken has browned.\n• 2 medium chopped onions\n2. Add light soy sauce, vinegar, paprika,\n• 1 ",
        "pound (½ kg) chicken breasts, no skin\nblack pepper, and bay leaf. Stir.\n• 2 tablespoons light soy sauce\n3. Bring to a boil. Simmer for 45–60 minutes or\n• ¼ cup vinegar until chicken is done.\n• 1 teaspoon paprika 4. Remove the chicken and save the liquid in the\npot. Arrange the chicken on a broiler pan. Broil\n• 2 tablespoons ground black pepper\nuntil the chicken has nicely browned. Remove\n• 1 bay leaf, broken in half\nfrom the broiler and place it in a serving bowl.\n• 1 medium red tomato (optional)\n5. Continue to boil the sauce in the uncovered\npan until volume is reduced to about half and\nthe sauce is thick.\nNutrition Information\n6. Pour the thickened sauce over broiled\nadobong (chicken) and garnish with red\nMakes 4 servings\ntomatoes, if desired.\nServing size: ½ cup\nEach serving provides:\nCalories: 190\nTotal Fat: 5 g\nQuick Tip\nSaturated Fat: 1 g\nCholesterol: 70 mg\nSodium: 330 mg This recipe is lower in saturated fat and cholesterol because:\nTotal Fiber: 2 g\n• It is made using chicken wi",
        "thout the skin and any extra fat\nProtein: 26 g\nis removed.\nCarbohydrates: 10 g\nPotassium: 370 mg • Only 1 teaspoon of unsaturated fat (olive oil) is added.\n• It is flavored with vegetables and herbs and is boiled an\nbroiled slowly in moist heat instead of fat.\nSource: Filipino-American Nutrition and Fitness Teacher’s\nGuide, Kalusugan Community Services, San Diego, CA.\n2\nLumpiang Sariwa (Fresh Lumpia)\nYou and your family will love this tasty recipe. The ingredients—ground chicken\nor pork, olive oil, peanuts, and fresh herbs and spices—add flavor. Also, the\nlumpiang sariwa is served fresh so it has fewer calories than fried lumpiang.\nIngredients Directions\n• ½ cup ground chicken or lean pork 1. Heat oil, and sauté ground meat with the\nshrimp and garlic.\n• ½ cup shrimp, cleaned and deveined\n2. Add vegetables until slightly crisp. Pour in\n• 1 tablespoon olive oil\nthe chicken broth until cooked.\n• 2 cloves chopped garlic\n3. Season with salt and pepper.\n• ½ cup cabbage, julienned\n4. Set asid",
        "e and drain in a colander.\n• ½ cup green beans, julienned\n5. Save the broth for the lumpia sauce.\n• ½ cup carrots, julienned\n6. Soak the Vietnamese spring roll wrappers\n• ¼ cup celery, julienned\none at a time in water until soft and\n• ¼ cup jicama, julienned (may substitute transparent. Dry immediately with a paper\nchestnuts) towel.\n• ½ cup chicken broth 7. Lay the lettuce on the wrapper.\n• ¼ teaspoon salt 8. Place 2 tablespoons of the vegetable\n• ¼ teaspoon pepper mixture on the wrapper.\n• 8 Vietnamese spring-roll wrappers or 9. Fold in one side of the wrapper and roll\nlumpia wrappers tightly.\n• 8 pieces red leaf lettuce 10. Serve with lumpia sauce on top.\nSprinkle with chopped peanuts.\n• ⅓ cup peanuts, dry, roasted, and chopped\nNutrition Information\nLumpia Sauce\nMakes 8 servings\n• 1 cup broth from the sautéed vegetables Serving size: 1 lumpia\n• 1 tablespoon light soy sauce\nEach serving provides:\n• 1 tablespoon brown sugar\nCalories: 160\n• 3 cloves garlic, minced\nTotal Fat: 4 g\n• 1 tea",
        "spoon cornstarch Saturated Fat: 0.5 g\n• 2 tablespoons cold water for mixing cornstarch Cholesterol: 55 mg\nSodium: 150 mg\nDirections Total Fiber: 2 g\n1. Mix together vegetable broth, soy sauce, brown Protein: 10 g\nsugar, and garlic, and bring to a boil. Carbohydrates: 21 g\nPotassium: 170 mg\n2. Mix the cornstarch in 2 tablespoons of cold water.\n3. Slowly add the cornstarch mixture to the broth.\nStir until sauce thickens.\nSource: Mula sa Puso, Heart Healthy Traditional\nFilipino Recipes, American Heart Association, 1999.\n3\nPesang Isda (Fish Simmered With\nGinger and Tomatoes)\nThis main dish is heart healthy because the fish is simmered in water,\nnot fried, and no fat is added. Flavoring comes from the herbs and spices\ninstead of sauces that are high in sodium.\nIngredients Directions\n1. In a 4-quart saucepan,\n• ¼ cup fresh ginger, thinly\nsimmer sliced ginger,\nsliced (about 2 inches long)\ntomatoes, and onions in\n• 1 cup ripe tomatoes, chopped\n4 cups of water over medium\n• 1 cup white or yello",
        "w onions, heat until onions are tender\nthinly sliced (about 7 to 8 minutes).\n• 4 cups water 2. Reduce heat to low, add fish\n• 2 pounds fleshy fish (c and poach gently until almost\nfillet, halibut steak, or trout done (about 3 to 4 minutes).\n• 2 cups pechay (bok choy) 3. Add pechay stems, salt, and\nstems and leaves, cut up ground pepper. Cook for\nseparately 1 minute; then add pechay\nleaves and green onions.\n• ½ teaspoon salt\nCook another 30 seconds.\n• ½ teaspoon ground pepper\n4. Serve immediately.\n• 1 cup green onions, sliced\nNutrition Information\nMakes 6 servings\nServing size: 3 ounces lean\nfish and ½ cup vegetables\nEach serving provides:\nCalories: 160\nTotal Fat: 2 g\nSaturated Fat: 0.5 g\nCholesterol: 80 mg\nSodium: 340 mg\nTotal Fiber: 2 g\nProtein: 30 g\nCarbohydrates: 6 g\nPotassium: 630 mg\nSource: Filipino American Food Practices, Customs, and Holidays, American Dietetic Association, 1994.\n4\nMunggo Guisado (Sautéed Mung Beans)\nYour family will love this heart healthy side dish. It is mad",
        "e with vegetables,\nseafood, lean meat, and a small amount of corn oil. The pork is slowly\nsimmered in moist heat with vegetables and mung beans, creating flavors\nthat will make your taste buds jump for joy!\nIngredients Directions\n• 1 tablespoon corn oil 1. In a skillet, heat oil, and sauté\ncrushed garlic until lightly brown.\n• 2 cloves fresh garlic, crushed (or\n1 tablespoon, minced) 2. Add onion and tomatoes. Sauté\nuntil skin begins to curl.\n• 1 cup white onions, chopped\n3. Add pork, and sauté until lightly\n• 1 cup ripe tomatoes, chopped\nbrown.\n• 1 cup (4 ounces) lean pork, thinly\n4. Add water, and simmer pork for\nsliced\nabout 15 minutes.\n• 4 cups water\n5. Add the sautéed mix to mung\n• 3½ cups precooked mung beans\nbeans, and continue to simmer\n(from 1¾ cups dry beans)*\n15 minutes.\n• 1 teaspoon salt\n6. Season with salt and ground\n• 1 teaspoon ground pepper\npepper.\n• 1 cup (4 ounces) shrimp, peeled\n7. Add peeled shrimp.\nand deveined\n8. Add frozen leaf spinach, and\n• 1 cup (about ⅔ of a 1",
        "0-ounce\ncook 4 minutes until done.\npackage) leaf spinach, frozen\n* To cook dry, uncooked mung beans:\nWash and boil the uncooked mung\nbeans in a large saucepan, using 6 Nutrition Information\ncups of water. Cook until tender, about\n1½ to 2 hours. Drain. Makes 8 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 160\nTotal Fat: 3.5 g\nSaturated Fat: 1 g\nCholesterol: 35 mg\nSodium: 350 mg\nSource: Filipino American Food Practices, Customs,\nand Holidays, American Dietetic Association, 1994. Total Fiber: 8 g\nProtein: 13 g\nCarbohydrates: 19 g\nPotassium: 370 mg\n5\nAmpalaya (Bitter Melon) With Pork\nThis recipe is lower in fat and sodium than a typical ampalaya dish\nbecause it uses lean meat that is sautéed and simmered instead of fried.\nIngredients Directions\n• 1 cup onion, chopped 1. Using a large skillet, lightly sauté\nonions and garlic in hot olive oil.\n• 6 cloves garlic, crushed\n2. Add the ground pork and cook until\n• 1 tablespoon olive oil\nalmost done.\n• ½ pound (0.2 kg) lean ground ",
        "pork\n3. Add the sliced bitter melon.\n• 2 cups Ampalaya*, sliced\n4. Cover and simmer until bitter melon\n• 2 teaspoons light soy sauce\nturns green. Do not overcook.\n• ½ teaspoon black pepper\n5. Season with light soy sauce and\nblack pepper.\n* Ampalaya (bitter melon) is a fruit that is\noblong, cylindrical, pointed at both ends,\nribbed, and wrinkled.\nNutrition Information\nMakes 4 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 150\nTotal Fat: 6 g\nSaturated Fat: 1.5 g\nCholesterol: 45 mg\nSodium: 200 mg\nTotal Fiber: 1 g\nProtein: 17 g\nCarbohydrates: 7 g\nPotassium: 600 mg\nSource: Adapted from Mula Sa Puso, Heart Healthy Traditional Filipino Recipes, American Heart Association, 1999.\n6\nCantaloupe Crush\nTry this refreshing, heart healthy drink that uses fresh fruit, fat-free milk, and low\namounts of sweetener. Children and adults alike will love it!\nIngredients Directions\n• ½ cantaloupe 1. Cut cantaloupe into small\ncubes or thin strips.\n• 1 cup fat-free milk\n2. Mix cantaloupe, milk, a",
        "nd\n• 1½ cups ice\nice in a blender until smooth.\n• Sweetener, as needed\n3. Sweeten to taste.\n(about 1 to 2 teaspoons\nsugar or equivalent of\nanother sweetener)\nNutrition Information\nMakes 4 servings\nServing size: ½ cup\nEach serving provides:\nCalories: 50\nTotal Fat: 0 g\nSaturated Fat: 0 g\nCholesterol: 0 mg\nSodium: 40 mg\nTotal Fiber: 0 g\nProtein: 3 g\nCarbohydrates: 10 g\nPotassium: 280 mg\nSource: Adapted from the National Cancer Institute and InteliHealth (intelihealth.com), 2013.\n7\nVegetable Kare-Kare (Peanut Stew)\nThis version of vegetable kare-kare is healthier than the traditional Filipino dish\nbecause it has no cholesterol. It uses gluten instead of oxtail or other meat. It is also\npacked with vegetables and made complete with a nutty, low-sodium sauce.\nIngredients\n• 1 ¼ cup gluten or seitan,* cubes\n• 2 tablespoons corn oil\n• 2 cloves garlic, crushed\nNutrition Information\n• 1 onion, medium, sliced\n• ½ cup ground peanuts Makes 6 servings\n• ¼ cup ground toasted rice**\nEach serving provid",
        "es:\n• ¼ teaspoon salt\nCalories: 300\n• 1 cup eggplant, sliced\nTotal Fat: 12 g\n• ½ cup string beans, sliced Saturated Fat: 1.5 g\n• ⅔ cup banana heart or bud Cholesterol: 0 mg\nSodium: 125 mg\n• ½ cup bok choy (pechay), sliced\nTotal Fiber: 4 g\n* Gluten is made from protein that is in a variety of grains, such Protein: 36 g\nas wheat and rye, and is mixed and kneaded with water. Seitan\nCarbohydrates: 20 g\nis a form of wheat gluten. It is sold as strips or in cans at health\nPotassium: 320 mg\nfood stores and Asian supermarkets.\n** To make ground, toasted rice: Place rice, ½ cup at a time,\nin a frying pan or wok and heat over moderate heat, stirring\nfrequently to keep it from burning and to allow it to develop a\nuniform, deep golden color—2 to 3 minutes. Then remove it\nfrom heat and cool to room temperature. Grind the toasted rice\ncoarsely—not finely grounded—in a blende, or spice or coffee\ngrinder.\nDirections\n1. Sauté gluten cubes in corn oil. Add garlic and onions.\n2. Pour enough water to cove",
        "r gluten, and add ground\npeanuts and ground rice to thicken.\n3. Season with salt.\n4. Add the eggplant, then string beans, then banana,\nthen bok choy (pechay) on top of the cooked gluten.\nSource: PHC Alive Diet, Division of Nutrition and Dietetics, Philippine Heart Center, East Avenue, Quezon City, Philippines, page 91.\n8\nNational Heart, Lung, and Blood Institute\nAn Initiative of the to help reduce health disparities in cardiovascular disease\nand asthma in underserved and minority communities. For more information, visit www.nhlbi.nih.gov/health/healthdisp."
      ],
      "uploaded_at": "2025-07-25T19:51:16.922723",
      "source": "pdf_upload"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "chunks": [
        "Tasty and Healthy —\nHeart Healthy Filipino Recipes\nFish Cardillo\nThis is a delicious, low-cost recipe with low-sodium ingredients.\nKeep it low-fat by not adding meat fat (lard) or other fat.\nIngredients Directions\n1. Thoroughly clean fish. Remove scale\n• 1 pound (½ kg) red snapper\nand gills, and wash thoroughly. Drain and\n• 4 teaspoons corn oil for sauté\nset aside.\n• ¼ cup flou\n2. Slice the raw fish into six pieces\n• 1 large onion, sliced\n3. Heat corn oil in frying pan.\n• 3 or 4 medium-sized tomatoes,\n4. Place the flour into a bowl or plastic bag\nchopped\nPlace the raw fish in the flour and cov\n• ½ cup egg whites, beaten the outside of each fish with flo .\n• ½ cup water 5. Sauté fish until golden brown. Set asid\n• A dash ground pepper on top of a paper towel.\n• 15 stalks green onions, chopped 6. Sauté onion and tomatoes. Add ½ cup of\nwater.\n7. Add the beaten egg whites and fish\nCover and let it simmer for 5–10 minutes.\nNutrition Information\n8. Season with ground pepper.\nMakes 6 servings",
        " 9. Sprinkle with chopped green onions.\nEach serving provides:\nCalories: 170\nTotal Fat: 4 g\nSaturated Fat: 1 g\nCholesterol: 45 mg\nQuick Tip\nSodium: 115 mg\nTotal Fiber: 3 g\nProtein: 20 g\nThis recipe is lower in salt and sodium than other\nCarbohydrates: 13 g\nCardillo recipes because it uses:\nPotassium: 600 mg\n• Fresh, not canned, tomatoes\n• Ground pepper and corn oil with no salt added\n• Fresh onion and green onions\n• Fresh, not smoked or canned, fis\nSource: Philippine Heart Center’s Healthy Heart Cookbook.\n1\nAdobong Manok (Marinated Chicken)\nThis low-cost, low-sodium recipe has great flavor that you and your\nfamily will love! It uses chicken breast which is lower in fat than\nother parts of the chicken, like the thigh and leg.\nIngredients Directions\n• 1 teaspoon olive oil 1. Combine olive oil, garlic, and onion in a frying\npan. Add chicken and sauté together until\n• 2 cloves fresh crushed garlic\nchicken has browned.\n• 2 medium chopped onions\n2. Add light soy sauce, vinegar, paprika,\n• 1 ",
        "pound (½ kg) chicken breasts, no skin\nblack pepper, and bay leaf. Stir.\n• 2 tablespoons light soy sauce\n3. Bring to a boil. Simmer for 45–60 minutes or\n• ¼ cup vinegar until chicken is done.\n• 1 teaspoon paprika 4. Remove the chicken and save the liquid in the\npot. Arrange the chicken on a broiler pan. Broil\n• 2 tablespoons ground black pepper\nuntil the chicken has nicely browned. Remove\n• 1 bay leaf, broken in half\nfrom the broiler and place it in a serving bowl.\n• 1 medium red tomato (optional)\n5. Continue to boil the sauce in the uncovered\npan until volume is reduced to about half and\nthe sauce is thick.\nNutrition Information\n6. Pour the thickened sauce over broiled\nadobong (chicken) and garnish with red\nMakes 4 servings\ntomatoes, if desired.\nServing size: ½ cup\nEach serving provides:\nCalories: 190\nTotal Fat: 5 g\nQuick Tip\nSaturated Fat: 1 g\nCholesterol: 70 mg\nSodium: 330 mg This recipe is lower in saturated fat and cholesterol because:\nTotal Fiber: 2 g\n• It is made using chicken wi",
        "thout the skin and any extra fat\nProtein: 26 g\nis removed.\nCarbohydrates: 10 g\nPotassium: 370 mg • Only 1 teaspoon of unsaturated fat (olive oil) is added.\n• It is flavored with vegetables and herbs and is boiled an\nbroiled slowly in moist heat instead of fat.\nSource: Filipino-American Nutrition and Fitness Teacher’s\nGuide, Kalusugan Community Services, San Diego, CA.\n2\nLumpiang Sariwa (Fresh Lumpia)\nYou and your family will love this tasty recipe. The ingredients—ground chicken\nor pork, olive oil, peanuts, and fresh herbs and spices—add flavor. Also, the\nlumpiang sariwa is served fresh so it has fewer calories than fried lumpiang.\nIngredients Directions\n• ½ cup ground chicken or lean pork 1. Heat oil, and sauté ground meat with the\nshrimp and garlic.\n• ½ cup shrimp, cleaned and deveined\n2. Add vegetables until slightly crisp. Pour in\n• 1 tablespoon olive oil\nthe chicken broth until cooked.\n• 2 cloves chopped garlic\n3. Season with salt and pepper.\n• ½ cup cabbage, julienned\n4. Set asid",
        "e and drain in a colander.\n• ½ cup green beans, julienned\n5. Save the broth for the lumpia sauce.\n• ½ cup carrots, julienned\n6. Soak the Vietnamese spring roll wrappers\n• ¼ cup celery, julienned\none at a time in water until soft and\n• ¼ cup jicama, julienned (may substitute transparent. Dry immediately with a paper\nchestnuts) towel.\n• ½ cup chicken broth 7. Lay the lettuce on the wrapper.\n• ¼ teaspoon salt 8. Place 2 tablespoons of the vegetable\n• ¼ teaspoon pepper mixture on the wrapper.\n• 8 Vietnamese spring-roll wrappers or 9. Fold in one side of the wrapper and roll\nlumpia wrappers tightly.\n• 8 pieces red leaf lettuce 10. Serve with lumpia sauce on top.\nSprinkle with chopped peanuts.\n• ⅓ cup peanuts, dry, roasted, and chopped\nNutrition Information\nLumpia Sauce\nMakes 8 servings\n• 1 cup broth from the sautéed vegetables Serving size: 1 lumpia\n• 1 tablespoon light soy sauce\nEach serving provides:\n• 1 tablespoon brown sugar\nCalories: 160\n• 3 cloves garlic, minced\nTotal Fat: 4 g\n• 1 tea",
        "spoon cornstarch Saturated Fat: 0.5 g\n• 2 tablespoons cold water for mixing cornstarch Cholesterol: 55 mg\nSodium: 150 mg\nDirections Total Fiber: 2 g\n1. Mix together vegetable broth, soy sauce, brown Protein: 10 g\nsugar, and garlic, and bring to a boil. Carbohydrates: 21 g\nPotassium: 170 mg\n2. Mix the cornstarch in 2 tablespoons of cold water.\n3. Slowly add the cornstarch mixture to the broth.\nStir until sauce thickens.\nSource: Mula sa Puso, Heart Healthy Traditional\nFilipino Recipes, American Heart Association, 1999.\n3\nPesang Isda (Fish Simmered With\nGinger and Tomatoes)\nThis main dish is heart healthy because the fish is simmered in water,\nnot fried, and no fat is added. Flavoring comes from the herbs and spices\ninstead of sauces that are high in sodium.\nIngredients Directions\n1. In a 4-quart saucepan,\n• ¼ cup fresh ginger, thinly\nsimmer sliced ginger,\nsliced (about 2 inches long)\ntomatoes, and onions in\n• 1 cup ripe tomatoes, chopped\n4 cups of water over medium\n• 1 cup white or yello",
        "w onions, heat until onions are tender\nthinly sliced (about 7 to 8 minutes).\n• 4 cups water 2. Reduce heat to low, add fish\n• 2 pounds fleshy fish (c and poach gently until almost\nfillet, halibut steak, or trout done (about 3 to 4 minutes).\n• 2 cups pechay (bok choy) 3. Add pechay stems, salt, and\nstems and leaves, cut up ground pepper. Cook for\nseparately 1 minute; then add pechay\nleaves and green onions.\n• ½ teaspoon salt\nCook another 30 seconds.\n• ½ teaspoon ground pepper\n4. Serve immediately.\n• 1 cup green onions, sliced\nNutrition Information\nMakes 6 servings\nServing size: 3 ounces lean\nfish and ½ cup vegetables\nEach serving provides:\nCalories: 160\nTotal Fat: 2 g\nSaturated Fat: 0.5 g\nCholesterol: 80 mg\nSodium: 340 mg\nTotal Fiber: 2 g\nProtein: 30 g\nCarbohydrates: 6 g\nPotassium: 630 mg\nSource: Filipino American Food Practices, Customs, and Holidays, American Dietetic Association, 1994.\n4\nMunggo Guisado (Sautéed Mung Beans)\nYour family will love this heart healthy side dish. It is mad",
        "e with vegetables,\nseafood, lean meat, and a small amount of corn oil. The pork is slowly\nsimmered in moist heat with vegetables and mung beans, creating flavors\nthat will make your taste buds jump for joy!\nIngredients Directions\n• 1 tablespoon corn oil 1. In a skillet, heat oil, and sauté\ncrushed garlic until lightly brown.\n• 2 cloves fresh garlic, crushed (or\n1 tablespoon, minced) 2. Add onion and tomatoes. Sauté\nuntil skin begins to curl.\n• 1 cup white onions, chopped\n3. Add pork, and sauté until lightly\n• 1 cup ripe tomatoes, chopped\nbrown.\n• 1 cup (4 ounces) lean pork, thinly\n4. Add water, and simmer pork for\nsliced\nabout 15 minutes.\n• 4 cups water\n5. Add the sautéed mix to mung\n• 3½ cups precooked mung beans\nbeans, and continue to simmer\n(from 1¾ cups dry beans)*\n15 minutes.\n• 1 teaspoon salt\n6. Season with salt and ground\n• 1 teaspoon ground pepper\npepper.\n• 1 cup (4 ounces) shrimp, peeled\n7. Add peeled shrimp.\nand deveined\n8. Add frozen leaf spinach, and\n• 1 cup (about ⅔ of a 1",
        "0-ounce\ncook 4 minutes until done.\npackage) leaf spinach, frozen\n* To cook dry, uncooked mung beans:\nWash and boil the uncooked mung\nbeans in a large saucepan, using 6 Nutrition Information\ncups of water. Cook until tender, about\n1½ to 2 hours. Drain. Makes 8 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 160\nTotal Fat: 3.5 g\nSaturated Fat: 1 g\nCholesterol: 35 mg\nSodium: 350 mg\nSource: Filipino American Food Practices, Customs,\nand Holidays, American Dietetic Association, 1994. Total Fiber: 8 g\nProtein: 13 g\nCarbohydrates: 19 g\nPotassium: 370 mg\n5\nAmpalaya (Bitter Melon) With Pork\nThis recipe is lower in fat and sodium than a typical ampalaya dish\nbecause it uses lean meat that is sautéed and simmered instead of fried.\nIngredients Directions\n• 1 cup onion, chopped 1. Using a large skillet, lightly sauté\nonions and garlic in hot olive oil.\n• 6 cloves garlic, crushed\n2. Add the ground pork and cook until\n• 1 tablespoon olive oil\nalmost done.\n• ½ pound (0.2 kg) lean ground ",
        "pork\n3. Add the sliced bitter melon.\n• 2 cups Ampalaya*, sliced\n4. Cover and simmer until bitter melon\n• 2 teaspoons light soy sauce\nturns green. Do not overcook.\n• ½ teaspoon black pepper\n5. Season with light soy sauce and\nblack pepper.\n* Ampalaya (bitter melon) is a fruit that is\noblong, cylindrical, pointed at both ends,\nribbed, and wrinkled.\nNutrition Information\nMakes 4 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 150\nTotal Fat: 6 g\nSaturated Fat: 1.5 g\nCholesterol: 45 mg\nSodium: 200 mg\nTotal Fiber: 1 g\nProtein: 17 g\nCarbohydrates: 7 g\nPotassium: 600 mg\nSource: Adapted from Mula Sa Puso, Heart Healthy Traditional Filipino Recipes, American Heart Association, 1999.\n6\nCantaloupe Crush\nTry this refreshing, heart healthy drink that uses fresh fruit, fat-free milk, and low\namounts of sweetener. Children and adults alike will love it!\nIngredients Directions\n• ½ cantaloupe 1. Cut cantaloupe into small\ncubes or thin strips.\n• 1 cup fat-free milk\n2. Mix cantaloupe, milk, a",
        "nd\n• 1½ cups ice\nice in a blender until smooth.\n• Sweetener, as needed\n3. Sweeten to taste.\n(about 1 to 2 teaspoons\nsugar or equivalent of\nanother sweetener)\nNutrition Information\nMakes 4 servings\nServing size: ½ cup\nEach serving provides:\nCalories: 50\nTotal Fat: 0 g\nSaturated Fat: 0 g\nCholesterol: 0 mg\nSodium: 40 mg\nTotal Fiber: 0 g\nProtein: 3 g\nCarbohydrates: 10 g\nPotassium: 280 mg\nSource: Adapted from the National Cancer Institute and InteliHealth (intelihealth.com), 2013.\n7\nVegetable Kare-Kare (Peanut Stew)\nThis version of vegetable kare-kare is healthier than the traditional Filipino dish\nbecause it has no cholesterol. It uses gluten instead of oxtail or other meat. It is also\npacked with vegetables and made complete with a nutty, low-sodium sauce.\nIngredients\n• 1 ¼ cup gluten or seitan,* cubes\n• 2 tablespoons corn oil\n• 2 cloves garlic, crushed\nNutrition Information\n• 1 onion, medium, sliced\n• ½ cup ground peanuts Makes 6 servings\n• ¼ cup ground toasted rice**\nEach serving provid",
        "es:\n• ¼ teaspoon salt\nCalories: 300\n• 1 cup eggplant, sliced\nTotal Fat: 12 g\n• ½ cup string beans, sliced Saturated Fat: 1.5 g\n• ⅔ cup banana heart or bud Cholesterol: 0 mg\nSodium: 125 mg\n• ½ cup bok choy (pechay), sliced\nTotal Fiber: 4 g\n* Gluten is made from protein that is in a variety of grains, such Protein: 36 g\nas wheat and rye, and is mixed and kneaded with water. Seitan\nCarbohydrates: 20 g\nis a form of wheat gluten. It is sold as strips or in cans at health\nPotassium: 320 mg\nfood stores and Asian supermarkets.\n** To make ground, toasted rice: Place rice, ½ cup at a time,\nin a frying pan or wok and heat over moderate heat, stirring\nfrequently to keep it from burning and to allow it to develop a\nuniform, deep golden color—2 to 3 minutes. Then remove it\nfrom heat and cool to room temperature. Grind the toasted rice\ncoarsely—not finely grounded—in a blende, or spice or coffee\ngrinder.\nDirections\n1. Sauté gluten cubes in corn oil. Add garlic and onions.\n2. Pour enough water to cove",
        "r gluten, and add ground\npeanuts and ground rice to thicken.\n3. Season with salt.\n4. Add the eggplant, then string beans, then banana,\nthen bok choy (pechay) on top of the cooked gluten.\nSource: PHC Alive Diet, Division of Nutrition and Dietetics, Philippine Heart Center, East Avenue, Quezon City, Philippines, page 91.\n8\nNational Heart, Lung, and Blood Institute\nAn Initiative of the to help reduce health disparities in cardiovascular disease\nand asthma in underserved and minority communities. For more information, visit www.nhlbi.nih.gov/health/healthdisp."
      ],
      "uploaded_at": "2025-07-25T19:51:21.723792",
      "source": "pdf_upload"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "chunks": [
        "Tasty and Healthy —\nHeart Healthy Filipino Recipes\nFish Cardillo\nThis is a delicious, low-cost recipe with low-sodium ingredients.\nKeep it low-fat by not adding meat fat (lard) or other fat.\nIngredients Directions\n1. Thoroughly clean fish. Remove scale\n• 1 pound (½ kg) red snapper\nand gills, and wash thoroughly. Drain and\n• 4 teaspoons corn oil for sauté\nset aside.\n• ¼ cup flou\n2. Slice the raw fish into six pieces\n• 1 large onion, sliced\n3. Heat corn oil in frying pan.\n• 3 or 4 medium-sized tomatoes,\n4. Place the flour into a bowl or plastic bag\nchopped\nPlace the raw fish in the flour and cov\n• ½ cup egg whites, beaten the outside of each fish with flo .\n• ½ cup water 5. Sauté fish until golden brown. Set asid\n• A dash ground pepper on top of a paper towel.\n• 15 stalks green onions, chopped 6. Sauté onion and tomatoes. Add ½ cup of\nwater.\n7. Add the beaten egg whites and fish\nCover and let it simmer for 5–10 minutes.\nNutrition Information\n8. Season with ground pepper.\nMakes 6 servings",
        " 9. Sprinkle with chopped green onions.\nEach serving provides:\nCalories: 170\nTotal Fat: 4 g\nSaturated Fat: 1 g\nCholesterol: 45 mg\nQuick Tip\nSodium: 115 mg\nTotal Fiber: 3 g\nProtein: 20 g\nThis recipe is lower in salt and sodium than other\nCarbohydrates: 13 g\nCardillo recipes because it uses:\nPotassium: 600 mg\n• Fresh, not canned, tomatoes\n• Ground pepper and corn oil with no salt added\n• Fresh onion and green onions\n• Fresh, not smoked or canned, fis\nSource: Philippine Heart Center’s Healthy Heart Cookbook.\n1\nAdobong Manok (Marinated Chicken)\nThis low-cost, low-sodium recipe has great flavor that you and your\nfamily will love! It uses chicken breast which is lower in fat than\nother parts of the chicken, like the thigh and leg.\nIngredients Directions\n• 1 teaspoon olive oil 1. Combine olive oil, garlic, and onion in a frying\npan. Add chicken and sauté together until\n• 2 cloves fresh crushed garlic\nchicken has browned.\n• 2 medium chopped onions\n2. Add light soy sauce, vinegar, paprika,\n• 1 ",
        "pound (½ kg) chicken breasts, no skin\nblack pepper, and bay leaf. Stir.\n• 2 tablespoons light soy sauce\n3. Bring to a boil. Simmer for 45–60 minutes or\n• ¼ cup vinegar until chicken is done.\n• 1 teaspoon paprika 4. Remove the chicken and save the liquid in the\npot. Arrange the chicken on a broiler pan. Broil\n• 2 tablespoons ground black pepper\nuntil the chicken has nicely browned. Remove\n• 1 bay leaf, broken in half\nfrom the broiler and place it in a serving bowl.\n• 1 medium red tomato (optional)\n5. Continue to boil the sauce in the uncovered\npan until volume is reduced to about half and\nthe sauce is thick.\nNutrition Information\n6. Pour the thickened sauce over broiled\nadobong (chicken) and garnish with red\nMakes 4 servings\ntomatoes, if desired.\nServing size: ½ cup\nEach serving provides:\nCalories: 190\nTotal Fat: 5 g\nQuick Tip\nSaturated Fat: 1 g\nCholesterol: 70 mg\nSodium: 330 mg This recipe is lower in saturated fat and cholesterol because:\nTotal Fiber: 2 g\n• It is made using chicken wi",
        "thout the skin and any extra fat\nProtein: 26 g\nis removed.\nCarbohydrates: 10 g\nPotassium: 370 mg • Only 1 teaspoon of unsaturated fat (olive oil) is added.\n• It is flavored with vegetables and herbs and is boiled an\nbroiled slowly in moist heat instead of fat.\nSource: Filipino-American Nutrition and Fitness Teacher’s\nGuide, Kalusugan Community Services, San Diego, CA.\n2\nLumpiang Sariwa (Fresh Lumpia)\nYou and your family will love this tasty recipe. The ingredients—ground chicken\nor pork, olive oil, peanuts, and fresh herbs and spices—add flavor. Also, the\nlumpiang sariwa is served fresh so it has fewer calories than fried lumpiang.\nIngredients Directions\n• ½ cup ground chicken or lean pork 1. Heat oil, and sauté ground meat with the\nshrimp and garlic.\n• ½ cup shrimp, cleaned and deveined\n2. Add vegetables until slightly crisp. Pour in\n• 1 tablespoon olive oil\nthe chicken broth until cooked.\n• 2 cloves chopped garlic\n3. Season with salt and pepper.\n• ½ cup cabbage, julienned\n4. Set asid",
        "e and drain in a colander.\n• ½ cup green beans, julienned\n5. Save the broth for the lumpia sauce.\n• ½ cup carrots, julienned\n6. Soak the Vietnamese spring roll wrappers\n• ¼ cup celery, julienned\none at a time in water until soft and\n• ¼ cup jicama, julienned (may substitute transparent. Dry immediately with a paper\nchestnuts) towel.\n• ½ cup chicken broth 7. Lay the lettuce on the wrapper.\n• ¼ teaspoon salt 8. Place 2 tablespoons of the vegetable\n• ¼ teaspoon pepper mixture on the wrapper.\n• 8 Vietnamese spring-roll wrappers or 9. Fold in one side of the wrapper and roll\nlumpia wrappers tightly.\n• 8 pieces red leaf lettuce 10. Serve with lumpia sauce on top.\nSprinkle with chopped peanuts.\n• ⅓ cup peanuts, dry, roasted, and chopped\nNutrition Information\nLumpia Sauce\nMakes 8 servings\n• 1 cup broth from the sautéed vegetables Serving size: 1 lumpia\n• 1 tablespoon light soy sauce\nEach serving provides:\n• 1 tablespoon brown sugar\nCalories: 160\n• 3 cloves garlic, minced\nTotal Fat: 4 g\n• 1 tea",
        "spoon cornstarch Saturated Fat: 0.5 g\n• 2 tablespoons cold water for mixing cornstarch Cholesterol: 55 mg\nSodium: 150 mg\nDirections Total Fiber: 2 g\n1. Mix together vegetable broth, soy sauce, brown Protein: 10 g\nsugar, and garlic, and bring to a boil. Carbohydrates: 21 g\nPotassium: 170 mg\n2. Mix the cornstarch in 2 tablespoons of cold water.\n3. Slowly add the cornstarch mixture to the broth.\nStir until sauce thickens.\nSource: Mula sa Puso, Heart Healthy Traditional\nFilipino Recipes, American Heart Association, 1999.\n3\nPesang Isda (Fish Simmered With\nGinger and Tomatoes)\nThis main dish is heart healthy because the fish is simmered in water,\nnot fried, and no fat is added. Flavoring comes from the herbs and spices\ninstead of sauces that are high in sodium.\nIngredients Directions\n1. In a 4-quart saucepan,\n• ¼ cup fresh ginger, thinly\nsimmer sliced ginger,\nsliced (about 2 inches long)\ntomatoes, and onions in\n• 1 cup ripe tomatoes, chopped\n4 cups of water over medium\n• 1 cup white or yello",
        "w onions, heat until onions are tender\nthinly sliced (about 7 to 8 minutes).\n• 4 cups water 2. Reduce heat to low, add fish\n• 2 pounds fleshy fish (c and poach gently until almost\nfillet, halibut steak, or trout done (about 3 to 4 minutes).\n• 2 cups pechay (bok choy) 3. Add pechay stems, salt, and\nstems and leaves, cut up ground pepper. Cook for\nseparately 1 minute; then add pechay\nleaves and green onions.\n• ½ teaspoon salt\nCook another 30 seconds.\n• ½ teaspoon ground pepper\n4. Serve immediately.\n• 1 cup green onions, sliced\nNutrition Information\nMakes 6 servings\nServing size: 3 ounces lean\nfish and ½ cup vegetables\nEach serving provides:\nCalories: 160\nTotal Fat: 2 g\nSaturated Fat: 0.5 g\nCholesterol: 80 mg\nSodium: 340 mg\nTotal Fiber: 2 g\nProtein: 30 g\nCarbohydrates: 6 g\nPotassium: 630 mg\nSource: Filipino American Food Practices, Customs, and Holidays, American Dietetic Association, 1994.\n4\nMunggo Guisado (Sautéed Mung Beans)\nYour family will love this heart healthy side dish. It is mad",
        "e with vegetables,\nseafood, lean meat, and a small amount of corn oil. The pork is slowly\nsimmered in moist heat with vegetables and mung beans, creating flavors\nthat will make your taste buds jump for joy!\nIngredients Directions\n• 1 tablespoon corn oil 1. In a skillet, heat oil, and sauté\ncrushed garlic until lightly brown.\n• 2 cloves fresh garlic, crushed (or\n1 tablespoon, minced) 2. Add onion and tomatoes. Sauté\nuntil skin begins to curl.\n• 1 cup white onions, chopped\n3. Add pork, and sauté until lightly\n• 1 cup ripe tomatoes, chopped\nbrown.\n• 1 cup (4 ounces) lean pork, thinly\n4. Add water, and simmer pork for\nsliced\nabout 15 minutes.\n• 4 cups water\n5. Add the sautéed mix to mung\n• 3½ cups precooked mung beans\nbeans, and continue to simmer\n(from 1¾ cups dry beans)*\n15 minutes.\n• 1 teaspoon salt\n6. Season with salt and ground\n• 1 teaspoon ground pepper\npepper.\n• 1 cup (4 ounces) shrimp, peeled\n7. Add peeled shrimp.\nand deveined\n8. Add frozen leaf spinach, and\n• 1 cup (about ⅔ of a 1",
        "0-ounce\ncook 4 minutes until done.\npackage) leaf spinach, frozen\n* To cook dry, uncooked mung beans:\nWash and boil the uncooked mung\nbeans in a large saucepan, using 6 Nutrition Information\ncups of water. Cook until tender, about\n1½ to 2 hours. Drain. Makes 8 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 160\nTotal Fat: 3.5 g\nSaturated Fat: 1 g\nCholesterol: 35 mg\nSodium: 350 mg\nSource: Filipino American Food Practices, Customs,\nand Holidays, American Dietetic Association, 1994. Total Fiber: 8 g\nProtein: 13 g\nCarbohydrates: 19 g\nPotassium: 370 mg\n5\nAmpalaya (Bitter Melon) With Pork\nThis recipe is lower in fat and sodium than a typical ampalaya dish\nbecause it uses lean meat that is sautéed and simmered instead of fried.\nIngredients Directions\n• 1 cup onion, chopped 1. Using a large skillet, lightly sauté\nonions and garlic in hot olive oil.\n• 6 cloves garlic, crushed\n2. Add the ground pork and cook until\n• 1 tablespoon olive oil\nalmost done.\n• ½ pound (0.2 kg) lean ground ",
        "pork\n3. Add the sliced bitter melon.\n• 2 cups Ampalaya*, sliced\n4. Cover and simmer until bitter melon\n• 2 teaspoons light soy sauce\nturns green. Do not overcook.\n• ½ teaspoon black pepper\n5. Season with light soy sauce and\nblack pepper.\n* Ampalaya (bitter melon) is a fruit that is\noblong, cylindrical, pointed at both ends,\nribbed, and wrinkled.\nNutrition Information\nMakes 4 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 150\nTotal Fat: 6 g\nSaturated Fat: 1.5 g\nCholesterol: 45 mg\nSodium: 200 mg\nTotal Fiber: 1 g\nProtein: 17 g\nCarbohydrates: 7 g\nPotassium: 600 mg\nSource: Adapted from Mula Sa Puso, Heart Healthy Traditional Filipino Recipes, American Heart Association, 1999.\n6\nCantaloupe Crush\nTry this refreshing, heart healthy drink that uses fresh fruit, fat-free milk, and low\namounts of sweetener. Children and adults alike will love it!\nIngredients Directions\n• ½ cantaloupe 1. Cut cantaloupe into small\ncubes or thin strips.\n• 1 cup fat-free milk\n2. Mix cantaloupe, milk, a",
        "nd\n• 1½ cups ice\nice in a blender until smooth.\n• Sweetener, as needed\n3. Sweeten to taste.\n(about 1 to 2 teaspoons\nsugar or equivalent of\nanother sweetener)\nNutrition Information\nMakes 4 servings\nServing size: ½ cup\nEach serving provides:\nCalories: 50\nTotal Fat: 0 g\nSaturated Fat: 0 g\nCholesterol: 0 mg\nSodium: 40 mg\nTotal Fiber: 0 g\nProtein: 3 g\nCarbohydrates: 10 g\nPotassium: 280 mg\nSource: Adapted from the National Cancer Institute and InteliHealth (intelihealth.com), 2013.\n7\nVegetable Kare-Kare (Peanut Stew)\nThis version of vegetable kare-kare is healthier than the traditional Filipino dish\nbecause it has no cholesterol. It uses gluten instead of oxtail or other meat. It is also\npacked with vegetables and made complete with a nutty, low-sodium sauce.\nIngredients\n• 1 ¼ cup gluten or seitan,* cubes\n• 2 tablespoons corn oil\n• 2 cloves garlic, crushed\nNutrition Information\n• 1 onion, medium, sliced\n• ½ cup ground peanuts Makes 6 servings\n• ¼ cup ground toasted rice**\nEach serving provid",
        "es:\n• ¼ teaspoon salt\nCalories: 300\n• 1 cup eggplant, sliced\nTotal Fat: 12 g\n• ½ cup string beans, sliced Saturated Fat: 1.5 g\n• ⅔ cup banana heart or bud Cholesterol: 0 mg\nSodium: 125 mg\n• ½ cup bok choy (pechay), sliced\nTotal Fiber: 4 g\n* Gluten is made from protein that is in a variety of grains, such Protein: 36 g\nas wheat and rye, and is mixed and kneaded with water. Seitan\nCarbohydrates: 20 g\nis a form of wheat gluten. It is sold as strips or in cans at health\nPotassium: 320 mg\nfood stores and Asian supermarkets.\n** To make ground, toasted rice: Place rice, ½ cup at a time,\nin a frying pan or wok and heat over moderate heat, stirring\nfrequently to keep it from burning and to allow it to develop a\nuniform, deep golden color—2 to 3 minutes. Then remove it\nfrom heat and cool to room temperature. Grind the toasted rice\ncoarsely—not finely grounded—in a blende, or spice or coffee\ngrinder.\nDirections\n1. Sauté gluten cubes in corn oil. Add garlic and onions.\n2. Pour enough water to cove",
        "r gluten, and add ground\npeanuts and ground rice to thicken.\n3. Season with salt.\n4. Add the eggplant, then string beans, then banana,\nthen bok choy (pechay) on top of the cooked gluten.\nSource: PHC Alive Diet, Division of Nutrition and Dietetics, Philippine Heart Center, East Avenue, Quezon City, Philippines, page 91.\n8\nNational Heart, Lung, and Blood Institute\nAn Initiative of the to help reduce health disparities in cardiovascular disease\nand asthma in underserved and minority communities. For more information, visit www.nhlbi.nih.gov/health/healthdisp."
      ],
      "uploaded_at": "2025-07-25T19:51:26.933470",
      "source": "pdf_upload"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "chunks": [
        "Tasty and Healthy —\nHeart Healthy Filipino Recipes\nFish Cardillo\nThis is a delicious, low-cost recipe with low-sodium ingredients.\nKeep it low-fat by not adding meat fat (lard) or other fat.\nIngredients Directions\n1. Thoroughly clean fish. Remove scale\n• 1 pound (½ kg) red snapper\nand gills, and wash thoroughly. Drain and\n• 4 teaspoons corn oil for sauté\nset aside.\n• ¼ cup flou\n2. Slice the raw fish into six pieces\n• 1 large onion, sliced\n3. Heat corn oil in frying pan.\n• 3 or 4 medium-sized tomatoes,\n4. Place the flour into a bowl or plastic bag\nchopped\nPlace the raw fish in the flour and cov\n• ½ cup egg whites, beaten the outside of each fish with flo .\n• ½ cup water 5. Sauté fish until golden brown. Set asid\n• A dash ground pepper on top of a paper towel.\n• 15 stalks green onions, chopped 6. Sauté onion and tomatoes. Add ½ cup of\nwater.\n7. Add the beaten egg whites and fish\nCover and let it simmer for 5–10 minutes.\nNutrition Information\n8. Season with ground pepper.\nMakes 6 servings",
        " 9. Sprinkle with chopped green onions.\nEach serving provides:\nCalories: 170\nTotal Fat: 4 g\nSaturated Fat: 1 g\nCholesterol: 45 mg\nQuick Tip\nSodium: 115 mg\nTotal Fiber: 3 g\nProtein: 20 g\nThis recipe is lower in salt and sodium than other\nCarbohydrates: 13 g\nCardillo recipes because it uses:\nPotassium: 600 mg\n• Fresh, not canned, tomatoes\n• Ground pepper and corn oil with no salt added\n• Fresh onion and green onions\n• Fresh, not smoked or canned, fis\nSource: Philippine Heart Center’s Healthy Heart Cookbook.\n1\nAdobong Manok (Marinated Chicken)\nThis low-cost, low-sodium recipe has great flavor that you and your\nfamily will love! It uses chicken breast which is lower in fat than\nother parts of the chicken, like the thigh and leg.\nIngredients Directions\n• 1 teaspoon olive oil 1. Combine olive oil, garlic, and onion in a frying\npan. Add chicken and sauté together until\n• 2 cloves fresh crushed garlic\nchicken has browned.\n• 2 medium chopped onions\n2. Add light soy sauce, vinegar, paprika,\n• 1 ",
        "pound (½ kg) chicken breasts, no skin\nblack pepper, and bay leaf. Stir.\n• 2 tablespoons light soy sauce\n3. Bring to a boil. Simmer for 45–60 minutes or\n• ¼ cup vinegar until chicken is done.\n• 1 teaspoon paprika 4. Remove the chicken and save the liquid in the\npot. Arrange the chicken on a broiler pan. Broil\n• 2 tablespoons ground black pepper\nuntil the chicken has nicely browned. Remove\n• 1 bay leaf, broken in half\nfrom the broiler and place it in a serving bowl.\n• 1 medium red tomato (optional)\n5. Continue to boil the sauce in the uncovered\npan until volume is reduced to about half and\nthe sauce is thick.\nNutrition Information\n6. Pour the thickened sauce over broiled\nadobong (chicken) and garnish with red\nMakes 4 servings\ntomatoes, if desired.\nServing size: ½ cup\nEach serving provides:\nCalories: 190\nTotal Fat: 5 g\nQuick Tip\nSaturated Fat: 1 g\nCholesterol: 70 mg\nSodium: 330 mg This recipe is lower in saturated fat and cholesterol because:\nTotal Fiber: 2 g\n• It is made using chicken wi",
        "thout the skin and any extra fat\nProtein: 26 g\nis removed.\nCarbohydrates: 10 g\nPotassium: 370 mg • Only 1 teaspoon of unsaturated fat (olive oil) is added.\n• It is flavored with vegetables and herbs and is boiled an\nbroiled slowly in moist heat instead of fat.\nSource: Filipino-American Nutrition and Fitness Teacher’s\nGuide, Kalusugan Community Services, San Diego, CA.\n2\nLumpiang Sariwa (Fresh Lumpia)\nYou and your family will love this tasty recipe. The ingredients—ground chicken\nor pork, olive oil, peanuts, and fresh herbs and spices—add flavor. Also, the\nlumpiang sariwa is served fresh so it has fewer calories than fried lumpiang.\nIngredients Directions\n• ½ cup ground chicken or lean pork 1. Heat oil, and sauté ground meat with the\nshrimp and garlic.\n• ½ cup shrimp, cleaned and deveined\n2. Add vegetables until slightly crisp. Pour in\n• 1 tablespoon olive oil\nthe chicken broth until cooked.\n• 2 cloves chopped garlic\n3. Season with salt and pepper.\n• ½ cup cabbage, julienned\n4. Set asid",
        "e and drain in a colander.\n• ½ cup green beans, julienned\n5. Save the broth for the lumpia sauce.\n• ½ cup carrots, julienned\n6. Soak the Vietnamese spring roll wrappers\n• ¼ cup celery, julienned\none at a time in water until soft and\n• ¼ cup jicama, julienned (may substitute transparent. Dry immediately with a paper\nchestnuts) towel.\n• ½ cup chicken broth 7. Lay the lettuce on the wrapper.\n• ¼ teaspoon salt 8. Place 2 tablespoons of the vegetable\n• ¼ teaspoon pepper mixture on the wrapper.\n• 8 Vietnamese spring-roll wrappers or 9. Fold in one side of the wrapper and roll\nlumpia wrappers tightly.\n• 8 pieces red leaf lettuce 10. Serve with lumpia sauce on top.\nSprinkle with chopped peanuts.\n• ⅓ cup peanuts, dry, roasted, and chopped\nNutrition Information\nLumpia Sauce\nMakes 8 servings\n• 1 cup broth from the sautéed vegetables Serving size: 1 lumpia\n• 1 tablespoon light soy sauce\nEach serving provides:\n• 1 tablespoon brown sugar\nCalories: 160\n• 3 cloves garlic, minced\nTotal Fat: 4 g\n• 1 tea",
        "spoon cornstarch Saturated Fat: 0.5 g\n• 2 tablespoons cold water for mixing cornstarch Cholesterol: 55 mg\nSodium: 150 mg\nDirections Total Fiber: 2 g\n1. Mix together vegetable broth, soy sauce, brown Protein: 10 g\nsugar, and garlic, and bring to a boil. Carbohydrates: 21 g\nPotassium: 170 mg\n2. Mix the cornstarch in 2 tablespoons of cold water.\n3. Slowly add the cornstarch mixture to the broth.\nStir until sauce thickens.\nSource: Mula sa Puso, Heart Healthy Traditional\nFilipino Recipes, American Heart Association, 1999.\n3\nPesang Isda (Fish Simmered With\nGinger and Tomatoes)\nThis main dish is heart healthy because the fish is simmered in water,\nnot fried, and no fat is added. Flavoring comes from the herbs and spices\ninstead of sauces that are high in sodium.\nIngredients Directions\n1. In a 4-quart saucepan,\n• ¼ cup fresh ginger, thinly\nsimmer sliced ginger,\nsliced (about 2 inches long)\ntomatoes, and onions in\n• 1 cup ripe tomatoes, chopped\n4 cups of water over medium\n• 1 cup white or yello",
        "w onions, heat until onions are tender\nthinly sliced (about 7 to 8 minutes).\n• 4 cups water 2. Reduce heat to low, add fish\n• 2 pounds fleshy fish (c and poach gently until almost\nfillet, halibut steak, or trout done (about 3 to 4 minutes).\n• 2 cups pechay (bok choy) 3. Add pechay stems, salt, and\nstems and leaves, cut up ground pepper. Cook for\nseparately 1 minute; then add pechay\nleaves and green onions.\n• ½ teaspoon salt\nCook another 30 seconds.\n• ½ teaspoon ground pepper\n4. Serve immediately.\n• 1 cup green onions, sliced\nNutrition Information\nMakes 6 servings\nServing size: 3 ounces lean\nfish and ½ cup vegetables\nEach serving provides:\nCalories: 160\nTotal Fat: 2 g\nSaturated Fat: 0.5 g\nCholesterol: 80 mg\nSodium: 340 mg\nTotal Fiber: 2 g\nProtein: 30 g\nCarbohydrates: 6 g\nPotassium: 630 mg\nSource: Filipino American Food Practices, Customs, and Holidays, American Dietetic Association, 1994.\n4\nMunggo Guisado (Sautéed Mung Beans)\nYour family will love this heart healthy side dish. It is mad",
        "e with vegetables,\nseafood, lean meat, and a small amount of corn oil. The pork is slowly\nsimmered in moist heat with vegetables and mung beans, creating flavors\nthat will make your taste buds jump for joy!\nIngredients Directions\n• 1 tablespoon corn oil 1. In a skillet, heat oil, and sauté\ncrushed garlic until lightly brown.\n• 2 cloves fresh garlic, crushed (or\n1 tablespoon, minced) 2. Add onion and tomatoes. Sauté\nuntil skin begins to curl.\n• 1 cup white onions, chopped\n3. Add pork, and sauté until lightly\n• 1 cup ripe tomatoes, chopped\nbrown.\n• 1 cup (4 ounces) lean pork, thinly\n4. Add water, and simmer pork for\nsliced\nabout 15 minutes.\n• 4 cups water\n5. Add the sautéed mix to mung\n• 3½ cups precooked mung beans\nbeans, and continue to simmer\n(from 1¾ cups dry beans)*\n15 minutes.\n• 1 teaspoon salt\n6. Season with salt and ground\n• 1 teaspoon ground pepper\npepper.\n• 1 cup (4 ounces) shrimp, peeled\n7. Add peeled shrimp.\nand deveined\n8. Add frozen leaf spinach, and\n• 1 cup (about ⅔ of a 1",
        "0-ounce\ncook 4 minutes until done.\npackage) leaf spinach, frozen\n* To cook dry, uncooked mung beans:\nWash and boil the uncooked mung\nbeans in a large saucepan, using 6 Nutrition Information\ncups of water. Cook until tender, about\n1½ to 2 hours. Drain. Makes 8 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 160\nTotal Fat: 3.5 g\nSaturated Fat: 1 g\nCholesterol: 35 mg\nSodium: 350 mg\nSource: Filipino American Food Practices, Customs,\nand Holidays, American Dietetic Association, 1994. Total Fiber: 8 g\nProtein: 13 g\nCarbohydrates: 19 g\nPotassium: 370 mg\n5\nAmpalaya (Bitter Melon) With Pork\nThis recipe is lower in fat and sodium than a typical ampalaya dish\nbecause it uses lean meat that is sautéed and simmered instead of fried.\nIngredients Directions\n• 1 cup onion, chopped 1. Using a large skillet, lightly sauté\nonions and garlic in hot olive oil.\n• 6 cloves garlic, crushed\n2. Add the ground pork and cook until\n• 1 tablespoon olive oil\nalmost done.\n• ½ pound (0.2 kg) lean ground ",
        "pork\n3. Add the sliced bitter melon.\n• 2 cups Ampalaya*, sliced\n4. Cover and simmer until bitter melon\n• 2 teaspoons light soy sauce\nturns green. Do not overcook.\n• ½ teaspoon black pepper\n5. Season with light soy sauce and\nblack pepper.\n* Ampalaya (bitter melon) is a fruit that is\noblong, cylindrical, pointed at both ends,\nribbed, and wrinkled.\nNutrition Information\nMakes 4 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 150\nTotal Fat: 6 g\nSaturated Fat: 1.5 g\nCholesterol: 45 mg\nSodium: 200 mg\nTotal Fiber: 1 g\nProtein: 17 g\nCarbohydrates: 7 g\nPotassium: 600 mg\nSource: Adapted from Mula Sa Puso, Heart Healthy Traditional Filipino Recipes, American Heart Association, 1999.\n6\nCantaloupe Crush\nTry this refreshing, heart healthy drink that uses fresh fruit, fat-free milk, and low\namounts of sweetener. Children and adults alike will love it!\nIngredients Directions\n• ½ cantaloupe 1. Cut cantaloupe into small\ncubes or thin strips.\n• 1 cup fat-free milk\n2. Mix cantaloupe, milk, a",
        "nd\n• 1½ cups ice\nice in a blender until smooth.\n• Sweetener, as needed\n3. Sweeten to taste.\n(about 1 to 2 teaspoons\nsugar or equivalent of\nanother sweetener)\nNutrition Information\nMakes 4 servings\nServing size: ½ cup\nEach serving provides:\nCalories: 50\nTotal Fat: 0 g\nSaturated Fat: 0 g\nCholesterol: 0 mg\nSodium: 40 mg\nTotal Fiber: 0 g\nProtein: 3 g\nCarbohydrates: 10 g\nPotassium: 280 mg\nSource: Adapted from the National Cancer Institute and InteliHealth (intelihealth.com), 2013.\n7\nVegetable Kare-Kare (Peanut Stew)\nThis version of vegetable kare-kare is healthier than the traditional Filipino dish\nbecause it has no cholesterol. It uses gluten instead of oxtail or other meat. It is also\npacked with vegetables and made complete with a nutty, low-sodium sauce.\nIngredients\n• 1 ¼ cup gluten or seitan,* cubes\n• 2 tablespoons corn oil\n• 2 cloves garlic, crushed\nNutrition Information\n• 1 onion, medium, sliced\n• ½ cup ground peanuts Makes 6 servings\n• ¼ cup ground toasted rice**\nEach serving provid",
        "es:\n• ¼ teaspoon salt\nCalories: 300\n• 1 cup eggplant, sliced\nTotal Fat: 12 g\n• ½ cup string beans, sliced Saturated Fat: 1.5 g\n• ⅔ cup banana heart or bud Cholesterol: 0 mg\nSodium: 125 mg\n• ½ cup bok choy (pechay), sliced\nTotal Fiber: 4 g\n* Gluten is made from protein that is in a variety of grains, such Protein: 36 g\nas wheat and rye, and is mixed and kneaded with water. Seitan\nCarbohydrates: 20 g\nis a form of wheat gluten. It is sold as strips or in cans at health\nPotassium: 320 mg\nfood stores and Asian supermarkets.\n** To make ground, toasted rice: Place rice, ½ cup at a time,\nin a frying pan or wok and heat over moderate heat, stirring\nfrequently to keep it from burning and to allow it to develop a\nuniform, deep golden color—2 to 3 minutes. Then remove it\nfrom heat and cool to room temperature. Grind the toasted rice\ncoarsely—not finely grounded—in a blende, or spice or coffee\ngrinder.\nDirections\n1. Sauté gluten cubes in corn oil. Add garlic and onions.\n2. Pour enough water to cove",
        "r gluten, and add ground\npeanuts and ground rice to thicken.\n3. Season with salt.\n4. Add the eggplant, then string beans, then banana,\nthen bok choy (pechay) on top of the cooked gluten.\nSource: PHC Alive Diet, Division of Nutrition and Dietetics, Philippine Heart Center, East Avenue, Quezon City, Philippines, page 91.\n8\nNational Heart, Lung, and Blood Institute\nAn Initiative of the to help reduce health disparities in cardiovascular disease\nand asthma in underserved and minority communities. For more information, visit www.nhlbi.nih.gov/health/healthdisp."
      ],
      "uploaded_at": "2025-07-25T19:51:32.400528",
      "source": "pdf_upload"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "chunks": [
        "Tasty and Healthy —\nHeart Healthy Filipino Recipes\nFish Cardillo\nThis is a delicious, low-cost recipe with low-sodium ingredients.\nKeep it low-fat by not adding meat fat (lard) or other fat.\nIngredients Directions\n1. Thoroughly clean fish. Remove scale\n• 1 pound (½ kg) red snapper\nand gills, and wash thoroughly. Drain and\n• 4 teaspoons corn oil for sauté\nset aside.\n• ¼ cup flou\n2. Slice the raw fish into six pieces\n• 1 large onion, sliced\n3. Heat corn oil in frying pan.\n• 3 or 4 medium-sized tomatoes,\n4. Place the flour into a bowl or plastic bag\nchopped\nPlace the raw fish in the flour and cov\n• ½ cup egg whites, beaten the outside of each fish with flo .\n• ½ cup water 5. Sauté fish until golden brown. Set asid\n• A dash ground pepper on top of a paper towel.\n• 15 stalks green onions, chopped 6. Sauté onion and tomatoes. Add ½ cup of\nwater.\n7. Add the beaten egg whites and fish\nCover and let it simmer for 5–10 minutes.\nNutrition Information\n8. Season with ground pepper.\nMakes 6 servings",
        " 9. Sprinkle with chopped green onions.\nEach serving provides:\nCalories: 170\nTotal Fat: 4 g\nSaturated Fat: 1 g\nCholesterol: 45 mg\nQuick Tip\nSodium: 115 mg\nTotal Fiber: 3 g\nProtein: 20 g\nThis recipe is lower in salt and sodium than other\nCarbohydrates: 13 g\nCardillo recipes because it uses:\nPotassium: 600 mg\n• Fresh, not canned, tomatoes\n• Ground pepper and corn oil with no salt added\n• Fresh onion and green onions\n• Fresh, not smoked or canned, fis\nSource: Philippine Heart Center’s Healthy Heart Cookbook.\n1\nAdobong Manok (Marinated Chicken)\nThis low-cost, low-sodium recipe has great flavor that you and your\nfamily will love! It uses chicken breast which is lower in fat than\nother parts of the chicken, like the thigh and leg.\nIngredients Directions\n• 1 teaspoon olive oil 1. Combine olive oil, garlic, and onion in a frying\npan. Add chicken and sauté together until\n• 2 cloves fresh crushed garlic\nchicken has browned.\n• 2 medium chopped onions\n2. Add light soy sauce, vinegar, paprika,\n• 1 ",
        "pound (½ kg) chicken breasts, no skin\nblack pepper, and bay leaf. Stir.\n• 2 tablespoons light soy sauce\n3. Bring to a boil. Simmer for 45–60 minutes or\n• ¼ cup vinegar until chicken is done.\n• 1 teaspoon paprika 4. Remove the chicken and save the liquid in the\npot. Arrange the chicken on a broiler pan. Broil\n• 2 tablespoons ground black pepper\nuntil the chicken has nicely browned. Remove\n• 1 bay leaf, broken in half\nfrom the broiler and place it in a serving bowl.\n• 1 medium red tomato (optional)\n5. Continue to boil the sauce in the uncovered\npan until volume is reduced to about half and\nthe sauce is thick.\nNutrition Information\n6. Pour the thickened sauce over broiled\nadobong (chicken) and garnish with red\nMakes 4 servings\ntomatoes, if desired.\nServing size: ½ cup\nEach serving provides:\nCalories: 190\nTotal Fat: 5 g\nQuick Tip\nSaturated Fat: 1 g\nCholesterol: 70 mg\nSodium: 330 mg This recipe is lower in saturated fat and cholesterol because:\nTotal Fiber: 2 g\n• It is made using chicken wi",
        "thout the skin and any extra fat\nProtein: 26 g\nis removed.\nCarbohydrates: 10 g\nPotassium: 370 mg • Only 1 teaspoon of unsaturated fat (olive oil) is added.\n• It is flavored with vegetables and herbs and is boiled an\nbroiled slowly in moist heat instead of fat.\nSource: Filipino-American Nutrition and Fitness Teacher’s\nGuide, Kalusugan Community Services, San Diego, CA.\n2\nLumpiang Sariwa (Fresh Lumpia)\nYou and your family will love this tasty recipe. The ingredients—ground chicken\nor pork, olive oil, peanuts, and fresh herbs and spices—add flavor. Also, the\nlumpiang sariwa is served fresh so it has fewer calories than fried lumpiang.\nIngredients Directions\n• ½ cup ground chicken or lean pork 1. Heat oil, and sauté ground meat with the\nshrimp and garlic.\n• ½ cup shrimp, cleaned and deveined\n2. Add vegetables until slightly crisp. Pour in\n• 1 tablespoon olive oil\nthe chicken broth until cooked.\n• 2 cloves chopped garlic\n3. Season with salt and pepper.\n• ½ cup cabbage, julienned\n4. Set asid",
        "e and drain in a colander.\n• ½ cup green beans, julienned\n5. Save the broth for the lumpia sauce.\n• ½ cup carrots, julienned\n6. Soak the Vietnamese spring roll wrappers\n• ¼ cup celery, julienned\none at a time in water until soft and\n• ¼ cup jicama, julienned (may substitute transparent. Dry immediately with a paper\nchestnuts) towel.\n• ½ cup chicken broth 7. Lay the lettuce on the wrapper.\n• ¼ teaspoon salt 8. Place 2 tablespoons of the vegetable\n• ¼ teaspoon pepper mixture on the wrapper.\n• 8 Vietnamese spring-roll wrappers or 9. Fold in one side of the wrapper and roll\nlumpia wrappers tightly.\n• 8 pieces red leaf lettuce 10. Serve with lumpia sauce on top.\nSprinkle with chopped peanuts.\n• ⅓ cup peanuts, dry, roasted, and chopped\nNutrition Information\nLumpia Sauce\nMakes 8 servings\n• 1 cup broth from the sautéed vegetables Serving size: 1 lumpia\n• 1 tablespoon light soy sauce\nEach serving provides:\n• 1 tablespoon brown sugar\nCalories: 160\n• 3 cloves garlic, minced\nTotal Fat: 4 g\n• 1 tea",
        "spoon cornstarch Saturated Fat: 0.5 g\n• 2 tablespoons cold water for mixing cornstarch Cholesterol: 55 mg\nSodium: 150 mg\nDirections Total Fiber: 2 g\n1. Mix together vegetable broth, soy sauce, brown Protein: 10 g\nsugar, and garlic, and bring to a boil. Carbohydrates: 21 g\nPotassium: 170 mg\n2. Mix the cornstarch in 2 tablespoons of cold water.\n3. Slowly add the cornstarch mixture to the broth.\nStir until sauce thickens.\nSource: Mula sa Puso, Heart Healthy Traditional\nFilipino Recipes, American Heart Association, 1999.\n3\nPesang Isda (Fish Simmered With\nGinger and Tomatoes)\nThis main dish is heart healthy because the fish is simmered in water,\nnot fried, and no fat is added. Flavoring comes from the herbs and spices\ninstead of sauces that are high in sodium.\nIngredients Directions\n1. In a 4-quart saucepan,\n• ¼ cup fresh ginger, thinly\nsimmer sliced ginger,\nsliced (about 2 inches long)\ntomatoes, and onions in\n• 1 cup ripe tomatoes, chopped\n4 cups of water over medium\n• 1 cup white or yello",
        "w onions, heat until onions are tender\nthinly sliced (about 7 to 8 minutes).\n• 4 cups water 2. Reduce heat to low, add fish\n• 2 pounds fleshy fish (c and poach gently until almost\nfillet, halibut steak, or trout done (about 3 to 4 minutes).\n• 2 cups pechay (bok choy) 3. Add pechay stems, salt, and\nstems and leaves, cut up ground pepper. Cook for\nseparately 1 minute; then add pechay\nleaves and green onions.\n• ½ teaspoon salt\nCook another 30 seconds.\n• ½ teaspoon ground pepper\n4. Serve immediately.\n• 1 cup green onions, sliced\nNutrition Information\nMakes 6 servings\nServing size: 3 ounces lean\nfish and ½ cup vegetables\nEach serving provides:\nCalories: 160\nTotal Fat: 2 g\nSaturated Fat: 0.5 g\nCholesterol: 80 mg\nSodium: 340 mg\nTotal Fiber: 2 g\nProtein: 30 g\nCarbohydrates: 6 g\nPotassium: 630 mg\nSource: Filipino American Food Practices, Customs, and Holidays, American Dietetic Association, 1994.\n4\nMunggo Guisado (Sautéed Mung Beans)\nYour family will love this heart healthy side dish. It is mad",
        "e with vegetables,\nseafood, lean meat, and a small amount of corn oil. The pork is slowly\nsimmered in moist heat with vegetables and mung beans, creating flavors\nthat will make your taste buds jump for joy!\nIngredients Directions\n• 1 tablespoon corn oil 1. In a skillet, heat oil, and sauté\ncrushed garlic until lightly brown.\n• 2 cloves fresh garlic, crushed (or\n1 tablespoon, minced) 2. Add onion and tomatoes. Sauté\nuntil skin begins to curl.\n• 1 cup white onions, chopped\n3. Add pork, and sauté until lightly\n• 1 cup ripe tomatoes, chopped\nbrown.\n• 1 cup (4 ounces) lean pork, thinly\n4. Add water, and simmer pork for\nsliced\nabout 15 minutes.\n• 4 cups water\n5. Add the sautéed mix to mung\n• 3½ cups precooked mung beans\nbeans, and continue to simmer\n(from 1¾ cups dry beans)*\n15 minutes.\n• 1 teaspoon salt\n6. Season with salt and ground\n• 1 teaspoon ground pepper\npepper.\n• 1 cup (4 ounces) shrimp, peeled\n7. Add peeled shrimp.\nand deveined\n8. Add frozen leaf spinach, and\n• 1 cup (about ⅔ of a 1",
        "0-ounce\ncook 4 minutes until done.\npackage) leaf spinach, frozen\n* To cook dry, uncooked mung beans:\nWash and boil the uncooked mung\nbeans in a large saucepan, using 6 Nutrition Information\ncups of water. Cook until tender, about\n1½ to 2 hours. Drain. Makes 8 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 160\nTotal Fat: 3.5 g\nSaturated Fat: 1 g\nCholesterol: 35 mg\nSodium: 350 mg\nSource: Filipino American Food Practices, Customs,\nand Holidays, American Dietetic Association, 1994. Total Fiber: 8 g\nProtein: 13 g\nCarbohydrates: 19 g\nPotassium: 370 mg\n5\nAmpalaya (Bitter Melon) With Pork\nThis recipe is lower in fat and sodium than a typical ampalaya dish\nbecause it uses lean meat that is sautéed and simmered instead of fried.\nIngredients Directions\n• 1 cup onion, chopped 1. Using a large skillet, lightly sauté\nonions and garlic in hot olive oil.\n• 6 cloves garlic, crushed\n2. Add the ground pork and cook until\n• 1 tablespoon olive oil\nalmost done.\n• ½ pound (0.2 kg) lean ground ",
        "pork\n3. Add the sliced bitter melon.\n• 2 cups Ampalaya*, sliced\n4. Cover and simmer until bitter melon\n• 2 teaspoons light soy sauce\nturns green. Do not overcook.\n• ½ teaspoon black pepper\n5. Season with light soy sauce and\nblack pepper.\n* Ampalaya (bitter melon) is a fruit that is\noblong, cylindrical, pointed at both ends,\nribbed, and wrinkled.\nNutrition Information\nMakes 4 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 150\nTotal Fat: 6 g\nSaturated Fat: 1.5 g\nCholesterol: 45 mg\nSodium: 200 mg\nTotal Fiber: 1 g\nProtein: 17 g\nCarbohydrates: 7 g\nPotassium: 600 mg\nSource: Adapted from Mula Sa Puso, Heart Healthy Traditional Filipino Recipes, American Heart Association, 1999.\n6\nCantaloupe Crush\nTry this refreshing, heart healthy drink that uses fresh fruit, fat-free milk, and low\namounts of sweetener. Children and adults alike will love it!\nIngredients Directions\n• ½ cantaloupe 1. Cut cantaloupe into small\ncubes or thin strips.\n• 1 cup fat-free milk\n2. Mix cantaloupe, milk, a",
        "nd\n• 1½ cups ice\nice in a blender until smooth.\n• Sweetener, as needed\n3. Sweeten to taste.\n(about 1 to 2 teaspoons\nsugar or equivalent of\nanother sweetener)\nNutrition Information\nMakes 4 servings\nServing size: ½ cup\nEach serving provides:\nCalories: 50\nTotal Fat: 0 g\nSaturated Fat: 0 g\nCholesterol: 0 mg\nSodium: 40 mg\nTotal Fiber: 0 g\nProtein: 3 g\nCarbohydrates: 10 g\nPotassium: 280 mg\nSource: Adapted from the National Cancer Institute and InteliHealth (intelihealth.com), 2013.\n7\nVegetable Kare-Kare (Peanut Stew)\nThis version of vegetable kare-kare is healthier than the traditional Filipino dish\nbecause it has no cholesterol. It uses gluten instead of oxtail or other meat. It is also\npacked with vegetables and made complete with a nutty, low-sodium sauce.\nIngredients\n• 1 ¼ cup gluten or seitan,* cubes\n• 2 tablespoons corn oil\n• 2 cloves garlic, crushed\nNutrition Information\n• 1 onion, medium, sliced\n• ½ cup ground peanuts Makes 6 servings\n• ¼ cup ground toasted rice**\nEach serving provid",
        "es:\n• ¼ teaspoon salt\nCalories: 300\n• 1 cup eggplant, sliced\nTotal Fat: 12 g\n• ½ cup string beans, sliced Saturated Fat: 1.5 g\n• ⅔ cup banana heart or bud Cholesterol: 0 mg\nSodium: 125 mg\n• ½ cup bok choy (pechay), sliced\nTotal Fiber: 4 g\n* Gluten is made from protein that is in a variety of grains, such Protein: 36 g\nas wheat and rye, and is mixed and kneaded with water. Seitan\nCarbohydrates: 20 g\nis a form of wheat gluten. It is sold as strips or in cans at health\nPotassium: 320 mg\nfood stores and Asian supermarkets.\n** To make ground, toasted rice: Place rice, ½ cup at a time,\nin a frying pan or wok and heat over moderate heat, stirring\nfrequently to keep it from burning and to allow it to develop a\nuniform, deep golden color—2 to 3 minutes. Then remove it\nfrom heat and cool to room temperature. Grind the toasted rice\ncoarsely—not finely grounded—in a blende, or spice or coffee\ngrinder.\nDirections\n1. Sauté gluten cubes in corn oil. Add garlic and onions.\n2. Pour enough water to cove",
        "r gluten, and add ground\npeanuts and ground rice to thicken.\n3. Season with salt.\n4. Add the eggplant, then string beans, then banana,\nthen bok choy (pechay) on top of the cooked gluten.\nSource: PHC Alive Diet, Division of Nutrition and Dietetics, Philippine Heart Center, East Avenue, Quezon City, Philippines, page 91.\n8\nNational Heart, Lung, and Blood Institute\nAn Initiative of the to help reduce health disparities in cardiovascular disease\nand asthma in underserved and minority communities. For more information, visit www.nhlbi.nih.gov/health/healthdisp."
      ],
      "uploaded_at": "2025-07-25T19:51:39.166555",
      "source": "pdf_upload"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "chunks": [
        "Tasty and Healthy —\nHeart Healthy Filipino Recipes\nFish Cardillo\nThis is a delicious, low-cost recipe with low-sodium ingredients.\nKeep it low-fat by not adding meat fat (lard) or other fat.\nIngredients Directions\n1. Thoroughly clean fish. Remove scale\n• 1 pound (½ kg) red snapper\nand gills, and wash thoroughly. Drain and\n• 4 teaspoons corn oil for sauté\nset aside.\n• ¼ cup flou\n2. Slice the raw fish into six pieces\n• 1 large onion, sliced\n3. Heat corn oil in frying pan.\n• 3 or 4 medium-sized tomatoes,\n4. Place the flour into a bowl or plastic bag\nchopped\nPlace the raw fish in the flour and cov\n• ½ cup egg whites, beaten the outside of each fish with flo .\n• ½ cup water 5. Sauté fish until golden brown. Set asid\n• A dash ground pepper on top of a paper towel.\n• 15 stalks green onions, chopped 6. Sauté onion and tomatoes. Add ½ cup of\nwater.\n7. Add the beaten egg whites and fish\nCover and let it simmer for 5–10 minutes.\nNutrition Information\n8. Season with ground pepper.\nMakes 6 servings",
        " 9. Sprinkle with chopped green onions.\nEach serving provides:\nCalories: 170\nTotal Fat: 4 g\nSaturated Fat: 1 g\nCholesterol: 45 mg\nQuick Tip\nSodium: 115 mg\nTotal Fiber: 3 g\nProtein: 20 g\nThis recipe is lower in salt and sodium than other\nCarbohydrates: 13 g\nCardillo recipes because it uses:\nPotassium: 600 mg\n• Fresh, not canned, tomatoes\n• Ground pepper and corn oil with no salt added\n• Fresh onion and green onions\n• Fresh, not smoked or canned, fis\nSource: Philippine Heart Center’s Healthy Heart Cookbook.\n1\nAdobong Manok (Marinated Chicken)\nThis low-cost, low-sodium recipe has great flavor that you and your\nfamily will love! It uses chicken breast which is lower in fat than\nother parts of the chicken, like the thigh and leg.\nIngredients Directions\n• 1 teaspoon olive oil 1. Combine olive oil, garlic, and onion in a frying\npan. Add chicken and sauté together until\n• 2 cloves fresh crushed garlic\nchicken has browned.\n• 2 medium chopped onions\n2. Add light soy sauce, vinegar, paprika,\n• 1 ",
        "pound (½ kg) chicken breasts, no skin\nblack pepper, and bay leaf. Stir.\n• 2 tablespoons light soy sauce\n3. Bring to a boil. Simmer for 45–60 minutes or\n• ¼ cup vinegar until chicken is done.\n• 1 teaspoon paprika 4. Remove the chicken and save the liquid in the\npot. Arrange the chicken on a broiler pan. Broil\n• 2 tablespoons ground black pepper\nuntil the chicken has nicely browned. Remove\n• 1 bay leaf, broken in half\nfrom the broiler and place it in a serving bowl.\n• 1 medium red tomato (optional)\n5. Continue to boil the sauce in the uncovered\npan until volume is reduced to about half and\nthe sauce is thick.\nNutrition Information\n6. Pour the thickened sauce over broiled\nadobong (chicken) and garnish with red\nMakes 4 servings\ntomatoes, if desired.\nServing size: ½ cup\nEach serving provides:\nCalories: 190\nTotal Fat: 5 g\nQuick Tip\nSaturated Fat: 1 g\nCholesterol: 70 mg\nSodium: 330 mg This recipe is lower in saturated fat and cholesterol because:\nTotal Fiber: 2 g\n• It is made using chicken wi",
        "thout the skin and any extra fat\nProtein: 26 g\nis removed.\nCarbohydrates: 10 g\nPotassium: 370 mg • Only 1 teaspoon of unsaturated fat (olive oil) is added.\n• It is flavored with vegetables and herbs and is boiled an\nbroiled slowly in moist heat instead of fat.\nSource: Filipino-American Nutrition and Fitness Teacher’s\nGuide, Kalusugan Community Services, San Diego, CA.\n2\nLumpiang Sariwa (Fresh Lumpia)\nYou and your family will love this tasty recipe. The ingredients—ground chicken\nor pork, olive oil, peanuts, and fresh herbs and spices—add flavor. Also, the\nlumpiang sariwa is served fresh so it has fewer calories than fried lumpiang.\nIngredients Directions\n• ½ cup ground chicken or lean pork 1. Heat oil, and sauté ground meat with the\nshrimp and garlic.\n• ½ cup shrimp, cleaned and deveined\n2. Add vegetables until slightly crisp. Pour in\n• 1 tablespoon olive oil\nthe chicken broth until cooked.\n• 2 cloves chopped garlic\n3. Season with salt and pepper.\n• ½ cup cabbage, julienned\n4. Set asid",
        "e and drain in a colander.\n• ½ cup green beans, julienned\n5. Save the broth for the lumpia sauce.\n• ½ cup carrots, julienned\n6. Soak the Vietnamese spring roll wrappers\n• ¼ cup celery, julienned\none at a time in water until soft and\n• ¼ cup jicama, julienned (may substitute transparent. Dry immediately with a paper\nchestnuts) towel.\n• ½ cup chicken broth 7. Lay the lettuce on the wrapper.\n• ¼ teaspoon salt 8. Place 2 tablespoons of the vegetable\n• ¼ teaspoon pepper mixture on the wrapper.\n• 8 Vietnamese spring-roll wrappers or 9. Fold in one side of the wrapper and roll\nlumpia wrappers tightly.\n• 8 pieces red leaf lettuce 10. Serve with lumpia sauce on top.\nSprinkle with chopped peanuts.\n• ⅓ cup peanuts, dry, roasted, and chopped\nNutrition Information\nLumpia Sauce\nMakes 8 servings\n• 1 cup broth from the sautéed vegetables Serving size: 1 lumpia\n• 1 tablespoon light soy sauce\nEach serving provides:\n• 1 tablespoon brown sugar\nCalories: 160\n• 3 cloves garlic, minced\nTotal Fat: 4 g\n• 1 tea",
        "spoon cornstarch Saturated Fat: 0.5 g\n• 2 tablespoons cold water for mixing cornstarch Cholesterol: 55 mg\nSodium: 150 mg\nDirections Total Fiber: 2 g\n1. Mix together vegetable broth, soy sauce, brown Protein: 10 g\nsugar, and garlic, and bring to a boil. Carbohydrates: 21 g\nPotassium: 170 mg\n2. Mix the cornstarch in 2 tablespoons of cold water.\n3. Slowly add the cornstarch mixture to the broth.\nStir until sauce thickens.\nSource: Mula sa Puso, Heart Healthy Traditional\nFilipino Recipes, American Heart Association, 1999.\n3\nPesang Isda (Fish Simmered With\nGinger and Tomatoes)\nThis main dish is heart healthy because the fish is simmered in water,\nnot fried, and no fat is added. Flavoring comes from the herbs and spices\ninstead of sauces that are high in sodium.\nIngredients Directions\n1. In a 4-quart saucepan,\n• ¼ cup fresh ginger, thinly\nsimmer sliced ginger,\nsliced (about 2 inches long)\ntomatoes, and onions in\n• 1 cup ripe tomatoes, chopped\n4 cups of water over medium\n• 1 cup white or yello",
        "w onions, heat until onions are tender\nthinly sliced (about 7 to 8 minutes).\n• 4 cups water 2. Reduce heat to low, add fish\n• 2 pounds fleshy fish (c and poach gently until almost\nfillet, halibut steak, or trout done (about 3 to 4 minutes).\n• 2 cups pechay (bok choy) 3. Add pechay stems, salt, and\nstems and leaves, cut up ground pepper. Cook for\nseparately 1 minute; then add pechay\nleaves and green onions.\n• ½ teaspoon salt\nCook another 30 seconds.\n• ½ teaspoon ground pepper\n4. Serve immediately.\n• 1 cup green onions, sliced\nNutrition Information\nMakes 6 servings\nServing size: 3 ounces lean\nfish and ½ cup vegetables\nEach serving provides:\nCalories: 160\nTotal Fat: 2 g\nSaturated Fat: 0.5 g\nCholesterol: 80 mg\nSodium: 340 mg\nTotal Fiber: 2 g\nProtein: 30 g\nCarbohydrates: 6 g\nPotassium: 630 mg\nSource: Filipino American Food Practices, Customs, and Holidays, American Dietetic Association, 1994.\n4\nMunggo Guisado (Sautéed Mung Beans)\nYour family will love this heart healthy side dish. It is mad",
        "e with vegetables,\nseafood, lean meat, and a small amount of corn oil. The pork is slowly\nsimmered in moist heat with vegetables and mung beans, creating flavors\nthat will make your taste buds jump for joy!\nIngredients Directions\n• 1 tablespoon corn oil 1. In a skillet, heat oil, and sauté\ncrushed garlic until lightly brown.\n• 2 cloves fresh garlic, crushed (or\n1 tablespoon, minced) 2. Add onion and tomatoes. Sauté\nuntil skin begins to curl.\n• 1 cup white onions, chopped\n3. Add pork, and sauté until lightly\n• 1 cup ripe tomatoes, chopped\nbrown.\n• 1 cup (4 ounces) lean pork, thinly\n4. Add water, and simmer pork for\nsliced\nabout 15 minutes.\n• 4 cups water\n5. Add the sautéed mix to mung\n• 3½ cups precooked mung beans\nbeans, and continue to simmer\n(from 1¾ cups dry beans)*\n15 minutes.\n• 1 teaspoon salt\n6. Season with salt and ground\n• 1 teaspoon ground pepper\npepper.\n• 1 cup (4 ounces) shrimp, peeled\n7. Add peeled shrimp.\nand deveined\n8. Add frozen leaf spinach, and\n• 1 cup (about ⅔ of a 1",
        "0-ounce\ncook 4 minutes until done.\npackage) leaf spinach, frozen\n* To cook dry, uncooked mung beans:\nWash and boil the uncooked mung\nbeans in a large saucepan, using 6 Nutrition Information\ncups of water. Cook until tender, about\n1½ to 2 hours. Drain. Makes 8 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 160\nTotal Fat: 3.5 g\nSaturated Fat: 1 g\nCholesterol: 35 mg\nSodium: 350 mg\nSource: Filipino American Food Practices, Customs,\nand Holidays, American Dietetic Association, 1994. Total Fiber: 8 g\nProtein: 13 g\nCarbohydrates: 19 g\nPotassium: 370 mg\n5\nAmpalaya (Bitter Melon) With Pork\nThis recipe is lower in fat and sodium than a typical ampalaya dish\nbecause it uses lean meat that is sautéed and simmered instead of fried.\nIngredients Directions\n• 1 cup onion, chopped 1. Using a large skillet, lightly sauté\nonions and garlic in hot olive oil.\n• 6 cloves garlic, crushed\n2. Add the ground pork and cook until\n• 1 tablespoon olive oil\nalmost done.\n• ½ pound (0.2 kg) lean ground ",
        "pork\n3. Add the sliced bitter melon.\n• 2 cups Ampalaya*, sliced\n4. Cover and simmer until bitter melon\n• 2 teaspoons light soy sauce\nturns green. Do not overcook.\n• ½ teaspoon black pepper\n5. Season with light soy sauce and\nblack pepper.\n* Ampalaya (bitter melon) is a fruit that is\noblong, cylindrical, pointed at both ends,\nribbed, and wrinkled.\nNutrition Information\nMakes 4 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 150\nTotal Fat: 6 g\nSaturated Fat: 1.5 g\nCholesterol: 45 mg\nSodium: 200 mg\nTotal Fiber: 1 g\nProtein: 17 g\nCarbohydrates: 7 g\nPotassium: 600 mg\nSource: Adapted from Mula Sa Puso, Heart Healthy Traditional Filipino Recipes, American Heart Association, 1999.\n6\nCantaloupe Crush\nTry this refreshing, heart healthy drink that uses fresh fruit, fat-free milk, and low\namounts of sweetener. Children and adults alike will love it!\nIngredients Directions\n• ½ cantaloupe 1. Cut cantaloupe into small\ncubes or thin strips.\n• 1 cup fat-free milk\n2. Mix cantaloupe, milk, a",
        "nd\n• 1½ cups ice\nice in a blender until smooth.\n• Sweetener, as needed\n3. Sweeten to taste.\n(about 1 to 2 teaspoons\nsugar or equivalent of\nanother sweetener)\nNutrition Information\nMakes 4 servings\nServing size: ½ cup\nEach serving provides:\nCalories: 50\nTotal Fat: 0 g\nSaturated Fat: 0 g\nCholesterol: 0 mg\nSodium: 40 mg\nTotal Fiber: 0 g\nProtein: 3 g\nCarbohydrates: 10 g\nPotassium: 280 mg\nSource: Adapted from the National Cancer Institute and InteliHealth (intelihealth.com), 2013.\n7\nVegetable Kare-Kare (Peanut Stew)\nThis version of vegetable kare-kare is healthier than the traditional Filipino dish\nbecause it has no cholesterol. It uses gluten instead of oxtail or other meat. It is also\npacked with vegetables and made complete with a nutty, low-sodium sauce.\nIngredients\n• 1 ¼ cup gluten or seitan,* cubes\n• 2 tablespoons corn oil\n• 2 cloves garlic, crushed\nNutrition Information\n• 1 onion, medium, sliced\n• ½ cup ground peanuts Makes 6 servings\n• ¼ cup ground toasted rice**\nEach serving provid",
        "es:\n• ¼ teaspoon salt\nCalories: 300\n• 1 cup eggplant, sliced\nTotal Fat: 12 g\n• ½ cup string beans, sliced Saturated Fat: 1.5 g\n• ⅔ cup banana heart or bud Cholesterol: 0 mg\nSodium: 125 mg\n• ½ cup bok choy (pechay), sliced\nTotal Fiber: 4 g\n* Gluten is made from protein that is in a variety of grains, such Protein: 36 g\nas wheat and rye, and is mixed and kneaded with water. Seitan\nCarbohydrates: 20 g\nis a form of wheat gluten. It is sold as strips or in cans at health\nPotassium: 320 mg\nfood stores and Asian supermarkets.\n** To make ground, toasted rice: Place rice, ½ cup at a time,\nin a frying pan or wok and heat over moderate heat, stirring\nfrequently to keep it from burning and to allow it to develop a\nuniform, deep golden color—2 to 3 minutes. Then remove it\nfrom heat and cool to room temperature. Grind the toasted rice\ncoarsely—not finely grounded—in a blende, or spice or coffee\ngrinder.\nDirections\n1. Sauté gluten cubes in corn oil. Add garlic and onions.\n2. Pour enough water to cove",
        "r gluten, and add ground\npeanuts and ground rice to thicken.\n3. Season with salt.\n4. Add the eggplant, then string beans, then banana,\nthen bok choy (pechay) on top of the cooked gluten.\nSource: PHC Alive Diet, Division of Nutrition and Dietetics, Philippine Heart Center, East Avenue, Quezon City, Philippines, page 91.\n8\nNational Heart, Lung, and Blood Institute\nAn Initiative of the to help reduce health disparities in cardiovascular disease\nand asthma in underserved and minority communities. For more information, visit www.nhlbi.nih.gov/health/healthdisp."
      ],
      "uploaded_at": "2025-07-25T19:51:46.585775",
      "source": "pdf_upload"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "chunks": [
        "Tasty and Healthy —\nHeart Healthy Filipino Recipes\nFish Cardillo\nThis is a delicious, low-cost recipe with low-sodium ingredients.\nKeep it low-fat by not adding meat fat (lard) or other fat.\nIngredients Directions\n1. Thoroughly clean fish. Remove scale\n• 1 pound (½ kg) red snapper\nand gills, and wash thoroughly. Drain and\n• 4 teaspoons corn oil for sauté\nset aside.\n• ¼ cup flou\n2. Slice the raw fish into six pieces\n• 1 large onion, sliced\n3. Heat corn oil in frying pan.\n• 3 or 4 medium-sized tomatoes,\n4. Place the flour into a bowl or plastic bag\nchopped\nPlace the raw fish in the flour and cov\n• ½ cup egg whites, beaten the outside of each fish with flo .\n• ½ cup water 5. Sauté fish until golden brown. Set asid\n• A dash ground pepper on top of a paper towel.\n• 15 stalks green onions, chopped 6. Sauté onion and tomatoes. Add ½ cup of\nwater.\n7. Add the beaten egg whites and fish\nCover and let it simmer for 5–10 minutes.\nNutrition Information\n8. Season with ground pepper.\nMakes 6 servings",
        " 9. Sprinkle with chopped green onions.\nEach serving provides:\nCalories: 170\nTotal Fat: 4 g\nSaturated Fat: 1 g\nCholesterol: 45 mg\nQuick Tip\nSodium: 115 mg\nTotal Fiber: 3 g\nProtein: 20 g\nThis recipe is lower in salt and sodium than other\nCarbohydrates: 13 g\nCardillo recipes because it uses:\nPotassium: 600 mg\n• Fresh, not canned, tomatoes\n• Ground pepper and corn oil with no salt added\n• Fresh onion and green onions\n• Fresh, not smoked or canned, fis\nSource: Philippine Heart Center’s Healthy Heart Cookbook.\n1\nAdobong Manok (Marinated Chicken)\nThis low-cost, low-sodium recipe has great flavor that you and your\nfamily will love! It uses chicken breast which is lower in fat than\nother parts of the chicken, like the thigh and leg.\nIngredients Directions\n• 1 teaspoon olive oil 1. Combine olive oil, garlic, and onion in a frying\npan. Add chicken and sauté together until\n• 2 cloves fresh crushed garlic\nchicken has browned.\n• 2 medium chopped onions\n2. Add light soy sauce, vinegar, paprika,\n• 1 ",
        "pound (½ kg) chicken breasts, no skin\nblack pepper, and bay leaf. Stir.\n• 2 tablespoons light soy sauce\n3. Bring to a boil. Simmer for 45–60 minutes or\n• ¼ cup vinegar until chicken is done.\n• 1 teaspoon paprika 4. Remove the chicken and save the liquid in the\npot. Arrange the chicken on a broiler pan. Broil\n• 2 tablespoons ground black pepper\nuntil the chicken has nicely browned. Remove\n• 1 bay leaf, broken in half\nfrom the broiler and place it in a serving bowl.\n• 1 medium red tomato (optional)\n5. Continue to boil the sauce in the uncovered\npan until volume is reduced to about half and\nthe sauce is thick.\nNutrition Information\n6. Pour the thickened sauce over broiled\nadobong (chicken) and garnish with red\nMakes 4 servings\ntomatoes, if desired.\nServing size: ½ cup\nEach serving provides:\nCalories: 190\nTotal Fat: 5 g\nQuick Tip\nSaturated Fat: 1 g\nCholesterol: 70 mg\nSodium: 330 mg This recipe is lower in saturated fat and cholesterol because:\nTotal Fiber: 2 g\n• It is made using chicken wi",
        "thout the skin and any extra fat\nProtein: 26 g\nis removed.\nCarbohydrates: 10 g\nPotassium: 370 mg • Only 1 teaspoon of unsaturated fat (olive oil) is added.\n• It is flavored with vegetables and herbs and is boiled an\nbroiled slowly in moist heat instead of fat.\nSource: Filipino-American Nutrition and Fitness Teacher’s\nGuide, Kalusugan Community Services, San Diego, CA.\n2\nLumpiang Sariwa (Fresh Lumpia)\nYou and your family will love this tasty recipe. The ingredients—ground chicken\nor pork, olive oil, peanuts, and fresh herbs and spices—add flavor. Also, the\nlumpiang sariwa is served fresh so it has fewer calories than fried lumpiang.\nIngredients Directions\n• ½ cup ground chicken or lean pork 1. Heat oil, and sauté ground meat with the\nshrimp and garlic.\n• ½ cup shrimp, cleaned and deveined\n2. Add vegetables until slightly crisp. Pour in\n• 1 tablespoon olive oil\nthe chicken broth until cooked.\n• 2 cloves chopped garlic\n3. Season with salt and pepper.\n• ½ cup cabbage, julienned\n4. Set asid",
        "e and drain in a colander.\n• ½ cup green beans, julienned\n5. Save the broth for the lumpia sauce.\n• ½ cup carrots, julienned\n6. Soak the Vietnamese spring roll wrappers\n• ¼ cup celery, julienned\none at a time in water until soft and\n• ¼ cup jicama, julienned (may substitute transparent. Dry immediately with a paper\nchestnuts) towel.\n• ½ cup chicken broth 7. Lay the lettuce on the wrapper.\n• ¼ teaspoon salt 8. Place 2 tablespoons of the vegetable\n• ¼ teaspoon pepper mixture on the wrapper.\n• 8 Vietnamese spring-roll wrappers or 9. Fold in one side of the wrapper and roll\nlumpia wrappers tightly.\n• 8 pieces red leaf lettuce 10. Serve with lumpia sauce on top.\nSprinkle with chopped peanuts.\n• ⅓ cup peanuts, dry, roasted, and chopped\nNutrition Information\nLumpia Sauce\nMakes 8 servings\n• 1 cup broth from the sautéed vegetables Serving size: 1 lumpia\n• 1 tablespoon light soy sauce\nEach serving provides:\n• 1 tablespoon brown sugar\nCalories: 160\n• 3 cloves garlic, minced\nTotal Fat: 4 g\n• 1 tea",
        "spoon cornstarch Saturated Fat: 0.5 g\n• 2 tablespoons cold water for mixing cornstarch Cholesterol: 55 mg\nSodium: 150 mg\nDirections Total Fiber: 2 g\n1. Mix together vegetable broth, soy sauce, brown Protein: 10 g\nsugar, and garlic, and bring to a boil. Carbohydrates: 21 g\nPotassium: 170 mg\n2. Mix the cornstarch in 2 tablespoons of cold water.\n3. Slowly add the cornstarch mixture to the broth.\nStir until sauce thickens.\nSource: Mula sa Puso, Heart Healthy Traditional\nFilipino Recipes, American Heart Association, 1999.\n3\nPesang Isda (Fish Simmered With\nGinger and Tomatoes)\nThis main dish is heart healthy because the fish is simmered in water,\nnot fried, and no fat is added. Flavoring comes from the herbs and spices\ninstead of sauces that are high in sodium.\nIngredients Directions\n1. In a 4-quart saucepan,\n• ¼ cup fresh ginger, thinly\nsimmer sliced ginger,\nsliced (about 2 inches long)\ntomatoes, and onions in\n• 1 cup ripe tomatoes, chopped\n4 cups of water over medium\n• 1 cup white or yello",
        "w onions, heat until onions are tender\nthinly sliced (about 7 to 8 minutes).\n• 4 cups water 2. Reduce heat to low, add fish\n• 2 pounds fleshy fish (c and poach gently until almost\nfillet, halibut steak, or trout done (about 3 to 4 minutes).\n• 2 cups pechay (bok choy) 3. Add pechay stems, salt, and\nstems and leaves, cut up ground pepper. Cook for\nseparately 1 minute; then add pechay\nleaves and green onions.\n• ½ teaspoon salt\nCook another 30 seconds.\n• ½ teaspoon ground pepper\n4. Serve immediately.\n• 1 cup green onions, sliced\nNutrition Information\nMakes 6 servings\nServing size: 3 ounces lean\nfish and ½ cup vegetables\nEach serving provides:\nCalories: 160\nTotal Fat: 2 g\nSaturated Fat: 0.5 g\nCholesterol: 80 mg\nSodium: 340 mg\nTotal Fiber: 2 g\nProtein: 30 g\nCarbohydrates: 6 g\nPotassium: 630 mg\nSource: Filipino American Food Practices, Customs, and Holidays, American Dietetic Association, 1994.\n4\nMunggo Guisado (Sautéed Mung Beans)\nYour family will love this heart healthy side dish. It is mad",
        "e with vegetables,\nseafood, lean meat, and a small amount of corn oil. The pork is slowly\nsimmered in moist heat with vegetables and mung beans, creating flavors\nthat will make your taste buds jump for joy!\nIngredients Directions\n• 1 tablespoon corn oil 1. In a skillet, heat oil, and sauté\ncrushed garlic until lightly brown.\n• 2 cloves fresh garlic, crushed (or\n1 tablespoon, minced) 2. Add onion and tomatoes. Sauté\nuntil skin begins to curl.\n• 1 cup white onions, chopped\n3. Add pork, and sauté until lightly\n• 1 cup ripe tomatoes, chopped\nbrown.\n• 1 cup (4 ounces) lean pork, thinly\n4. Add water, and simmer pork for\nsliced\nabout 15 minutes.\n• 4 cups water\n5. Add the sautéed mix to mung\n• 3½ cups precooked mung beans\nbeans, and continue to simmer\n(from 1¾ cups dry beans)*\n15 minutes.\n• 1 teaspoon salt\n6. Season with salt and ground\n• 1 teaspoon ground pepper\npepper.\n• 1 cup (4 ounces) shrimp, peeled\n7. Add peeled shrimp.\nand deveined\n8. Add frozen leaf spinach, and\n• 1 cup (about ⅔ of a 1",
        "0-ounce\ncook 4 minutes until done.\npackage) leaf spinach, frozen\n* To cook dry, uncooked mung beans:\nWash and boil the uncooked mung\nbeans in a large saucepan, using 6 Nutrition Information\ncups of water. Cook until tender, about\n1½ to 2 hours. Drain. Makes 8 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 160\nTotal Fat: 3.5 g\nSaturated Fat: 1 g\nCholesterol: 35 mg\nSodium: 350 mg\nSource: Filipino American Food Practices, Customs,\nand Holidays, American Dietetic Association, 1994. Total Fiber: 8 g\nProtein: 13 g\nCarbohydrates: 19 g\nPotassium: 370 mg\n5\nAmpalaya (Bitter Melon) With Pork\nThis recipe is lower in fat and sodium than a typical ampalaya dish\nbecause it uses lean meat that is sautéed and simmered instead of fried.\nIngredients Directions\n• 1 cup onion, chopped 1. Using a large skillet, lightly sauté\nonions and garlic in hot olive oil.\n• 6 cloves garlic, crushed\n2. Add the ground pork and cook until\n• 1 tablespoon olive oil\nalmost done.\n• ½ pound (0.2 kg) lean ground ",
        "pork\n3. Add the sliced bitter melon.\n• 2 cups Ampalaya*, sliced\n4. Cover and simmer until bitter melon\n• 2 teaspoons light soy sauce\nturns green. Do not overcook.\n• ½ teaspoon black pepper\n5. Season with light soy sauce and\nblack pepper.\n* Ampalaya (bitter melon) is a fruit that is\noblong, cylindrical, pointed at both ends,\nribbed, and wrinkled.\nNutrition Information\nMakes 4 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 150\nTotal Fat: 6 g\nSaturated Fat: 1.5 g\nCholesterol: 45 mg\nSodium: 200 mg\nTotal Fiber: 1 g\nProtein: 17 g\nCarbohydrates: 7 g\nPotassium: 600 mg\nSource: Adapted from Mula Sa Puso, Heart Healthy Traditional Filipino Recipes, American Heart Association, 1999.\n6\nCantaloupe Crush\nTry this refreshing, heart healthy drink that uses fresh fruit, fat-free milk, and low\namounts of sweetener. Children and adults alike will love it!\nIngredients Directions\n• ½ cantaloupe 1. Cut cantaloupe into small\ncubes or thin strips.\n• 1 cup fat-free milk\n2. Mix cantaloupe, milk, a",
        "nd\n• 1½ cups ice\nice in a blender until smooth.\n• Sweetener, as needed\n3. Sweeten to taste.\n(about 1 to 2 teaspoons\nsugar or equivalent of\nanother sweetener)\nNutrition Information\nMakes 4 servings\nServing size: ½ cup\nEach serving provides:\nCalories: 50\nTotal Fat: 0 g\nSaturated Fat: 0 g\nCholesterol: 0 mg\nSodium: 40 mg\nTotal Fiber: 0 g\nProtein: 3 g\nCarbohydrates: 10 g\nPotassium: 280 mg\nSource: Adapted from the National Cancer Institute and InteliHealth (intelihealth.com), 2013.\n7\nVegetable Kare-Kare (Peanut Stew)\nThis version of vegetable kare-kare is healthier than the traditional Filipino dish\nbecause it has no cholesterol. It uses gluten instead of oxtail or other meat. It is also\npacked with vegetables and made complete with a nutty, low-sodium sauce.\nIngredients\n• 1 ¼ cup gluten or seitan,* cubes\n• 2 tablespoons corn oil\n• 2 cloves garlic, crushed\nNutrition Information\n• 1 onion, medium, sliced\n• ½ cup ground peanuts Makes 6 servings\n• ¼ cup ground toasted rice**\nEach serving provid",
        "es:\n• ¼ teaspoon salt\nCalories: 300\n• 1 cup eggplant, sliced\nTotal Fat: 12 g\n• ½ cup string beans, sliced Saturated Fat: 1.5 g\n• ⅔ cup banana heart or bud Cholesterol: 0 mg\nSodium: 125 mg\n• ½ cup bok choy (pechay), sliced\nTotal Fiber: 4 g\n* Gluten is made from protein that is in a variety of grains, such Protein: 36 g\nas wheat and rye, and is mixed and kneaded with water. Seitan\nCarbohydrates: 20 g\nis a form of wheat gluten. It is sold as strips or in cans at health\nPotassium: 320 mg\nfood stores and Asian supermarkets.\n** To make ground, toasted rice: Place rice, ½ cup at a time,\nin a frying pan or wok and heat over moderate heat, stirring\nfrequently to keep it from burning and to allow it to develop a\nuniform, deep golden color—2 to 3 minutes. Then remove it\nfrom heat and cool to room temperature. Grind the toasted rice\ncoarsely—not finely grounded—in a blende, or spice or coffee\ngrinder.\nDirections\n1. Sauté gluten cubes in corn oil. Add garlic and onions.\n2. Pour enough water to cove",
        "r gluten, and add ground\npeanuts and ground rice to thicken.\n3. Season with salt.\n4. Add the eggplant, then string beans, then banana,\nthen bok choy (pechay) on top of the cooked gluten.\nSource: PHC Alive Diet, Division of Nutrition and Dietetics, Philippine Heart Center, East Avenue, Quezon City, Philippines, page 91.\n8\nNational Heart, Lung, and Blood Institute\nAn Initiative of the to help reduce health disparities in cardiovascular disease\nand asthma in underserved and minority communities. For more information, visit www.nhlbi.nih.gov/health/healthdisp."
      ],
      "uploaded_at": "2025-07-25T19:51:55.846098",
      "source": "pdf_upload"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "chunks": [
        "Tasty and Healthy —\nHeart Healthy Filipino Recipes\nFish Cardillo\nThis is a delicious, low-cost recipe with low-sodium ingredients.\nKeep it low-fat by not adding meat fat (lard) or other fat.\nIngredients Directions\n1. Thoroughly clean fish. Remove scale\n• 1 pound (½ kg) red snapper\nand gills, and wash thoroughly. Drain and\n• 4 teaspoons corn oil for sauté\nset aside.\n• ¼ cup flou\n2. Slice the raw fish into six pieces\n• 1 large onion, sliced\n3. Heat corn oil in frying pan.\n• 3 or 4 medium-sized tomatoes,\n4. Place the flour into a bowl or plastic bag\nchopped\nPlace the raw fish in the flour and cov\n• ½ cup egg whites, beaten the outside of each fish with flo .\n• ½ cup water 5. Sauté fish until golden brown. Set asid\n• A dash ground pepper on top of a paper towel.\n• 15 stalks green onions, chopped 6. Sauté onion and tomatoes. Add ½ cup of\nwater.\n7. Add the beaten egg whites and fish\nCover and let it simmer for 5–10 minutes.\nNutrition Information\n8. Season with ground pepper.\nMakes 6 servings",
        " 9. Sprinkle with chopped green onions.\nEach serving provides:\nCalories: 170\nTotal Fat: 4 g\nSaturated Fat: 1 g\nCholesterol: 45 mg\nQuick Tip\nSodium: 115 mg\nTotal Fiber: 3 g\nProtein: 20 g\nThis recipe is lower in salt and sodium than other\nCarbohydrates: 13 g\nCardillo recipes because it uses:\nPotassium: 600 mg\n• Fresh, not canned, tomatoes\n• Ground pepper and corn oil with no salt added\n• Fresh onion and green onions\n• Fresh, not smoked or canned, fis\nSource: Philippine Heart Center’s Healthy Heart Cookbook.\n1\nAdobong Manok (Marinated Chicken)\nThis low-cost, low-sodium recipe has great flavor that you and your\nfamily will love! It uses chicken breast which is lower in fat than\nother parts of the chicken, like the thigh and leg.\nIngredients Directions\n• 1 teaspoon olive oil 1. Combine olive oil, garlic, and onion in a frying\npan. Add chicken and sauté together until\n• 2 cloves fresh crushed garlic\nchicken has browned.\n• 2 medium chopped onions\n2. Add light soy sauce, vinegar, paprika,\n• 1 ",
        "pound (½ kg) chicken breasts, no skin\nblack pepper, and bay leaf. Stir.\n• 2 tablespoons light soy sauce\n3. Bring to a boil. Simmer for 45–60 minutes or\n• ¼ cup vinegar until chicken is done.\n• 1 teaspoon paprika 4. Remove the chicken and save the liquid in the\npot. Arrange the chicken on a broiler pan. Broil\n• 2 tablespoons ground black pepper\nuntil the chicken has nicely browned. Remove\n• 1 bay leaf, broken in half\nfrom the broiler and place it in a serving bowl.\n• 1 medium red tomato (optional)\n5. Continue to boil the sauce in the uncovered\npan until volume is reduced to about half and\nthe sauce is thick.\nNutrition Information\n6. Pour the thickened sauce over broiled\nadobong (chicken) and garnish with red\nMakes 4 servings\ntomatoes, if desired.\nServing size: ½ cup\nEach serving provides:\nCalories: 190\nTotal Fat: 5 g\nQuick Tip\nSaturated Fat: 1 g\nCholesterol: 70 mg\nSodium: 330 mg This recipe is lower in saturated fat and cholesterol because:\nTotal Fiber: 2 g\n• It is made using chicken wi",
        "thout the skin and any extra fat\nProtein: 26 g\nis removed.\nCarbohydrates: 10 g\nPotassium: 370 mg • Only 1 teaspoon of unsaturated fat (olive oil) is added.\n• It is flavored with vegetables and herbs and is boiled an\nbroiled slowly in moist heat instead of fat.\nSource: Filipino-American Nutrition and Fitness Teacher’s\nGuide, Kalusugan Community Services, San Diego, CA.\n2\nLumpiang Sariwa (Fresh Lumpia)\nYou and your family will love this tasty recipe. The ingredients—ground chicken\nor pork, olive oil, peanuts, and fresh herbs and spices—add flavor. Also, the\nlumpiang sariwa is served fresh so it has fewer calories than fried lumpiang.\nIngredients Directions\n• ½ cup ground chicken or lean pork 1. Heat oil, and sauté ground meat with the\nshrimp and garlic.\n• ½ cup shrimp, cleaned and deveined\n2. Add vegetables until slightly crisp. Pour in\n• 1 tablespoon olive oil\nthe chicken broth until cooked.\n• 2 cloves chopped garlic\n3. Season with salt and pepper.\n• ½ cup cabbage, julienned\n4. Set asid",
        "e and drain in a colander.\n• ½ cup green beans, julienned\n5. Save the broth for the lumpia sauce.\n• ½ cup carrots, julienned\n6. Soak the Vietnamese spring roll wrappers\n• ¼ cup celery, julienned\none at a time in water until soft and\n• ¼ cup jicama, julienned (may substitute transparent. Dry immediately with a paper\nchestnuts) towel.\n• ½ cup chicken broth 7. Lay the lettuce on the wrapper.\n• ¼ teaspoon salt 8. Place 2 tablespoons of the vegetable\n• ¼ teaspoon pepper mixture on the wrapper.\n• 8 Vietnamese spring-roll wrappers or 9. Fold in one side of the wrapper and roll\nlumpia wrappers tightly.\n• 8 pieces red leaf lettuce 10. Serve with lumpia sauce on top.\nSprinkle with chopped peanuts.\n• ⅓ cup peanuts, dry, roasted, and chopped\nNutrition Information\nLumpia Sauce\nMakes 8 servings\n• 1 cup broth from the sautéed vegetables Serving size: 1 lumpia\n• 1 tablespoon light soy sauce\nEach serving provides:\n• 1 tablespoon brown sugar\nCalories: 160\n• 3 cloves garlic, minced\nTotal Fat: 4 g\n• 1 tea",
        "spoon cornstarch Saturated Fat: 0.5 g\n• 2 tablespoons cold water for mixing cornstarch Cholesterol: 55 mg\nSodium: 150 mg\nDirections Total Fiber: 2 g\n1. Mix together vegetable broth, soy sauce, brown Protein: 10 g\nsugar, and garlic, and bring to a boil. Carbohydrates: 21 g\nPotassium: 170 mg\n2. Mix the cornstarch in 2 tablespoons of cold water.\n3. Slowly add the cornstarch mixture to the broth.\nStir until sauce thickens.\nSource: Mula sa Puso, Heart Healthy Traditional\nFilipino Recipes, American Heart Association, 1999.\n3\nPesang Isda (Fish Simmered With\nGinger and Tomatoes)\nThis main dish is heart healthy because the fish is simmered in water,\nnot fried, and no fat is added. Flavoring comes from the herbs and spices\ninstead of sauces that are high in sodium.\nIngredients Directions\n1. In a 4-quart saucepan,\n• ¼ cup fresh ginger, thinly\nsimmer sliced ginger,\nsliced (about 2 inches long)\ntomatoes, and onions in\n• 1 cup ripe tomatoes, chopped\n4 cups of water over medium\n• 1 cup white or yello",
        "w onions, heat until onions are tender\nthinly sliced (about 7 to 8 minutes).\n• 4 cups water 2. Reduce heat to low, add fish\n• 2 pounds fleshy fish (c and poach gently until almost\nfillet, halibut steak, or trout done (about 3 to 4 minutes).\n• 2 cups pechay (bok choy) 3. Add pechay stems, salt, and\nstems and leaves, cut up ground pepper. Cook for\nseparately 1 minute; then add pechay\nleaves and green onions.\n• ½ teaspoon salt\nCook another 30 seconds.\n• ½ teaspoon ground pepper\n4. Serve immediately.\n• 1 cup green onions, sliced\nNutrition Information\nMakes 6 servings\nServing size: 3 ounces lean\nfish and ½ cup vegetables\nEach serving provides:\nCalories: 160\nTotal Fat: 2 g\nSaturated Fat: 0.5 g\nCholesterol: 80 mg\nSodium: 340 mg\nTotal Fiber: 2 g\nProtein: 30 g\nCarbohydrates: 6 g\nPotassium: 630 mg\nSource: Filipino American Food Practices, Customs, and Holidays, American Dietetic Association, 1994.\n4\nMunggo Guisado (Sautéed Mung Beans)\nYour family will love this heart healthy side dish. It is mad",
        "e with vegetables,\nseafood, lean meat, and a small amount of corn oil. The pork is slowly\nsimmered in moist heat with vegetables and mung beans, creating flavors\nthat will make your taste buds jump for joy!\nIngredients Directions\n• 1 tablespoon corn oil 1. In a skillet, heat oil, and sauté\ncrushed garlic until lightly brown.\n• 2 cloves fresh garlic, crushed (or\n1 tablespoon, minced) 2. Add onion and tomatoes. Sauté\nuntil skin begins to curl.\n• 1 cup white onions, chopped\n3. Add pork, and sauté until lightly\n• 1 cup ripe tomatoes, chopped\nbrown.\n• 1 cup (4 ounces) lean pork, thinly\n4. Add water, and simmer pork for\nsliced\nabout 15 minutes.\n• 4 cups water\n5. Add the sautéed mix to mung\n• 3½ cups precooked mung beans\nbeans, and continue to simmer\n(from 1¾ cups dry beans)*\n15 minutes.\n• 1 teaspoon salt\n6. Season with salt and ground\n• 1 teaspoon ground pepper\npepper.\n• 1 cup (4 ounces) shrimp, peeled\n7. Add peeled shrimp.\nand deveined\n8. Add frozen leaf spinach, and\n• 1 cup (about ⅔ of a 1",
        "0-ounce\ncook 4 minutes until done.\npackage) leaf spinach, frozen\n* To cook dry, uncooked mung beans:\nWash and boil the uncooked mung\nbeans in a large saucepan, using 6 Nutrition Information\ncups of water. Cook until tender, about\n1½ to 2 hours. Drain. Makes 8 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 160\nTotal Fat: 3.5 g\nSaturated Fat: 1 g\nCholesterol: 35 mg\nSodium: 350 mg\nSource: Filipino American Food Practices, Customs,\nand Holidays, American Dietetic Association, 1994. Total Fiber: 8 g\nProtein: 13 g\nCarbohydrates: 19 g\nPotassium: 370 mg\n5\nAmpalaya (Bitter Melon) With Pork\nThis recipe is lower in fat and sodium than a typical ampalaya dish\nbecause it uses lean meat that is sautéed and simmered instead of fried.\nIngredients Directions\n• 1 cup onion, chopped 1. Using a large skillet, lightly sauté\nonions and garlic in hot olive oil.\n• 6 cloves garlic, crushed\n2. Add the ground pork and cook until\n• 1 tablespoon olive oil\nalmost done.\n• ½ pound (0.2 kg) lean ground ",
        "pork\n3. Add the sliced bitter melon.\n• 2 cups Ampalaya*, sliced\n4. Cover and simmer until bitter melon\n• 2 teaspoons light soy sauce\nturns green. Do not overcook.\n• ½ teaspoon black pepper\n5. Season with light soy sauce and\nblack pepper.\n* Ampalaya (bitter melon) is a fruit that is\noblong, cylindrical, pointed at both ends,\nribbed, and wrinkled.\nNutrition Information\nMakes 4 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 150\nTotal Fat: 6 g\nSaturated Fat: 1.5 g\nCholesterol: 45 mg\nSodium: 200 mg\nTotal Fiber: 1 g\nProtein: 17 g\nCarbohydrates: 7 g\nPotassium: 600 mg\nSource: Adapted from Mula Sa Puso, Heart Healthy Traditional Filipino Recipes, American Heart Association, 1999.\n6\nCantaloupe Crush\nTry this refreshing, heart healthy drink that uses fresh fruit, fat-free milk, and low\namounts of sweetener. Children and adults alike will love it!\nIngredients Directions\n• ½ cantaloupe 1. Cut cantaloupe into small\ncubes or thin strips.\n• 1 cup fat-free milk\n2. Mix cantaloupe, milk, a",
        "nd\n• 1½ cups ice\nice in a blender until smooth.\n• Sweetener, as needed\n3. Sweeten to taste.\n(about 1 to 2 teaspoons\nsugar or equivalent of\nanother sweetener)\nNutrition Information\nMakes 4 servings\nServing size: ½ cup\nEach serving provides:\nCalories: 50\nTotal Fat: 0 g\nSaturated Fat: 0 g\nCholesterol: 0 mg\nSodium: 40 mg\nTotal Fiber: 0 g\nProtein: 3 g\nCarbohydrates: 10 g\nPotassium: 280 mg\nSource: Adapted from the National Cancer Institute and InteliHealth (intelihealth.com), 2013.\n7\nVegetable Kare-Kare (Peanut Stew)\nThis version of vegetable kare-kare is healthier than the traditional Filipino dish\nbecause it has no cholesterol. It uses gluten instead of oxtail or other meat. It is also\npacked with vegetables and made complete with a nutty, low-sodium sauce.\nIngredients\n• 1 ¼ cup gluten or seitan,* cubes\n• 2 tablespoons corn oil\n• 2 cloves garlic, crushed\nNutrition Information\n• 1 onion, medium, sliced\n• ½ cup ground peanuts Makes 6 servings\n• ¼ cup ground toasted rice**\nEach serving provid",
        "es:\n• ¼ teaspoon salt\nCalories: 300\n• 1 cup eggplant, sliced\nTotal Fat: 12 g\n• ½ cup string beans, sliced Saturated Fat: 1.5 g\n• ⅔ cup banana heart or bud Cholesterol: 0 mg\nSodium: 125 mg\n• ½ cup bok choy (pechay), sliced\nTotal Fiber: 4 g\n* Gluten is made from protein that is in a variety of grains, such Protein: 36 g\nas wheat and rye, and is mixed and kneaded with water. Seitan\nCarbohydrates: 20 g\nis a form of wheat gluten. It is sold as strips or in cans at health\nPotassium: 320 mg\nfood stores and Asian supermarkets.\n** To make ground, toasted rice: Place rice, ½ cup at a time,\nin a frying pan or wok and heat over moderate heat, stirring\nfrequently to keep it from burning and to allow it to develop a\nuniform, deep golden color—2 to 3 minutes. Then remove it\nfrom heat and cool to room temperature. Grind the toasted rice\ncoarsely—not finely grounded—in a blende, or spice or coffee\ngrinder.\nDirections\n1. Sauté gluten cubes in corn oil. Add garlic and onions.\n2. Pour enough water to cove",
        "r gluten, and add ground\npeanuts and ground rice to thicken.\n3. Season with salt.\n4. Add the eggplant, then string beans, then banana,\nthen bok choy (pechay) on top of the cooked gluten.\nSource: PHC Alive Diet, Division of Nutrition and Dietetics, Philippine Heart Center, East Avenue, Quezon City, Philippines, page 91.\n8\nNational Heart, Lung, and Blood Institute\nAn Initiative of the to help reduce health disparities in cardiovascular disease\nand asthma in underserved and minority communities. For more information, visit www.nhlbi.nih.gov/health/healthdisp."
      ],
      "uploaded_at": "2025-07-25T19:52:07.812204",
      "source": "pdf_upload"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "chunks": [
        "Tasty and Healthy —\nHeart Healthy Filipino Recipes\nFish Cardillo\nThis is a delicious, low-cost recipe with low-sodium ingredients.\nKeep it low-fat by not adding meat fat (lard) or other fat.\nIngredients Directions\n1. Thoroughly clean fish. Remove scale\n• 1 pound (½ kg) red snapper\nand gills, and wash thoroughly. Drain and\n• 4 teaspoons corn oil for sauté\nset aside.\n• ¼ cup flou\n2. Slice the raw fish into six pieces\n• 1 large onion, sliced\n3. Heat corn oil in frying pan.\n• 3 or 4 medium-sized tomatoes,\n4. Place the flour into a bowl or plastic bag\nchopped\nPlace the raw fish in the flour and cov\n• ½ cup egg whites, beaten the outside of each fish with flo .\n• ½ cup water 5. Sauté fish until golden brown. Set asid\n• A dash ground pepper on top of a paper towel.\n• 15 stalks green onions, chopped 6. Sauté onion and tomatoes. Add ½ cup of\nwater.\n7. Add the beaten egg whites and fish\nCover and let it simmer for 5–10 minutes.\nNutrition Information\n8. Season with ground pepper.\nMakes 6 servings",
        " 9. Sprinkle with chopped green onions.\nEach serving provides:\nCalories: 170\nTotal Fat: 4 g\nSaturated Fat: 1 g\nCholesterol: 45 mg\nQuick Tip\nSodium: 115 mg\nTotal Fiber: 3 g\nProtein: 20 g\nThis recipe is lower in salt and sodium than other\nCarbohydrates: 13 g\nCardillo recipes because it uses:\nPotassium: 600 mg\n• Fresh, not canned, tomatoes\n• Ground pepper and corn oil with no salt added\n• Fresh onion and green onions\n• Fresh, not smoked or canned, fis\nSource: Philippine Heart Center’s Healthy Heart Cookbook.\n1\nAdobong Manok (Marinated Chicken)\nThis low-cost, low-sodium recipe has great flavor that you and your\nfamily will love! It uses chicken breast which is lower in fat than\nother parts of the chicken, like the thigh and leg.\nIngredients Directions\n• 1 teaspoon olive oil 1. Combine olive oil, garlic, and onion in a frying\npan. Add chicken and sauté together until\n• 2 cloves fresh crushed garlic\nchicken has browned.\n• 2 medium chopped onions\n2. Add light soy sauce, vinegar, paprika,\n• 1 ",
        "pound (½ kg) chicken breasts, no skin\nblack pepper, and bay leaf. Stir.\n• 2 tablespoons light soy sauce\n3. Bring to a boil. Simmer for 45–60 minutes or\n• ¼ cup vinegar until chicken is done.\n• 1 teaspoon paprika 4. Remove the chicken and save the liquid in the\npot. Arrange the chicken on a broiler pan. Broil\n• 2 tablespoons ground black pepper\nuntil the chicken has nicely browned. Remove\n• 1 bay leaf, broken in half\nfrom the broiler and place it in a serving bowl.\n• 1 medium red tomato (optional)\n5. Continue to boil the sauce in the uncovered\npan until volume is reduced to about half and\nthe sauce is thick.\nNutrition Information\n6. Pour the thickened sauce over broiled\nadobong (chicken) and garnish with red\nMakes 4 servings\ntomatoes, if desired.\nServing size: ½ cup\nEach serving provides:\nCalories: 190\nTotal Fat: 5 g\nQuick Tip\nSaturated Fat: 1 g\nCholesterol: 70 mg\nSodium: 330 mg This recipe is lower in saturated fat and cholesterol because:\nTotal Fiber: 2 g\n• It is made using chicken wi",
        "thout the skin and any extra fat\nProtein: 26 g\nis removed.\nCarbohydrates: 10 g\nPotassium: 370 mg • Only 1 teaspoon of unsaturated fat (olive oil) is added.\n• It is flavored with vegetables and herbs and is boiled an\nbroiled slowly in moist heat instead of fat.\nSource: Filipino-American Nutrition and Fitness Teacher’s\nGuide, Kalusugan Community Services, San Diego, CA.\n2\nLumpiang Sariwa (Fresh Lumpia)\nYou and your family will love this tasty recipe. The ingredients—ground chicken\nor pork, olive oil, peanuts, and fresh herbs and spices—add flavor. Also, the\nlumpiang sariwa is served fresh so it has fewer calories than fried lumpiang.\nIngredients Directions\n• ½ cup ground chicken or lean pork 1. Heat oil, and sauté ground meat with the\nshrimp and garlic.\n• ½ cup shrimp, cleaned and deveined\n2. Add vegetables until slightly crisp. Pour in\n• 1 tablespoon olive oil\nthe chicken broth until cooked.\n• 2 cloves chopped garlic\n3. Season with salt and pepper.\n• ½ cup cabbage, julienned\n4. Set asid",
        "e and drain in a colander.\n• ½ cup green beans, julienned\n5. Save the broth for the lumpia sauce.\n• ½ cup carrots, julienned\n6. Soak the Vietnamese spring roll wrappers\n• ¼ cup celery, julienned\none at a time in water until soft and\n• ¼ cup jicama, julienned (may substitute transparent. Dry immediately with a paper\nchestnuts) towel.\n• ½ cup chicken broth 7. Lay the lettuce on the wrapper.\n• ¼ teaspoon salt 8. Place 2 tablespoons of the vegetable\n• ¼ teaspoon pepper mixture on the wrapper.\n• 8 Vietnamese spring-roll wrappers or 9. Fold in one side of the wrapper and roll\nlumpia wrappers tightly.\n• 8 pieces red leaf lettuce 10. Serve with lumpia sauce on top.\nSprinkle with chopped peanuts.\n• ⅓ cup peanuts, dry, roasted, and chopped\nNutrition Information\nLumpia Sauce\nMakes 8 servings\n• 1 cup broth from the sautéed vegetables Serving size: 1 lumpia\n• 1 tablespoon light soy sauce\nEach serving provides:\n• 1 tablespoon brown sugar\nCalories: 160\n• 3 cloves garlic, minced\nTotal Fat: 4 g\n• 1 tea",
        "spoon cornstarch Saturated Fat: 0.5 g\n• 2 tablespoons cold water for mixing cornstarch Cholesterol: 55 mg\nSodium: 150 mg\nDirections Total Fiber: 2 g\n1. Mix together vegetable broth, soy sauce, brown Protein: 10 g\nsugar, and garlic, and bring to a boil. Carbohydrates: 21 g\nPotassium: 170 mg\n2. Mix the cornstarch in 2 tablespoons of cold water.\n3. Slowly add the cornstarch mixture to the broth.\nStir until sauce thickens.\nSource: Mula sa Puso, Heart Healthy Traditional\nFilipino Recipes, American Heart Association, 1999.\n3\nPesang Isda (Fish Simmered With\nGinger and Tomatoes)\nThis main dish is heart healthy because the fish is simmered in water,\nnot fried, and no fat is added. Flavoring comes from the herbs and spices\ninstead of sauces that are high in sodium.\nIngredients Directions\n1. In a 4-quart saucepan,\n• ¼ cup fresh ginger, thinly\nsimmer sliced ginger,\nsliced (about 2 inches long)\ntomatoes, and onions in\n• 1 cup ripe tomatoes, chopped\n4 cups of water over medium\n• 1 cup white or yello",
        "w onions, heat until onions are tender\nthinly sliced (about 7 to 8 minutes).\n• 4 cups water 2. Reduce heat to low, add fish\n• 2 pounds fleshy fish (c and poach gently until almost\nfillet, halibut steak, or trout done (about 3 to 4 minutes).\n• 2 cups pechay (bok choy) 3. Add pechay stems, salt, and\nstems and leaves, cut up ground pepper. Cook for\nseparately 1 minute; then add pechay\nleaves and green onions.\n• ½ teaspoon salt\nCook another 30 seconds.\n• ½ teaspoon ground pepper\n4. Serve immediately.\n• 1 cup green onions, sliced\nNutrition Information\nMakes 6 servings\nServing size: 3 ounces lean\nfish and ½ cup vegetables\nEach serving provides:\nCalories: 160\nTotal Fat: 2 g\nSaturated Fat: 0.5 g\nCholesterol: 80 mg\nSodium: 340 mg\nTotal Fiber: 2 g\nProtein: 30 g\nCarbohydrates: 6 g\nPotassium: 630 mg\nSource: Filipino American Food Practices, Customs, and Holidays, American Dietetic Association, 1994.\n4\nMunggo Guisado (Sautéed Mung Beans)\nYour family will love this heart healthy side dish. It is mad",
        "e with vegetables,\nseafood, lean meat, and a small amount of corn oil. The pork is slowly\nsimmered in moist heat with vegetables and mung beans, creating flavors\nthat will make your taste buds jump for joy!\nIngredients Directions\n• 1 tablespoon corn oil 1. In a skillet, heat oil, and sauté\ncrushed garlic until lightly brown.\n• 2 cloves fresh garlic, crushed (or\n1 tablespoon, minced) 2. Add onion and tomatoes. Sauté\nuntil skin begins to curl.\n• 1 cup white onions, chopped\n3. Add pork, and sauté until lightly\n• 1 cup ripe tomatoes, chopped\nbrown.\n• 1 cup (4 ounces) lean pork, thinly\n4. Add water, and simmer pork for\nsliced\nabout 15 minutes.\n• 4 cups water\n5. Add the sautéed mix to mung\n• 3½ cups precooked mung beans\nbeans, and continue to simmer\n(from 1¾ cups dry beans)*\n15 minutes.\n• 1 teaspoon salt\n6. Season with salt and ground\n• 1 teaspoon ground pepper\npepper.\n• 1 cup (4 ounces) shrimp, peeled\n7. Add peeled shrimp.\nand deveined\n8. Add frozen leaf spinach, and\n• 1 cup (about ⅔ of a 1",
        "0-ounce\ncook 4 minutes until done.\npackage) leaf spinach, frozen\n* To cook dry, uncooked mung beans:\nWash and boil the uncooked mung\nbeans in a large saucepan, using 6 Nutrition Information\ncups of water. Cook until tender, about\n1½ to 2 hours. Drain. Makes 8 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 160\nTotal Fat: 3.5 g\nSaturated Fat: 1 g\nCholesterol: 35 mg\nSodium: 350 mg\nSource: Filipino American Food Practices, Customs,\nand Holidays, American Dietetic Association, 1994. Total Fiber: 8 g\nProtein: 13 g\nCarbohydrates: 19 g\nPotassium: 370 mg\n5\nAmpalaya (Bitter Melon) With Pork\nThis recipe is lower in fat and sodium than a typical ampalaya dish\nbecause it uses lean meat that is sautéed and simmered instead of fried.\nIngredients Directions\n• 1 cup onion, chopped 1. Using a large skillet, lightly sauté\nonions and garlic in hot olive oil.\n• 6 cloves garlic, crushed\n2. Add the ground pork and cook until\n• 1 tablespoon olive oil\nalmost done.\n• ½ pound (0.2 kg) lean ground ",
        "pork\n3. Add the sliced bitter melon.\n• 2 cups Ampalaya*, sliced\n4. Cover and simmer until bitter melon\n• 2 teaspoons light soy sauce\nturns green. Do not overcook.\n• ½ teaspoon black pepper\n5. Season with light soy sauce and\nblack pepper.\n* Ampalaya (bitter melon) is a fruit that is\noblong, cylindrical, pointed at both ends,\nribbed, and wrinkled.\nNutrition Information\nMakes 4 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 150\nTotal Fat: 6 g\nSaturated Fat: 1.5 g\nCholesterol: 45 mg\nSodium: 200 mg\nTotal Fiber: 1 g\nProtein: 17 g\nCarbohydrates: 7 g\nPotassium: 600 mg\nSource: Adapted from Mula Sa Puso, Heart Healthy Traditional Filipino Recipes, American Heart Association, 1999.\n6\nCantaloupe Crush\nTry this refreshing, heart healthy drink that uses fresh fruit, fat-free milk, and low\namounts of sweetener. Children and adults alike will love it!\nIngredients Directions\n• ½ cantaloupe 1. Cut cantaloupe into small\ncubes or thin strips.\n• 1 cup fat-free milk\n2. Mix cantaloupe, milk, a",
        "nd\n• 1½ cups ice\nice in a blender until smooth.\n• Sweetener, as needed\n3. Sweeten to taste.\n(about 1 to 2 teaspoons\nsugar or equivalent of\nanother sweetener)\nNutrition Information\nMakes 4 servings\nServing size: ½ cup\nEach serving provides:\nCalories: 50\nTotal Fat: 0 g\nSaturated Fat: 0 g\nCholesterol: 0 mg\nSodium: 40 mg\nTotal Fiber: 0 g\nProtein: 3 g\nCarbohydrates: 10 g\nPotassium: 280 mg\nSource: Adapted from the National Cancer Institute and InteliHealth (intelihealth.com), 2013.\n7\nVegetable Kare-Kare (Peanut Stew)\nThis version of vegetable kare-kare is healthier than the traditional Filipino dish\nbecause it has no cholesterol. It uses gluten instead of oxtail or other meat. It is also\npacked with vegetables and made complete with a nutty, low-sodium sauce.\nIngredients\n• 1 ¼ cup gluten or seitan,* cubes\n• 2 tablespoons corn oil\n• 2 cloves garlic, crushed\nNutrition Information\n• 1 onion, medium, sliced\n• ½ cup ground peanuts Makes 6 servings\n• ¼ cup ground toasted rice**\nEach serving provid",
        "es:\n• ¼ teaspoon salt\nCalories: 300\n• 1 cup eggplant, sliced\nTotal Fat: 12 g\n• ½ cup string beans, sliced Saturated Fat: 1.5 g\n• ⅔ cup banana heart or bud Cholesterol: 0 mg\nSodium: 125 mg\n• ½ cup bok choy (pechay), sliced\nTotal Fiber: 4 g\n* Gluten is made from protein that is in a variety of grains, such Protein: 36 g\nas wheat and rye, and is mixed and kneaded with water. Seitan\nCarbohydrates: 20 g\nis a form of wheat gluten. It is sold as strips or in cans at health\nPotassium: 320 mg\nfood stores and Asian supermarkets.\n** To make ground, toasted rice: Place rice, ½ cup at a time,\nin a frying pan or wok and heat over moderate heat, stirring\nfrequently to keep it from burning and to allow it to develop a\nuniform, deep golden color—2 to 3 minutes. Then remove it\nfrom heat and cool to room temperature. Grind the toasted rice\ncoarsely—not finely grounded—in a blende, or spice or coffee\ngrinder.\nDirections\n1. Sauté gluten cubes in corn oil. Add garlic and onions.\n2. Pour enough water to cove",
        "r gluten, and add ground\npeanuts and ground rice to thicken.\n3. Season with salt.\n4. Add the eggplant, then string beans, then banana,\nthen bok choy (pechay) on top of the cooked gluten.\nSource: PHC Alive Diet, Division of Nutrition and Dietetics, Philippine Heart Center, East Avenue, Quezon City, Philippines, page 91.\n8\nNational Heart, Lung, and Blood Institute\nAn Initiative of the to help reduce health disparities in cardiovascular disease\nand asthma in underserved and minority communities. For more information, visit www.nhlbi.nih.gov/health/healthdisp."
      ],
      "uploaded_at": "2025-07-25T19:52:17.861071",
      "source": "pdf_upload"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "chunks": [
        "Tasty and Healthy —\nHeart Healthy Filipino Recipes\nFish Cardillo\nThis is a delicious, low-cost recipe with low-sodium ingredients.\nKeep it low-fat by not adding meat fat (lard) or other fat.\nIngredients Directions\n1. Thoroughly clean fish. Remove scale\n• 1 pound (½ kg) red snapper\nand gills, and wash thoroughly. Drain and\n• 4 teaspoons corn oil for sauté\nset aside.\n• ¼ cup flou\n2. Slice the raw fish into six pieces\n• 1 large onion, sliced\n3. Heat corn oil in frying pan.\n• 3 or 4 medium-sized tomatoes,\n4. Place the flour into a bowl or plastic bag\nchopped\nPlace the raw fish in the flour and cov\n• ½ cup egg whites, beaten the outside of each fish with flo .\n• ½ cup water 5. Sauté fish until golden brown. Set asid\n• A dash ground pepper on top of a paper towel.\n• 15 stalks green onions, chopped 6. Sauté onion and tomatoes. Add ½ cup of\nwater.\n7. Add the beaten egg whites and fish\nCover and let it simmer for 5–10 minutes.\nNutrition Information\n8. Season with ground pepper.\nMakes 6 servings",
        " 9. Sprinkle with chopped green onions.\nEach serving provides:\nCalories: 170\nTotal Fat: 4 g\nSaturated Fat: 1 g\nCholesterol: 45 mg\nQuick Tip\nSodium: 115 mg\nTotal Fiber: 3 g\nProtein: 20 g\nThis recipe is lower in salt and sodium than other\nCarbohydrates: 13 g\nCardillo recipes because it uses:\nPotassium: 600 mg\n• Fresh, not canned, tomatoes\n• Ground pepper and corn oil with no salt added\n• Fresh onion and green onions\n• Fresh, not smoked or canned, fis\nSource: Philippine Heart Center’s Healthy Heart Cookbook.\n1\nAdobong Manok (Marinated Chicken)\nThis low-cost, low-sodium recipe has great flavor that you and your\nfamily will love! It uses chicken breast which is lower in fat than\nother parts of the chicken, like the thigh and leg.\nIngredients Directions\n• 1 teaspoon olive oil 1. Combine olive oil, garlic, and onion in a frying\npan. Add chicken and sauté together until\n• 2 cloves fresh crushed garlic\nchicken has browned.\n• 2 medium chopped onions\n2. Add light soy sauce, vinegar, paprika,\n• 1 ",
        "pound (½ kg) chicken breasts, no skin\nblack pepper, and bay leaf. Stir.\n• 2 tablespoons light soy sauce\n3. Bring to a boil. Simmer for 45–60 minutes or\n• ¼ cup vinegar until chicken is done.\n• 1 teaspoon paprika 4. Remove the chicken and save the liquid in the\npot. Arrange the chicken on a broiler pan. Broil\n• 2 tablespoons ground black pepper\nuntil the chicken has nicely browned. Remove\n• 1 bay leaf, broken in half\nfrom the broiler and place it in a serving bowl.\n• 1 medium red tomato (optional)\n5. Continue to boil the sauce in the uncovered\npan until volume is reduced to about half and\nthe sauce is thick.\nNutrition Information\n6. Pour the thickened sauce over broiled\nadobong (chicken) and garnish with red\nMakes 4 servings\ntomatoes, if desired.\nServing size: ½ cup\nEach serving provides:\nCalories: 190\nTotal Fat: 5 g\nQuick Tip\nSaturated Fat: 1 g\nCholesterol: 70 mg\nSodium: 330 mg This recipe is lower in saturated fat and cholesterol because:\nTotal Fiber: 2 g\n• It is made using chicken wi",
        "thout the skin and any extra fat\nProtein: 26 g\nis removed.\nCarbohydrates: 10 g\nPotassium: 370 mg • Only 1 teaspoon of unsaturated fat (olive oil) is added.\n• It is flavored with vegetables and herbs and is boiled an\nbroiled slowly in moist heat instead of fat.\nSource: Filipino-American Nutrition and Fitness Teacher’s\nGuide, Kalusugan Community Services, San Diego, CA.\n2\nLumpiang Sariwa (Fresh Lumpia)\nYou and your family will love this tasty recipe. The ingredients—ground chicken\nor pork, olive oil, peanuts, and fresh herbs and spices—add flavor. Also, the\nlumpiang sariwa is served fresh so it has fewer calories than fried lumpiang.\nIngredients Directions\n• ½ cup ground chicken or lean pork 1. Heat oil, and sauté ground meat with the\nshrimp and garlic.\n• ½ cup shrimp, cleaned and deveined\n2. Add vegetables until slightly crisp. Pour in\n• 1 tablespoon olive oil\nthe chicken broth until cooked.\n• 2 cloves chopped garlic\n3. Season with salt and pepper.\n• ½ cup cabbage, julienned\n4. Set asid",
        "e and drain in a colander.\n• ½ cup green beans, julienned\n5. Save the broth for the lumpia sauce.\n• ½ cup carrots, julienned\n6. Soak the Vietnamese spring roll wrappers\n• ¼ cup celery, julienned\none at a time in water until soft and\n• ¼ cup jicama, julienned (may substitute transparent. Dry immediately with a paper\nchestnuts) towel.\n• ½ cup chicken broth 7. Lay the lettuce on the wrapper.\n• ¼ teaspoon salt 8. Place 2 tablespoons of the vegetable\n• ¼ teaspoon pepper mixture on the wrapper.\n• 8 Vietnamese spring-roll wrappers or 9. Fold in one side of the wrapper and roll\nlumpia wrappers tightly.\n• 8 pieces red leaf lettuce 10. Serve with lumpia sauce on top.\nSprinkle with chopped peanuts.\n• ⅓ cup peanuts, dry, roasted, and chopped\nNutrition Information\nLumpia Sauce\nMakes 8 servings\n• 1 cup broth from the sautéed vegetables Serving size: 1 lumpia\n• 1 tablespoon light soy sauce\nEach serving provides:\n• 1 tablespoon brown sugar\nCalories: 160\n• 3 cloves garlic, minced\nTotal Fat: 4 g\n• 1 tea",
        "spoon cornstarch Saturated Fat: 0.5 g\n• 2 tablespoons cold water for mixing cornstarch Cholesterol: 55 mg\nSodium: 150 mg\nDirections Total Fiber: 2 g\n1. Mix together vegetable broth, soy sauce, brown Protein: 10 g\nsugar, and garlic, and bring to a boil. Carbohydrates: 21 g\nPotassium: 170 mg\n2. Mix the cornstarch in 2 tablespoons of cold water.\n3. Slowly add the cornstarch mixture to the broth.\nStir until sauce thickens.\nSource: Mula sa Puso, Heart Healthy Traditional\nFilipino Recipes, American Heart Association, 1999.\n3\nPesang Isda (Fish Simmered With\nGinger and Tomatoes)\nThis main dish is heart healthy because the fish is simmered in water,\nnot fried, and no fat is added. Flavoring comes from the herbs and spices\ninstead of sauces that are high in sodium.\nIngredients Directions\n1. In a 4-quart saucepan,\n• ¼ cup fresh ginger, thinly\nsimmer sliced ginger,\nsliced (about 2 inches long)\ntomatoes, and onions in\n• 1 cup ripe tomatoes, chopped\n4 cups of water over medium\n• 1 cup white or yello",
        "w onions, heat until onions are tender\nthinly sliced (about 7 to 8 minutes).\n• 4 cups water 2. Reduce heat to low, add fish\n• 2 pounds fleshy fish (c and poach gently until almost\nfillet, halibut steak, or trout done (about 3 to 4 minutes).\n• 2 cups pechay (bok choy) 3. Add pechay stems, salt, and\nstems and leaves, cut up ground pepper. Cook for\nseparately 1 minute; then add pechay\nleaves and green onions.\n• ½ teaspoon salt\nCook another 30 seconds.\n• ½ teaspoon ground pepper\n4. Serve immediately.\n• 1 cup green onions, sliced\nNutrition Information\nMakes 6 servings\nServing size: 3 ounces lean\nfish and ½ cup vegetables\nEach serving provides:\nCalories: 160\nTotal Fat: 2 g\nSaturated Fat: 0.5 g\nCholesterol: 80 mg\nSodium: 340 mg\nTotal Fiber: 2 g\nProtein: 30 g\nCarbohydrates: 6 g\nPotassium: 630 mg\nSource: Filipino American Food Practices, Customs, and Holidays, American Dietetic Association, 1994.\n4\nMunggo Guisado (Sautéed Mung Beans)\nYour family will love this heart healthy side dish. It is mad",
        "e with vegetables,\nseafood, lean meat, and a small amount of corn oil. The pork is slowly\nsimmered in moist heat with vegetables and mung beans, creating flavors\nthat will make your taste buds jump for joy!\nIngredients Directions\n• 1 tablespoon corn oil 1. In a skillet, heat oil, and sauté\ncrushed garlic until lightly brown.\n• 2 cloves fresh garlic, crushed (or\n1 tablespoon, minced) 2. Add onion and tomatoes. Sauté\nuntil skin begins to curl.\n• 1 cup white onions, chopped\n3. Add pork, and sauté until lightly\n• 1 cup ripe tomatoes, chopped\nbrown.\n• 1 cup (4 ounces) lean pork, thinly\n4. Add water, and simmer pork for\nsliced\nabout 15 minutes.\n• 4 cups water\n5. Add the sautéed mix to mung\n• 3½ cups precooked mung beans\nbeans, and continue to simmer\n(from 1¾ cups dry beans)*\n15 minutes.\n• 1 teaspoon salt\n6. Season with salt and ground\n• 1 teaspoon ground pepper\npepper.\n• 1 cup (4 ounces) shrimp, peeled\n7. Add peeled shrimp.\nand deveined\n8. Add frozen leaf spinach, and\n• 1 cup (about ⅔ of a 1",
        "0-ounce\ncook 4 minutes until done.\npackage) leaf spinach, frozen\n* To cook dry, uncooked mung beans:\nWash and boil the uncooked mung\nbeans in a large saucepan, using 6 Nutrition Information\ncups of water. Cook until tender, about\n1½ to 2 hours. Drain. Makes 8 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 160\nTotal Fat: 3.5 g\nSaturated Fat: 1 g\nCholesterol: 35 mg\nSodium: 350 mg\nSource: Filipino American Food Practices, Customs,\nand Holidays, American Dietetic Association, 1994. Total Fiber: 8 g\nProtein: 13 g\nCarbohydrates: 19 g\nPotassium: 370 mg\n5\nAmpalaya (Bitter Melon) With Pork\nThis recipe is lower in fat and sodium than a typical ampalaya dish\nbecause it uses lean meat that is sautéed and simmered instead of fried.\nIngredients Directions\n• 1 cup onion, chopped 1. Using a large skillet, lightly sauté\nonions and garlic in hot olive oil.\n• 6 cloves garlic, crushed\n2. Add the ground pork and cook until\n• 1 tablespoon olive oil\nalmost done.\n• ½ pound (0.2 kg) lean ground ",
        "pork\n3. Add the sliced bitter melon.\n• 2 cups Ampalaya*, sliced\n4. Cover and simmer until bitter melon\n• 2 teaspoons light soy sauce\nturns green. Do not overcook.\n• ½ teaspoon black pepper\n5. Season with light soy sauce and\nblack pepper.\n* Ampalaya (bitter melon) is a fruit that is\noblong, cylindrical, pointed at both ends,\nribbed, and wrinkled.\nNutrition Information\nMakes 4 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 150\nTotal Fat: 6 g\nSaturated Fat: 1.5 g\nCholesterol: 45 mg\nSodium: 200 mg\nTotal Fiber: 1 g\nProtein: 17 g\nCarbohydrates: 7 g\nPotassium: 600 mg\nSource: Adapted from Mula Sa Puso, Heart Healthy Traditional Filipino Recipes, American Heart Association, 1999.\n6\nCantaloupe Crush\nTry this refreshing, heart healthy drink that uses fresh fruit, fat-free milk, and low\namounts of sweetener. Children and adults alike will love it!\nIngredients Directions\n• ½ cantaloupe 1. Cut cantaloupe into small\ncubes or thin strips.\n• 1 cup fat-free milk\n2. Mix cantaloupe, milk, a",
        "nd\n• 1½ cups ice\nice in a blender until smooth.\n• Sweetener, as needed\n3. Sweeten to taste.\n(about 1 to 2 teaspoons\nsugar or equivalent of\nanother sweetener)\nNutrition Information\nMakes 4 servings\nServing size: ½ cup\nEach serving provides:\nCalories: 50\nTotal Fat: 0 g\nSaturated Fat: 0 g\nCholesterol: 0 mg\nSodium: 40 mg\nTotal Fiber: 0 g\nProtein: 3 g\nCarbohydrates: 10 g\nPotassium: 280 mg\nSource: Adapted from the National Cancer Institute and InteliHealth (intelihealth.com), 2013.\n7\nVegetable Kare-Kare (Peanut Stew)\nThis version of vegetable kare-kare is healthier than the traditional Filipino dish\nbecause it has no cholesterol. It uses gluten instead of oxtail or other meat. It is also\npacked with vegetables and made complete with a nutty, low-sodium sauce.\nIngredients\n• 1 ¼ cup gluten or seitan,* cubes\n• 2 tablespoons corn oil\n• 2 cloves garlic, crushed\nNutrition Information\n• 1 onion, medium, sliced\n• ½ cup ground peanuts Makes 6 servings\n• ¼ cup ground toasted rice**\nEach serving provid",
        "es:\n• ¼ teaspoon salt\nCalories: 300\n• 1 cup eggplant, sliced\nTotal Fat: 12 g\n• ½ cup string beans, sliced Saturated Fat: 1.5 g\n• ⅔ cup banana heart or bud Cholesterol: 0 mg\nSodium: 125 mg\n• ½ cup bok choy (pechay), sliced\nTotal Fiber: 4 g\n* Gluten is made from protein that is in a variety of grains, such Protein: 36 g\nas wheat and rye, and is mixed and kneaded with water. Seitan\nCarbohydrates: 20 g\nis a form of wheat gluten. It is sold as strips or in cans at health\nPotassium: 320 mg\nfood stores and Asian supermarkets.\n** To make ground, toasted rice: Place rice, ½ cup at a time,\nin a frying pan or wok and heat over moderate heat, stirring\nfrequently to keep it from burning and to allow it to develop a\nuniform, deep golden color—2 to 3 minutes. Then remove it\nfrom heat and cool to room temperature. Grind the toasted rice\ncoarsely—not finely grounded—in a blende, or spice or coffee\ngrinder.\nDirections\n1. Sauté gluten cubes in corn oil. Add garlic and onions.\n2. Pour enough water to cove",
        "r gluten, and add ground\npeanuts and ground rice to thicken.\n3. Season with salt.\n4. Add the eggplant, then string beans, then banana,\nthen bok choy (pechay) on top of the cooked gluten.\nSource: PHC Alive Diet, Division of Nutrition and Dietetics, Philippine Heart Center, East Avenue, Quezon City, Philippines, page 91.\n8\nNational Heart, Lung, and Blood Institute\nAn Initiative of the to help reduce health disparities in cardiovascular disease\nand asthma in underserved and minority communities. For more information, visit www.nhlbi.nih.gov/health/healthdisp."
      ],
      "uploaded_at": "2025-07-25T19:52:27.341599",
      "source": "pdf_upload"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "chunks": [
        "Tasty and Healthy —\nHeart Healthy Filipino Recipes\nFish Cardillo\nThis is a delicious, low-cost recipe with low-sodium ingredients.\nKeep it low-fat by not adding meat fat (lard) or other fat.\nIngredients Directions\n1. Thoroughly clean fish. Remove scale\n• 1 pound (½ kg) red snapper\nand gills, and wash thoroughly. Drain and\n• 4 teaspoons corn oil for sauté\nset aside.\n• ¼ cup flou\n2. Slice the raw fish into six pieces\n• 1 large onion, sliced\n3. Heat corn oil in frying pan.\n• 3 or 4 medium-sized tomatoes,\n4. Place the flour into a bowl or plastic bag\nchopped\nPlace the raw fish in the flour and cov\n• ½ cup egg whites, beaten the outside of each fish with flo .\n• ½ cup water 5. Sauté fish until golden brown. Set asid\n• A dash ground pepper on top of a paper towel.\n• 15 stalks green onions, chopped 6. Sauté onion and tomatoes. Add ½ cup of\nwater.\n7. Add the beaten egg whites and fish\nCover and let it simmer for 5–10 minutes.\nNutrition Information\n8. Season with ground pepper.\nMakes 6 servings",
        " 9. Sprinkle with chopped green onions.\nEach serving provides:\nCalories: 170\nTotal Fat: 4 g\nSaturated Fat: 1 g\nCholesterol: 45 mg\nQuick Tip\nSodium: 115 mg\nTotal Fiber: 3 g\nProtein: 20 g\nThis recipe is lower in salt and sodium than other\nCarbohydrates: 13 g\nCardillo recipes because it uses:\nPotassium: 600 mg\n• Fresh, not canned, tomatoes\n• Ground pepper and corn oil with no salt added\n• Fresh onion and green onions\n• Fresh, not smoked or canned, fis\nSource: Philippine Heart Center’s Healthy Heart Cookbook.\n1\nAdobong Manok (Marinated Chicken)\nThis low-cost, low-sodium recipe has great flavor that you and your\nfamily will love! It uses chicken breast which is lower in fat than\nother parts of the chicken, like the thigh and leg.\nIngredients Directions\n• 1 teaspoon olive oil 1. Combine olive oil, garlic, and onion in a frying\npan. Add chicken and sauté together until\n• 2 cloves fresh crushed garlic\nchicken has browned.\n• 2 medium chopped onions\n2. Add light soy sauce, vinegar, paprika,\n• 1 ",
        "pound (½ kg) chicken breasts, no skin\nblack pepper, and bay leaf. Stir.\n• 2 tablespoons light soy sauce\n3. Bring to a boil. Simmer for 45–60 minutes or\n• ¼ cup vinegar until chicken is done.\n• 1 teaspoon paprika 4. Remove the chicken and save the liquid in the\npot. Arrange the chicken on a broiler pan. Broil\n• 2 tablespoons ground black pepper\nuntil the chicken has nicely browned. Remove\n• 1 bay leaf, broken in half\nfrom the broiler and place it in a serving bowl.\n• 1 medium red tomato (optional)\n5. Continue to boil the sauce in the uncovered\npan until volume is reduced to about half and\nthe sauce is thick.\nNutrition Information\n6. Pour the thickened sauce over broiled\nadobong (chicken) and garnish with red\nMakes 4 servings\ntomatoes, if desired.\nServing size: ½ cup\nEach serving provides:\nCalories: 190\nTotal Fat: 5 g\nQuick Tip\nSaturated Fat: 1 g\nCholesterol: 70 mg\nSodium: 330 mg This recipe is lower in saturated fat and cholesterol because:\nTotal Fiber: 2 g\n• It is made using chicken wi",
        "thout the skin and any extra fat\nProtein: 26 g\nis removed.\nCarbohydrates: 10 g\nPotassium: 370 mg • Only 1 teaspoon of unsaturated fat (olive oil) is added.\n• It is flavored with vegetables and herbs and is boiled an\nbroiled slowly in moist heat instead of fat.\nSource: Filipino-American Nutrition and Fitness Teacher’s\nGuide, Kalusugan Community Services, San Diego, CA.\n2\nLumpiang Sariwa (Fresh Lumpia)\nYou and your family will love this tasty recipe. The ingredients—ground chicken\nor pork, olive oil, peanuts, and fresh herbs and spices—add flavor. Also, the\nlumpiang sariwa is served fresh so it has fewer calories than fried lumpiang.\nIngredients Directions\n• ½ cup ground chicken or lean pork 1. Heat oil, and sauté ground meat with the\nshrimp and garlic.\n• ½ cup shrimp, cleaned and deveined\n2. Add vegetables until slightly crisp. Pour in\n• 1 tablespoon olive oil\nthe chicken broth until cooked.\n• 2 cloves chopped garlic\n3. Season with salt and pepper.\n• ½ cup cabbage, julienned\n4. Set asid",
        "e and drain in a colander.\n• ½ cup green beans, julienned\n5. Save the broth for the lumpia sauce.\n• ½ cup carrots, julienned\n6. Soak the Vietnamese spring roll wrappers\n• ¼ cup celery, julienned\none at a time in water until soft and\n• ¼ cup jicama, julienned (may substitute transparent. Dry immediately with a paper\nchestnuts) towel.\n• ½ cup chicken broth 7. Lay the lettuce on the wrapper.\n• ¼ teaspoon salt 8. Place 2 tablespoons of the vegetable\n• ¼ teaspoon pepper mixture on the wrapper.\n• 8 Vietnamese spring-roll wrappers or 9. Fold in one side of the wrapper and roll\nlumpia wrappers tightly.\n• 8 pieces red leaf lettuce 10. Serve with lumpia sauce on top.\nSprinkle with chopped peanuts.\n• ⅓ cup peanuts, dry, roasted, and chopped\nNutrition Information\nLumpia Sauce\nMakes 8 servings\n• 1 cup broth from the sautéed vegetables Serving size: 1 lumpia\n• 1 tablespoon light soy sauce\nEach serving provides:\n• 1 tablespoon brown sugar\nCalories: 160\n• 3 cloves garlic, minced\nTotal Fat: 4 g\n• 1 tea",
        "spoon cornstarch Saturated Fat: 0.5 g\n• 2 tablespoons cold water for mixing cornstarch Cholesterol: 55 mg\nSodium: 150 mg\nDirections Total Fiber: 2 g\n1. Mix together vegetable broth, soy sauce, brown Protein: 10 g\nsugar, and garlic, and bring to a boil. Carbohydrates: 21 g\nPotassium: 170 mg\n2. Mix the cornstarch in 2 tablespoons of cold water.\n3. Slowly add the cornstarch mixture to the broth.\nStir until sauce thickens.\nSource: Mula sa Puso, Heart Healthy Traditional\nFilipino Recipes, American Heart Association, 1999.\n3\nPesang Isda (Fish Simmered With\nGinger and Tomatoes)\nThis main dish is heart healthy because the fish is simmered in water,\nnot fried, and no fat is added. Flavoring comes from the herbs and spices\ninstead of sauces that are high in sodium.\nIngredients Directions\n1. In a 4-quart saucepan,\n• ¼ cup fresh ginger, thinly\nsimmer sliced ginger,\nsliced (about 2 inches long)\ntomatoes, and onions in\n• 1 cup ripe tomatoes, chopped\n4 cups of water over medium\n• 1 cup white or yello",
        "w onions, heat until onions are tender\nthinly sliced (about 7 to 8 minutes).\n• 4 cups water 2. Reduce heat to low, add fish\n• 2 pounds fleshy fish (c and poach gently until almost\nfillet, halibut steak, or trout done (about 3 to 4 minutes).\n• 2 cups pechay (bok choy) 3. Add pechay stems, salt, and\nstems and leaves, cut up ground pepper. Cook for\nseparately 1 minute; then add pechay\nleaves and green onions.\n• ½ teaspoon salt\nCook another 30 seconds.\n• ½ teaspoon ground pepper\n4. Serve immediately.\n• 1 cup green onions, sliced\nNutrition Information\nMakes 6 servings\nServing size: 3 ounces lean\nfish and ½ cup vegetables\nEach serving provides:\nCalories: 160\nTotal Fat: 2 g\nSaturated Fat: 0.5 g\nCholesterol: 80 mg\nSodium: 340 mg\nTotal Fiber: 2 g\nProtein: 30 g\nCarbohydrates: 6 g\nPotassium: 630 mg\nSource: Filipino American Food Practices, Customs, and Holidays, American Dietetic Association, 1994.\n4\nMunggo Guisado (Sautéed Mung Beans)\nYour family will love this heart healthy side dish. It is mad",
        "e with vegetables,\nseafood, lean meat, and a small amount of corn oil. The pork is slowly\nsimmered in moist heat with vegetables and mung beans, creating flavors\nthat will make your taste buds jump for joy!\nIngredients Directions\n• 1 tablespoon corn oil 1. In a skillet, heat oil, and sauté\ncrushed garlic until lightly brown.\n• 2 cloves fresh garlic, crushed (or\n1 tablespoon, minced) 2. Add onion and tomatoes. Sauté\nuntil skin begins to curl.\n• 1 cup white onions, chopped\n3. Add pork, and sauté until lightly\n• 1 cup ripe tomatoes, chopped\nbrown.\n• 1 cup (4 ounces) lean pork, thinly\n4. Add water, and simmer pork for\nsliced\nabout 15 minutes.\n• 4 cups water\n5. Add the sautéed mix to mung\n• 3½ cups precooked mung beans\nbeans, and continue to simmer\n(from 1¾ cups dry beans)*\n15 minutes.\n• 1 teaspoon salt\n6. Season with salt and ground\n• 1 teaspoon ground pepper\npepper.\n• 1 cup (4 ounces) shrimp, peeled\n7. Add peeled shrimp.\nand deveined\n8. Add frozen leaf spinach, and\n• 1 cup (about ⅔ of a 1",
        "0-ounce\ncook 4 minutes until done.\npackage) leaf spinach, frozen\n* To cook dry, uncooked mung beans:\nWash and boil the uncooked mung\nbeans in a large saucepan, using 6 Nutrition Information\ncups of water. Cook until tender, about\n1½ to 2 hours. Drain. Makes 8 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 160\nTotal Fat: 3.5 g\nSaturated Fat: 1 g\nCholesterol: 35 mg\nSodium: 350 mg\nSource: Filipino American Food Practices, Customs,\nand Holidays, American Dietetic Association, 1994. Total Fiber: 8 g\nProtein: 13 g\nCarbohydrates: 19 g\nPotassium: 370 mg\n5\nAmpalaya (Bitter Melon) With Pork\nThis recipe is lower in fat and sodium than a typical ampalaya dish\nbecause it uses lean meat that is sautéed and simmered instead of fried.\nIngredients Directions\n• 1 cup onion, chopped 1. Using a large skillet, lightly sauté\nonions and garlic in hot olive oil.\n• 6 cloves garlic, crushed\n2. Add the ground pork and cook until\n• 1 tablespoon olive oil\nalmost done.\n• ½ pound (0.2 kg) lean ground ",
        "pork\n3. Add the sliced bitter melon.\n• 2 cups Ampalaya*, sliced\n4. Cover and simmer until bitter melon\n• 2 teaspoons light soy sauce\nturns green. Do not overcook.\n• ½ teaspoon black pepper\n5. Season with light soy sauce and\nblack pepper.\n* Ampalaya (bitter melon) is a fruit that is\noblong, cylindrical, pointed at both ends,\nribbed, and wrinkled.\nNutrition Information\nMakes 4 servings\nServing size: 1 cup\nEach serving provides:\nCalories: 150\nTotal Fat: 6 g\nSaturated Fat: 1.5 g\nCholesterol: 45 mg\nSodium: 200 mg\nTotal Fiber: 1 g\nProtein: 17 g\nCarbohydrates: 7 g\nPotassium: 600 mg\nSource: Adapted from Mula Sa Puso, Heart Healthy Traditional Filipino Recipes, American Heart Association, 1999.\n6\nCantaloupe Crush\nTry this refreshing, heart healthy drink that uses fresh fruit, fat-free milk, and low\namounts of sweetener. Children and adults alike will love it!\nIngredients Directions\n• ½ cantaloupe 1. Cut cantaloupe into small\ncubes or thin strips.\n• 1 cup fat-free milk\n2. Mix cantaloupe, milk, a",
        "nd\n• 1½ cups ice\nice in a blender until smooth.\n• Sweetener, as needed\n3. Sweeten to taste.\n(about 1 to 2 teaspoons\nsugar or equivalent of\nanother sweetener)\nNutrition Information\nMakes 4 servings\nServing size: ½ cup\nEach serving provides:\nCalories: 50\nTotal Fat: 0 g\nSaturated Fat: 0 g\nCholesterol: 0 mg\nSodium: 40 mg\nTotal Fiber: 0 g\nProtein: 3 g\nCarbohydrates: 10 g\nPotassium: 280 mg\nSource: Adapted from the National Cancer Institute and InteliHealth (intelihealth.com), 2013.\n7\nVegetable Kare-Kare (Peanut Stew)\nThis version of vegetable kare-kare is healthier than the traditional Filipino dish\nbecause it has no cholesterol. It uses gluten instead of oxtail or other meat. It is also\npacked with vegetables and made complete with a nutty, low-sodium sauce.\nIngredients\n• 1 ¼ cup gluten or seitan,* cubes\n• 2 tablespoons corn oil\n• 2 cloves garlic, crushed\nNutrition Information\n• 1 onion, medium, sliced\n• ½ cup ground peanuts Makes 6 servings\n• ¼ cup ground toasted rice**\nEach serving provid",
        "es:\n• ¼ teaspoon salt\nCalories: 300\n• 1 cup eggplant, sliced\nTotal Fat: 12 g\n• ½ cup string beans, sliced Saturated Fat: 1.5 g\n• ⅔ cup banana heart or bud Cholesterol: 0 mg\nSodium: 125 mg\n• ½ cup bok choy (pechay), sliced\nTotal Fiber: 4 g\n* Gluten is made from protein that is in a variety of grains, such Protein: 36 g\nas wheat and rye, and is mixed and kneaded with water. Seitan\nCarbohydrates: 20 g\nis a form of wheat gluten. It is sold as strips or in cans at health\nPotassium: 320 mg\nfood stores and Asian supermarkets.\n** To make ground, toasted rice: Place rice, ½ cup at a time,\nin a frying pan or wok and heat over moderate heat, stirring\nfrequently to keep it from burning and to allow it to develop a\nuniform, deep golden color—2 to 3 minutes. Then remove it\nfrom heat and cool to room temperature. Grind the toasted rice\ncoarsely—not finely grounded—in a blende, or spice or coffee\ngrinder.\nDirections\n1. Sauté gluten cubes in corn oil. Add garlic and onions.\n2. Pour enough water to cove",
        "r gluten, and add ground\npeanuts and ground rice to thicken.\n3. Season with salt.\n4. Add the eggplant, then string beans, then banana,\nthen bok choy (pechay) on top of the cooked gluten.\nSource: PHC Alive Diet, Division of Nutrition and Dietetics, Philippine Heart Center, East Avenue, Quezon City, Philippines, page 91.\n8\nNational Heart, Lung, and Blood Institute\nAn Initiative of the to help reduce health disparities in cardiovascular disease\nand asthma in underserved and minority communities. For more information, visit www.nhlbi.nih.gov/health/healthdisp."
      ],
      "uploaded_at": "2025-07-25T19:52:36.617191",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:06.971405",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:07.579594",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:08.102474",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:08.602491",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:09.118122",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:09.696276",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:10.197910",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:10.815552",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:11.272191",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:11.868298",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:12.524783",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:13.067966",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:13.656449",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:14.167300",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:14.776778",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:15.276832",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:15.925655",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:16.502995",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:17.104223",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:17.742594",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:18.309236",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:18.832483",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:19.250372",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:19.760792",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:20.420587",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:20.923786",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:21.431177",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:22.008278",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:22.580703",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:23.068596",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:23.547022",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:24.123101",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:24.654658",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:25.237386",
      "source": "pdf_upload"
    },
    {
      "name": "example_file.pdf",
      "chunks": [
        "Attention Is All You Need\nAshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\nGoogleBrain GoogleBrain GoogleResearch GoogleResearch\navaswani@google.com noam@google.com nikip@google.com usz@google.com\nLlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\nGoogleResearch UniversityofToronto GoogleBrain\nllion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\nIlliaPolosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\nconvolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbasedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\nentirely. Experiments on two machine translation tasks show these models to\nbesuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German ",
        "translation task, improving over the existing best results, including\nensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask,\nourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after\ntrainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe\nbestmodelsfromtheliterature. WeshowthattheTransformergeneralizeswellto\nothertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith\nlargeandlimitedtrainingdata.\n1 Introduction\nRecurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks\ninparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand\n∗Equalcontribution.Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted\ntheefforttoevaluatethisidea.Ashish,withIllia,designedandimplementedthefirstTransformermodelsand\nhasbeencruciallyinvolvedineveryaspectofthiswork.Noamproposedscaleddot-productattention,multi-head\nattentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvol",
        "vedinnearlyevery\ndetail.Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand\ntensor2tensor.Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and\nefficientinferenceandvisualizations.LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand\nimplementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating\nourresearch.\n†WorkperformedwhileatGoogleBrain.\n‡WorkperformedwhileatGoogleResearch.\n31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA.\n7102\nceD\n6\n]LC.sc[\n5v26730.6071:viXra\ntransductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5]. Numerous\neffortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder\narchitectures[38,24,15].\nRecurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput\nsequences. Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden\nstatesh ,asafunctionof",
        "theprevioushiddenstateh andtheinputforpositiont. Thisinherently\nt t−1\nsequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger\nsequencelengths,asmemoryconstraintslimitbatchingacrossexamples. Recentworkhasachieved\nsignificantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional\ncomputation[32],whilealsoimprovingmodelperformanceincaseofthelatter. Thefundamental\nconstraintofsequentialcomputation,however,remains.\nAttentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-\ntionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein\ntheinputoroutputsequences[2,19]. Inallbutafewcases[27],however,suchattentionmechanisms\nareusedinconjunctionwitharecurrentnetwork.\nInthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead\nrelyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.\nTheTransformerallowsforsignificantlymoreparallelizationand",
        "canreachanewstateoftheartin\ntranslationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.\n2 Background\nThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU\n[16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding\nblock,computinghiddenrepresentationsinparallelforallinputandoutputpositions.Inthesemodels,\nthenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows\ninthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet. Thismakes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribedinsection3.2.\nSelf-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\nofasinglesequenceinordertocomputearepresentationof",
        "thesequence. Self-attentionhasbeen\nusedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\ntextualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].\nEnd-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\nalignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\nlanguagemodelingtasks[34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\nalignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\nself-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].\n3 ModelArchitecture\nMostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n1 n\nof continuous representations z = (z ,...,z ). Given z, the ",
        "decoder then generates an output\n1 n\nsequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n1 m\n[10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext.\nTheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully\nconnectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1,\nrespectively.\n2\nFigure1: TheTransformer-modelarchitecture.\n3.1 EncoderandDecoderStacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position-\nwisefullyconnectedfeed-forwardnetwork. Weemployaresidualconnection[11]aroundeachof\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer\nitself. Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding\nlaye",
        "rs,produceoutputsofdimensiond =512.\nmodel\nDecoder: ThedecoderisalsocomposedofastackofN =6identicallayers. Inadditiontothetwo\nsub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head\nattentionovertheoutputoftheencoderstack. Similartotheencoder,weemployresidualconnections\naroundeachofthesub-layers,followedbylayernormalization. Wealsomodifytheself-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe\npredictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.\n3.2 Attention\nAnattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput,\nwherethequery,keys,values,andoutputareallvectors. Theoutputiscomputedasaweightedsum\nofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe\nquerywiththecorrespondingkey.\n3"
      ],
      "uploaded_at": "2025-07-25T19:53:25.860650",
      "source": "pdf_upload"
    }
  ],
  "uploaded_pdfs": [
    {
      "name": "Recipes-Filipino.pdf",
      "uploaded_at": "2025-07-25T19:50:43.933380"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "uploaded_at": "2025-07-25T19:50:48.684958"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "uploaded_at": "2025-07-25T19:50:53.332412"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "uploaded_at": "2025-07-25T19:50:58.195599"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "uploaded_at": "2025-07-25T19:51:03.032347"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "uploaded_at": "2025-07-25T19:51:07.594307"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "uploaded_at": "2025-07-25T19:51:12.119632"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "uploaded_at": "2025-07-25T19:51:16.922723"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "uploaded_at": "2025-07-25T19:51:21.723792"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "uploaded_at": "2025-07-25T19:51:26.933470"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "uploaded_at": "2025-07-25T19:51:32.400528"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "uploaded_at": "2025-07-25T19:51:39.166555"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "uploaded_at": "2025-07-25T19:51:46.585775"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "uploaded_at": "2025-07-25T19:51:55.846098"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "uploaded_at": "2025-07-25T19:52:07.812204"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "uploaded_at": "2025-07-25T19:52:17.861071"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "uploaded_at": "2025-07-25T19:52:27.341599"
    },
    {
      "name": "Recipes-Filipino.pdf",
      "uploaded_at": "2025-07-25T19:52:36.617191"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:06.971405"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:07.579594"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:08.102474"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:08.602491"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:09.118122"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:09.696276"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:10.197910"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:10.815552"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:11.272191"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:11.868298"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:12.524783"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:13.067966"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:13.656449"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:14.167300"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:14.776778"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:15.276832"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:15.925655"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:16.502995"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:17.104223"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:17.742594"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:18.309236"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:18.832483"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:19.250372"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:19.760792"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:20.420587"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:20.923786"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:21.431177"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:22.008278"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:22.580703"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:23.068596"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:23.547022"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:24.123101"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:24.654658"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:25.237386"
    },
    {
      "name": "example_file.pdf",
      "uploaded_at": "2025-07-25T19:53:25.860650"
    }
  ]
}